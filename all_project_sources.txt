# Arquivos fonte do projeto: Lotofacil_Analysis
# Gerado em: 2025-05-20 23:01:48
# Total de arquivos .py consolidados: 85
================================================================================

--------------------------------------------------------------------------------
# Arquivo: gerar_consolidado.py
--------------------------------------------------------------------------------
import os
import datetime

def create_consolidated_source_file(
    project_root_dir: str,
    output_file_name: str = "all_project_sources.txt",
    excluded_dirs: list[str] = None,
    excluded_file_extensions: list[str] = None,
    explicitly_excluded_files: list[str] = None
) -> None:
    """
    Varre um diretório de projeto, coleta o conteúdo de todos os arquivos .py
    e os consolida em um único arquivo de texto.

    Args:
        project_root_dir (str): O caminho para o diretório raiz do projeto.
        output_file_name (str): O nome do arquivo .txt de saída.
        excluded_dirs (list[str], optional): Lista de nomes de diretórios a serem ignorados.
                                            Padrões: ['.git', '__pycache__', 'venv', '.venv',
                                                      'Logs', 'Data', 'Plots', 'docs', 'tests_output',
                                                      '.pytest_cache', '.mypy_cache', 'build', 'dist', 'archive']
        excluded_file_extensions (list[str], optional): Lista de extensões de arquivo a serem ignoradas
                                                        (além de não ser .py). Padrão: [].
                                                        Arquivos não .py já são ignorados.
        explicitly_excluded_files (list[str], optional): Lista de nomes de arquivos específicos a serem ignorados.
                                                        Padrão: [output_file_name].
    """

    if excluded_dirs is None:
        excluded_dirs = [
            '.git', '__pycache__', 'venv', '.venv', 'Logs', 'Data', 'Plots',
            'docs', 'tests_output', '.pytest_cache', '.mypy_cache',
            'build', 'dist', 'archive', 'notebooks' # Adicionando notebooks como sugestão
        ]
    if excluded_file_extensions is None: # Note: o script já foca apenas em .py
        excluded_file_extensions = []
    if explicitly_excluded_files is None:
        explicitly_excluded_files = [output_file_name]
    else:
        if output_file_name not in explicitly_excluded_files:
            explicitly_excluded_files.append(output_file_name)

    # Normaliza o caminho do diretório raiz
    project_root_dir = os.path.abspath(project_root_dir)
    output_file_path = os.path.join(project_root_dir, output_file_name)

    print(f"Iniciando a consolidação dos arquivos .py do projeto: {project_root_dir}")
    print(f"Arquivo de saída será: {output_file_path}")
    print(f"Diretórios a serem ignorados: {excluded_dirs}")
    print(f"Arquivos explicitamente ignorados: {explicitly_excluded_files}")

    source_contents: list[tuple[str, str]] = [] # Lista para armazenar (caminho_relativo, conteudo)

    for root, dirs, files in os.walk(project_root_dir, topdown=True):
        # Remove diretórios excluídos da lista de 'dirs' para que os.walk não entre neles
        dirs[:] = [d for d in dirs if d not in excluded_dirs and not d.startswith('.')]

        for file_name in files:
            if file_name in explicitly_excluded_files:
                continue

            if file_name.endswith(".py"):
                # Verifica se alguma parte do caminho contém um diretório excluído
                # Isso é uma checagem adicional, pois dirs[:] já deveria ter lidado com isso
                current_dir_relative_path = os.path.relpath(root, project_root_dir)
                if any(excluded_dir in current_dir_relative_path.split(os.sep) for excluded_dir in excluded_dirs if excluded_dir != '.'):
                    continue

                file_path = os.path.join(root, file_name)
                relative_file_path = os.path.relpath(file_path, project_root_dir)

                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                    source_contents.append((relative_file_path, content))
                    print(f"Coletado: {relative_file_path}")
                except Exception as e:
                    print(f"Erro ao ler o arquivo {file_path}: {e}")

    # Ordena os arquivos por caminho para uma ordem consistente
    source_contents.sort(key=lambda x: x[0])

    try:
        with open(output_file_path, 'w', encoding='utf-8') as outfile:
            outfile.write(f"# Arquivos fonte do projeto: {os.path.basename(project_root_dir)}\n")
            outfile.write(f"# Gerado em: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            outfile.write(f"# Total de arquivos .py consolidados: {len(source_contents)}\n")
            outfile.write("=" * 80 + "\n\n")

            for relative_path, content in source_contents:
                outfile.write("-" * 80 + "\n")
                outfile.write(f"# Arquivo: {relative_path}\n")
                outfile.write("-" * 80 + "\n")
                outfile.write(content)
                outfile.write("\n\n") # Adiciona duas linhas em branco entre os arquivos

        print(f"\nConsolidação concluída! {len(source_contents)} arquivos .py foram salvos em '{output_file_path}'.")
    except Exception as e:
        print(f"Erro ao escrever o arquivo de saída {output_file_path}: {e}")

if __name__ == "__main__":
    # --- Configuração ---
    # Coloque o caminho para a pasta raiz do seu projeto aqui
    # Exemplo: se este script (gerador.py) estiver na raiz do projeto Lotofacil_Analysis,
    # o caminho seria "."
    # Se este script estiver em uma subpasta, ajuste conforme necessário.
    project_directory = "." # Assume que o script está na raiz do projeto

    # Nome do arquivo de saída
    output_filename = "all_project_sources.txt"

    # Você pode customizar as exclusões aqui se necessário:
    custom_excluded_dirs = [
        '.git', '__pycache__', 'venv', '.venv',
        'Logs', 'Data', 'Plots',  # Pastas do seu projeto que não contêm código Python executável
        'docs', 'tests_output', '.pytest_cache', '.mypy_cache',
        'build', 'dist', 'archive', 'notebooks',
        # Adicione outras pastas que você queira ignorar
        # Ex: 'old_code', 'experimental'
    ]
    # Arquivos não .py já são ignorados, então esta lista é para extensões
    # que você queira LOGAR que foram ignoradas, mas o script atual só pega .py
    custom_excluded_extensions = ['.csv', '.db', '.pkl', '.md', '.xlsx', '.log', '.ipynb']

    # Arquivos específicos para excluir (além do próprio arquivo de saída)
    custom_explicitly_excluded_files = [
        output_filename,
        # "nome_de_outro_arquivo_especifico.py" # se houver
    ]

    create_consolidated_source_file(
        project_root_dir=project_directory,
        output_file_name=output_filename,
        excluded_dirs=custom_excluded_dirs,
        # excluded_file_extensions=custom_excluded_extensions, # Descomente se quiser usar
        explicitly_excluded_files=custom_explicitly_excluded_files
    )

--------------------------------------------------------------------------------
# Arquivo: inspect_database.py
--------------------------------------------------------------------------------
# inspect_database.py
import pandas as pd
from pathlib import Path
import sys

# Ajuste para garantir que o diretório 'src' seja encontrado
project_root = Path(__file__).resolve().parent
src_path = project_root / 'src'
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

try:
    from config import Config # Importa a CLASSE Config
    from database_manager import DatabaseManager
except ImportError as e:
    print(f"Erro ao importar módulos. Certifique-se que 'inspect_database.py' está na raiz do projeto.")
    print(f"Detalhe do erro: {e}")
    sys.exit(1)


# Configurar pandas para melhor visualização
pd.set_option('display.max_rows', 50) 
pd.set_option('display.max_columns', None) 
pd.set_option('display.width', 1000)       
pd.set_option('display.colheader_justify', 'left') 
pd.set_option('display.precision', 2) 

def inspect_db():
    cfg = Config() 
    
    # cfg.DB_PATH já é um objeto Path pela definição da classe Config.
    # Esta linha garante que estamos usando este objeto Path consistentemente.
    # CORREÇÃO AQUI: Usar o nome da variável consistentemente.
    db_path_object = Path(cfg.DB_PATH) 

    # CORRIGIDO: Usa a variável correta 'db_path_object'
    if not db_path_object.exists(): 
        print(f"Arquivo do banco de dados não encontrado em: {db_path_object}")
        print("Execute o pipeline principal (python -m src.main --run-steps all_analysis OU --force-reload) primeiro.")
        return

    print(f"Inspecionando banco de dados: {db_path_object}\n")
    # DatabaseManager espera uma string para o path
    db_mngr = DatabaseManager(db_path=str(db_path_object))

    table_names = db_mngr.get_table_names()

    if not table_names:
        print("Nenhuma tabela encontrada no banco de dados.")
        db_mngr.close() 
        return

    print("Tabelas encontradas no banco de dados:")
    for i, name in enumerate(table_names):
        print(f"{i+1}. {name}")
    
    print("\n" + "="*50 + "\n")

    while True:
        try:
            choice = input("Digite o NÚMERO da tabela para ver as primeiras linhas (ou 'sair' para terminar): ")
            if choice.lower() == 'sair':
                break
            
            table_index = int(choice) - 1
            if 0 <= table_index < len(table_names):
                selected_table_name = table_names[table_index]
                print(f"\n--- Conteúdo da tabela: {selected_table_name} ---")
                
                df = db_mngr.load_dataframe(selected_table_name) 
                
                if df is not None:
                    if not df.empty:
                        print(f"Primeiras 5 linhas de '{selected_table_name}':")
                        print(df.head())
                        print(f"\nInformações da tabela '{selected_table_name}':")
                        df.info(verbose=True, show_counts=True)
                        print(f"\nTotal de {len(df)} linhas em '{selected_table_name}'.")
                    else:
                        print(f"A tabela '{selected_table_name}' está vazia.")
                else:
                    print(f"Não foi possível carregar dados da tabela '{selected_table_name}'. Verifique os logs do console.")
                print("\n" + "="*50 + "\n")
            else:
                print("Número inválido. Tente novamente.")
        except ValueError:
            print("Entrada inválida. Por favor, digite um número ou 'sair'.")
        except Exception as e:
            print(f"Ocorreu um erro inesperado: {e}")
    
    db_mngr.close() 

if __name__ == "__main__":
    inspect_db()

--------------------------------------------------------------------------------
# Arquivo: src/__init__.py
--------------------------------------------------------------------------------
# src/__init__.py
# Este arquivo pode ficar vazio.

--------------------------------------------------------------------------------
# Arquivo: src/analysis/__init__.py
--------------------------------------------------------------------------------


--------------------------------------------------------------------------------
# Arquivo: src/analysis/block_aggregator.py
--------------------------------------------------------------------------------
# Lotofacil_Analysis/src/analysis/block_aggregator.py
import pandas as pd
import logging
from typing import Dict, List, Any, Optional
import numpy as np

logger = logging.getLogger(__name__)

def aggregate_block_data_to_wide_format(db_manager: Any, config: Any):
    logger.info("Iniciando agregação de dados de bloco para formato largo (incluindo métricas de grupo).")

    per_dezena_metric_configs = [
        {"source_table_prefix_const_name": "EVOL_METRIC_FREQUENCY_BLOCK_PREFIX", "value_column": "frequencia_absoluta", "analysis_type_name": "frequencia_bloco", "dtype": "Int64"},
        {"source_table_prefix_const_name": "EVOL_RANK_FREQUENCY_BLOCK_PREFIX", "value_column": "rank_no_bloco", "analysis_type_name": "rank_freq_bloco", "dtype": "Int64"},
        {"source_table_prefix_const_name": "EVOL_METRIC_ATRASO_MEDIO_BLOCK_PREFIX", "value_column": "atraso_medio_no_bloco", "analysis_type_name": "atraso_medio_bloco", "dtype": "float"},
        {"source_table_prefix_const_name": "EVOL_METRIC_ATRASO_MAXIMO_BLOCK_PREFIX", "value_column": "atraso_maximo_no_bloco", "analysis_type_name": "atraso_maximo_bloco", "dtype": "Int64"},
        {"source_table_prefix_const_name": "EVOL_METRIC_ATRASO_FINAL_BLOCK_PREFIX", "value_column": "atraso_final_no_bloco", "analysis_type_name": "atraso_final_no_bloco", "dtype": "Int64"},
        {"source_table_prefix_const_name": "EVOL_METRIC_OCCURRENCE_STD_DEV_BLOCK_PREFIX", "value_column": "occurrence_std_dev", "analysis_type_name": "occurrence_std_dev_bloco", "dtype": "float"},
        {"source_table_prefix_const_name": "EVOL_METRIC_DELAY_STD_DEV_BLOCK_PREFIX", "value_column": "delay_std_dev", "analysis_type_name": "delay_std_dev_bloco", "dtype": "float"},
    ]

    required_config_attrs = [
        'CHUNK_TYPES_CONFIG', 'BLOCK_ANALISES_CONSOLIDADAS_PREFIX',
        'ALL_NUMBERS', 'EVOL_BLOCK_GROUP_METRICS_PREFIX'
    ] + [item["source_table_prefix_const_name"] for item in per_dezena_metric_configs]

    missing_attrs = [attr for attr in required_config_attrs if not hasattr(config, attr)]
    if missing_attrs:
        logger.error(f"Atributos de config críticos ausentes para aggregate_block_data_to_wide_format: {missing_attrs}.")
        return

    for chunk_type, list_of_sizes in config.CHUNK_TYPES_CONFIG.items():
        for size_val in list_of_sizes:
            consolidated_table_name = f"{config.BLOCK_ANALISES_CONSOLIDADAS_PREFIX}_{chunk_type}_{size_val}"
            logger.info(f"Processando para tabela consolidada de BLOCKS: '{consolidated_table_name}'")
            all_wide_dfs_for_this_chunk_config: List[pd.DataFrame] = []

            for metric_config_item in per_dezena_metric_configs:
                prefix_constant_name = metric_config_item["source_table_prefix_const_name"]
                source_table_prefix = getattr(config, prefix_constant_name)
                value_column_in_long_table = metric_config_item["value_column"]
                analysis_type_name_for_wide_table = metric_config_item["analysis_type_name"]
                expected_dtype_str = metric_config_item["dtype"]
                long_format_table_name = f"{source_table_prefix}_{chunk_type}_{size_val}"

                if not db_manager.table_exists(long_format_table_name):
                    logger.debug(f"Tabela '{long_format_table_name}' não encontrada para {analysis_type_name_for_wide_table}. Pulando.")
                    continue
                df_long = db_manager.load_dataframe(long_format_table_name)
                if df_long is None or df_long.empty:
                    logger.debug(f"DataFrame de '{long_format_table_name}' vazio para {analysis_type_name_for_wide_table}. Pulando.")
                    continue

                index_cols = ['chunk_seq_id', 'chunk_start_contest', 'chunk_end_contest']
                columns_col = 'dezena'
                required_for_pivot = index_cols + [columns_col, value_column_in_long_table]

                if not all(col in df_long.columns for col in required_for_pivot):
                    logger.error(f"Tabela '{long_format_table_name}' s/ colunas {required_for_pivot}. Colunas: {df_long.columns.tolist()}. Pulando.")
                    continue
                try:
                    if expected_dtype_str == "Int64":
                        df_long[value_column_in_long_table] = pd.to_numeric(df_long[value_column_in_long_table], errors='coerce').astype('Int64')
                    elif expected_dtype_str == "float":
                        df_long[value_column_in_long_table] = pd.to_numeric(df_long[value_column_in_long_table], errors='coerce').astype('float')

                    df_wide_metric = df_long.pivot_table(
                        index=index_cols, columns=columns_col,
                        values=value_column_in_long_table,
                        fill_value=pd.NA if expected_dtype_str == "Int64" else np.nan
                    )
                    df_wide_metric.columns = [f'dezena_{int(col)}' for col in df_wide_metric.columns]
                    df_wide_metric.reset_index(inplace=True)
                    df_wide_metric['tipo_analise'] = analysis_type_name_for_wide_table

                    final_cols_order = index_cols + ['tipo_analise'] + [f'dezena_{i}' for i in config.ALL_NUMBERS]
                    for i in config.ALL_NUMBERS:
                        col_name_to_check = f'dezena_{i}'
                        if col_name_to_check not in df_wide_metric.columns:
                            df_wide_metric[col_name_to_check] = pd.NA if expected_dtype_str == "Int64" else np.nan
                        if expected_dtype_str == "Int64":
                            df_wide_metric[col_name_to_check] = pd.to_numeric(df_wide_metric[col_name_to_check], errors='coerce').astype('Int64')
                        elif expected_dtype_str == "float":
                             df_wide_metric[col_name_to_check] = pd.to_numeric(df_wide_metric[col_name_to_check], errors='coerce').astype('float')

                    actual_final_cols = [col for col in final_cols_order if col in df_wide_metric.columns]
                    all_wide_dfs_for_this_chunk_config.append(df_wide_metric[actual_final_cols])
                except Exception as e:
                    logger.error(f"Erro ao pivotar '{long_format_table_name}' para '{analysis_type_name_for_wide_table}': {e}", exc_info=True)

            block_group_metrics_table_prefix = config.EVOL_BLOCK_GROUP_METRICS_PREFIX
            block_group_metrics_table_name = f"{block_group_metrics_table_prefix}_{chunk_type}_{size_val}"
            df_block_group_metrics = None
            if db_manager.table_exists(block_group_metrics_table_name):
                df_block_group_metrics = db_manager.load_dataframe(block_group_metrics_table_name)
                if df_block_group_metrics is None or df_block_group_metrics.empty:
                     df_block_group_metrics = None
            else:
                logger.debug(f"Tabela de métricas de grupo '{block_group_metrics_table_name}' não encontrada.")

            if all_wide_dfs_for_this_chunk_config:
                df_consolidated_wide = pd.concat(all_wide_dfs_for_this_chunk_config, ignore_index=True)
                if df_block_group_metrics is not None and not df_block_group_metrics.empty: # Adicionado check de não vazio
                    merge_keys = ['chunk_seq_id', 'chunk_start_contest', 'chunk_end_contest']
                    if not df_block_group_metrics[merge_keys].isnull().all(axis=1).any():
                        if all(key in df_consolidated_wide.columns for key in merge_keys) and \
                           all(key in df_block_group_metrics.columns for key in merge_keys):
                            df_consolidated_wide = pd.merge(df_consolidated_wide, df_block_group_metrics, on=merge_keys, how='left')
                        else:
                            logger.warning(f"Chaves de merge para '{consolidated_table_name}' não encontradas. Grupo de bloco não adicionado.")
                    else:
                        logger.warning(f"Métricas de grupo de bloco '{block_group_metrics_table_name}' com chaves nulas. Merge não realizado.")

                if not df_consolidated_wide.empty:
                    # CORREÇÃO APLICADA: removido index=False
                    db_manager.save_dataframe(df_consolidated_wide, consolidated_table_name, if_exists='replace')
                    logger.info(f"Tabela consolidada de BLOCKS '{consolidated_table_name}' salva ({len(df_consolidated_wide)} linhas).")
                else:
                    logger.info(f"DataFrame consolidado de BLOCKS para '{consolidated_table_name}' vazio. Nada salvo.")
            elif df_block_group_metrics is not None and not df_block_group_metrics.empty :
                 logger.info(f"Salvando apenas métricas de grupo para BLOCKS '{consolidated_table_name}'.")
                 # CORREÇÃO APLICADA: removido index=False
                 db_manager.save_dataframe(df_block_group_metrics, consolidated_table_name, if_exists='replace')
            else:
                 logger.warning(f"Nenhum DataFrame gerado para BLOCKS '{consolidated_table_name}'. Tabela não criada/atualizada.")
    logger.info("Agregação de dados de bloco para formato largo concluída.")


def aggregate_cycle_data_to_wide_format(db_manager: Any, config: Any):
    logger.info("Iniciando agregação de dados de CICLO para formato largo.")
    cycle_per_dezena_metric_configs = [
        {"source_table_name_const": "CYCLE_METRIC_FREQUENCY_TABLE_NAME", "value_column": "frequencia_no_ciclo", "analysis_type_name": "frequencia_no_ciclo", "dtype": "Int64"},
        {"source_table_name_const": "CYCLE_METRIC_ATRASO_MEDIO_TABLE_NAME", "value_column": "atraso_medio_no_ciclo", "analysis_type_name": "atraso_medio_no_ciclo", "dtype": "float"},
        {"source_table_name_const": "CYCLE_METRIC_ATRASO_MAXIMO_TABLE_NAME", "value_column": "atraso_maximo_no_ciclo", "analysis_type_name": "atraso_maximo_no_ciclo", "dtype": "Int64"},
        {"source_table_name_const": "CYCLE_METRIC_ATRASO_FINAL_TABLE_NAME", "value_column": "atraso_final_no_ciclo", "analysis_type_name": "atraso_final_no_ciclo", "dtype": "Int64"},
        {"source_table_name_const": "CYCLE_RANK_FREQUENCY_TABLE_NAME", "value_column": "rank_freq_no_ciclo", "analysis_type_name": "rank_freq_no_ciclo", "dtype": "Int64"},
    ]
    consolidated_cycle_table_name = config.CYCLE_ANALISES_CONSOLIDADAS_TABLE_NAME
    all_wide_dfs_for_cycles: List[pd.DataFrame] = []
    cycles_detail_input_table = config.ANALYSIS_CYCLES_DETAIL_TABLE_NAME

    if not db_manager.table_exists(cycles_detail_input_table):
        logger.error(f"Tabela '{cycles_detail_input_table}' não encontrada. Não criar consolidada de ciclos.")
        return
    df_ciclos_detalhe = db_manager.load_dataframe(cycles_detail_input_table)
    if df_ciclos_detalhe is None or df_ciclos_detalhe.empty:
        logger.warning(f"Tabela '{cycles_detail_input_table}' vazia. Não criar consolidada de ciclos.")
        return

    df_closed_ciclos_base_info = df_ciclos_detalhe[df_ciclos_detalhe['concurso_fim'].notna()].copy()
    if df_closed_ciclos_base_info.empty:
        logger.warning(f"Nenhum ciclo fechado em '{cycles_detail_input_table}'. Consolidada de ciclos não gerada.")
        return

    rename_map_ciclos = {'concurso_inicio': 'concurso_inicio_ciclo',
                         'concurso_fim': 'concurso_fim_ciclo'}
    if 'duracao_concursos' in df_closed_ciclos_base_info.columns:
        rename_map_ciclos['duracao_concursos'] = 'duracao_ciclo'
    df_closed_ciclos_base_info.rename(columns=rename_map_ciclos, inplace=True)
    
    # Garantir que ciclo_num exista e seja usado como base
    if 'ciclo_num' not in df_closed_ciclos_base_info.columns:
        logger.error(f"Coluna 'ciclo_num' não encontrada em '{cycles_detail_input_table}' após renomeações.")
        return
        
    base_info_cols = ['ciclo_num', 'concurso_inicio_ciclo', 'concurso_fim_ciclo', 'duracao_ciclo']
    actual_base_info_cols = [col for col in base_info_cols if col in df_closed_ciclos_base_info.columns]
    df_closed_ciclos_base_info = df_closed_ciclos_base_info[actual_base_info_cols].drop_duplicates(subset=['ciclo_num'])


    for metric_config_item in cycle_per_dezena_metric_configs:
        table_name_constant_key = metric_config_item["source_table_name_const"]
        long_format_table_name = getattr(config, table_name_constant_key, table_name_constant_key)
        value_column_in_long_table = metric_config_item["value_column"]
        analysis_type_name_for_wide_table = metric_config_item["analysis_type_name"]
        expected_dtype_str = metric_config_item["dtype"]

        if not db_manager.table_exists(long_format_table_name):
            logger.warning(f"Tabela '{long_format_table_name}' não encontrada para {analysis_type_name_for_wide_table}. Pulando.")
            continue
        df_long_cycle_metric = db_manager.load_dataframe(long_format_table_name)
        if df_long_cycle_metric is None or df_long_cycle_metric.empty:
            logger.warning(f"DataFrame de '{long_format_table_name}' vazio. Pulando."); continue

        index_cols_cycle = ['ciclo_num']; columns_col_cycle = 'dezena'
        required_for_pivot_cycle = index_cols_cycle + [columns_col_cycle, value_column_in_long_table]
        if not all(col in df_long_cycle_metric.columns for col in required_for_pivot_cycle) :
            logger.error(f"Tabela '{long_format_table_name}' s/ colunas {required_for_pivot_cycle}. Colunas: {df_long_cycle_metric.columns.tolist()}. Pulando.")
            continue
        try:
            if expected_dtype_str == "Int64":
                df_long_cycle_metric[value_column_in_long_table] = pd.to_numeric(df_long_cycle_metric[value_column_in_long_table], errors='coerce').astype('Int64')
            elif expected_dtype_str == "float":
                 df_long_cycle_metric[value_column_in_long_table] = pd.to_numeric(df_long_cycle_metric[value_column_in_long_table], errors='coerce').astype('float')

            df_wide_metric = df_long_cycle_metric.pivot_table(
                index=index_cols_cycle, columns=columns_col_cycle,
                values=value_column_in_long_table,
                fill_value=pd.NA if expected_dtype_str == "Int64" else np.nan
            )
            df_wide_metric.columns = [f'dezena_{int(col)}' for col in df_wide_metric.columns]
            df_wide_metric.reset_index(inplace=True)
            df_wide_metric['tipo_analise_ciclo'] = analysis_type_name_for_wide_table
            
            # Merge com df_closed_ciclos_base_info para adicionar informações do ciclo
            if 'ciclo_num' in df_wide_metric.columns: # df_closed_ciclos_base_info já foi verificado
                df_wide_metric_merged = pd.merge(df_closed_ciclos_base_info.copy(), df_wide_metric, on='ciclo_num', how='inner') # Usar .copy() para evitar warnings
            else:
                logger.warning(f"Não merge com base_info para {analysis_type_name_for_wide_table} (sem 'ciclo_num' no df_wide_metric).")
                continue # Pula esta métrica se não puder fazer merge

            final_cols_order = actual_base_info_cols + ['tipo_analise_ciclo'] + [f'dezena_{i}' for i in config.ALL_NUMBERS]
            for i in config.ALL_NUMBERS:
                col_name_to_check = f'dezena_{i}'
                if col_name_to_check not in df_wide_metric_merged.columns:
                    df_wide_metric_merged[col_name_to_check] = pd.NA if expected_dtype_str == "Int64" else np.nan
                if expected_dtype_str == "Int64":
                    df_wide_metric_merged[col_name_to_check] = pd.to_numeric(df_wide_metric_merged[col_name_to_check], errors='coerce').astype('Int64')
                elif expected_dtype_str == "float":
                     df_wide_metric_merged[col_name_to_check] = pd.to_numeric(df_wide_metric_merged[col_name_to_check], errors='coerce').astype('float')

            current_final_cols = [col for col in final_cols_order if col in df_wide_metric_merged.columns]
            all_wide_dfs_for_cycles.append(df_wide_metric_merged[current_final_cols])
        except Exception as e:
            logger.error(f"Erro ao pivotar '{long_format_table_name}' para '{analysis_type_name_for_wide_table}': {e}", exc_info=True)

    df_consolidated_cycles_wide = pd.DataFrame()
    if all_wide_dfs_for_cycles:
        df_consolidated_cycles_wide = pd.concat(all_wide_dfs_for_cycles, ignore_index=True)

    cycle_group_metrics_table_name = config.CYCLE_GROUP_METRICS_TABLE_NAME
    df_cycle_group_metrics = None
    if db_manager.table_exists(cycle_group_metrics_table_name):
        df_cycle_group_metrics = db_manager.load_dataframe(cycle_group_metrics_table_name)
        if df_cycle_group_metrics is None or df_cycle_group_metrics.empty:
            df_cycle_group_metrics = None
    else:
        logger.debug(f"Tabela '{cycle_group_metrics_table_name}' não encontrada.")

    if df_cycle_group_metrics is not None and not df_cycle_group_metrics.empty: # Adicionado check de não vazio
        # Se df_consolidated_cycles_wide estiver vazio, mas temos métricas de grupo e info base de ciclos
        if df_consolidated_cycles_wide.empty and not df_closed_ciclos_base_info.empty:
             if 'ciclo_num' in df_closed_ciclos_base_info.columns and 'ciclo_num' in df_cycle_group_metrics.columns:
                logger.info("Construindo consolidada de ciclos só com base e grupo (sem dados por dezena).")
                df_consolidated_cycles_wide = pd.merge(df_closed_ciclos_base_info.copy(), df_cycle_group_metrics, on='ciclo_num', how='inner') # Usar .copy()
             else:
                logger.warning("Não merge de grupo de ciclo com base_info (sem 'ciclo_num').")
        elif not df_consolidated_cycles_wide.empty: # Se já temos dados por dezena, fazemos merge neles
            if 'ciclo_num' in df_consolidated_cycles_wide.columns and 'ciclo_num' in df_cycle_group_metrics.columns:
                 df_consolidated_cycles_wide = pd.merge(df_consolidated_cycles_wide, df_cycle_group_metrics, on='ciclo_num', how='left')
                 logger.info(f"Métricas de grupo de ciclo adicionadas. Shape: {df_consolidated_cycles_wide.shape}")
            else:
                logger.warning("Coluna 'ciclo_num' não encontrada para merge de grupo de ciclo.")
        else: # Caso onde df_consolidated_cycles_wide estava vazio E df_closed_ciclos_base_info estava vazio
            logger.warning("Não foi possível mesclar métricas de grupo de ciclo por falta de dados base de ciclo.")


    if not df_consolidated_cycles_wide.empty:
        try:
            # CORREÇÃO APLICADA: removido index=False
            db_manager.save_dataframe(df_consolidated_cycles_wide, consolidated_cycle_table_name, if_exists='replace')
            logger.info(f"Tabela consolidada de CICLOS '{consolidated_cycle_table_name}' salva ({len(df_consolidated_cycles_wide)}).")
        except Exception as e:
            logger.error(f"Erro ao salvar consolidada de CICLOS '{consolidated_cycle_table_name}': {e}", exc_info=True)
    else:
        logger.warning(f"Nenhum DataFrame consolidado para CICLOS. Tabela '{consolidated_cycle_table_name}' não criada.")
    logger.info("Agregação de dados de CICLO para formato largo concluída.")

--------------------------------------------------------------------------------
# Arquivo: src/analysis/chunk_analysis.py
--------------------------------------------------------------------------------
# Lotofacil_Analysis/src/analysis/chunk_analysis.py
import pandas as pd
from typing import List, Dict, Tuple, Any, Set, Optional
import logging
import numpy as np
import math

from src.analysis.number_properties_analysis import analyze_draw_properties

logger = logging.getLogger(__name__)

def get_chunk_definitions(
    total_contests: int,
    chunk_type_from_config: str,
    chunk_sizes_from_config: List[int],
    config: Any
) -> List[Tuple[int, int, str, int]]:
    definitions: List[Tuple[int, int, str, int]] = []
    if not hasattr(config, 'CHUNK_TYPES_CONFIG'):
        logger.error("CHUNK_TYPES_CONFIG não encontrado no config.")
        return definitions

    if not chunk_sizes_from_config:
        logger.warning(f"Nenhum tamanho para tipo de bloco: {chunk_type_from_config}")
        return definitions

    for sz_item in chunk_sizes_from_config:
        if sz_item <= 0:
            logger.warning(f"Tamanho de chunk inválido: {sz_item} para {chunk_type_from_config}. Pulando.")
            continue
        current_pos = 0
        chunk_idx_for_type_size = 0

        while current_pos < total_contests:
            start_contest = current_pos + 1
            end_contest = min(current_pos + sz_item, total_contests)
            chunk_seq_id = chunk_idx_for_type_size + 1
            chunk_label = f"{chunk_type_from_config}_{sz_item}"
            definitions.append((start_contest, end_contest, chunk_label, chunk_seq_id))
            current_pos = end_contest
            chunk_idx_for_type_size +=1
            if start_contest > end_contest :
                logger.warning(f"Chunk inválido start > end ({start_contest} > {end_contest}). Interrompendo.")
                break
    logger.debug(f"Defs de chunk para '{chunk_type_from_config}', tamanhos={chunk_sizes_from_config}: {len(definitions)} blocos.")
    return definitions

def calculate_frequency_in_chunk(df_chunk: pd.DataFrame, config: Any) -> pd.Series:
    if not hasattr(config, 'ALL_NUMBERS') or not hasattr(config, 'BALL_NUMBER_COLUMNS'):
        logger.error("ALL_NUMBERS ou BALL_NUMBER_COLUMNS não em config para calculate_frequency_in_chunk.")
        return pd.Series(dtype='int', index=pd.Index([], name="dezena"), name="frequencia_absoluta")

    if df_chunk.empty:
        return pd.Series(dtype='int').reindex(config.ALL_NUMBERS, fill_value=0).rename("frequencia_absoluta").rename_axis("dezena")

    dezena_cols = config.BALL_NUMBER_COLUMNS
    actual_dezena_cols = [col for col in dezena_cols if col in df_chunk.columns]
    if not actual_dezena_cols:
        logger.warning(f"Nenhuma coluna de bola ({dezena_cols}) no chunk para frequência.")
        return pd.Series(dtype='int').reindex(config.ALL_NUMBERS, fill_value=0).rename("frequencia_absoluta").rename_axis("dezena")

    all_drawn_numbers_in_chunk_list = []
    for col in actual_dezena_cols:
        all_drawn_numbers_in_chunk_list.extend(pd.to_numeric(df_chunk[col], errors='coerce').dropna().astype(int).tolist())

    if not all_drawn_numbers_in_chunk_list:
        return pd.Series(dtype='int').reindex(config.ALL_NUMBERS, fill_value=0).rename("frequencia_absoluta").rename_axis("dezena")

    frequency_series = pd.Series(all_drawn_numbers_in_chunk_list).value_counts()
    frequency_series = frequency_series.reindex(config.ALL_NUMBERS, fill_value=0)
    frequency_series.name = "frequencia_absoluta"
    frequency_series.index.name = "dezena"
    return frequency_series.astype(int)


def get_draw_matrix_for_chunk(df_chunk: pd.DataFrame, chunk_start_contest: int, chunk_end_contest: int, config: Any) -> pd.DataFrame:
    default_cols = getattr(config, 'ALL_NUMBERS', list(range(1,26)))
    contest_id_col_name = getattr(config, 'CONTEST_ID_COLUMN_NAME', 'contest_id')

    if not hasattr(config, 'BALL_NUMBER_COLUMNS'):
        logger.error("BALL_NUMBER_COLUMNS não em config para get_draw_matrix_for_chunk.")
        return pd.DataFrame(columns=default_cols)

    all_contests_in_chunk_range = pd.Index(range(chunk_start_contest, chunk_end_contest + 1), name=contest_id_col_name)

    if df_chunk.empty:
        logger.debug(f"Chunk C{chunk_start_contest}-C{chunk_end_contest} vazio. Retornando matriz de zeros.")
        return pd.DataFrame(0, index=all_contests_in_chunk_range, columns=config.ALL_NUMBERS, dtype=int)

    dezena_cols = config.BALL_NUMBER_COLUMNS
    actual_dezena_cols = [col for col in dezena_cols if col in df_chunk.columns]
    if not actual_dezena_cols:
        logger.warning(f"Nenhuma coluna de bola ({dezena_cols}) em df_chunk para C{chunk_start_contest}-C{chunk_end_contest}")
        return pd.DataFrame(0, index=all_contests_in_chunk_range, columns=config.ALL_NUMBERS, dtype=int)

    if contest_id_col_name not in df_chunk.columns:
        logger.error(f"Coluna '{contest_id_col_name}' ausente em df_chunk para C{chunk_start_contest}-C{chunk_end_contest}")
        return pd.DataFrame(0, index=all_contests_in_chunk_range, columns=config.ALL_NUMBERS, dtype=int)

    df_chunk_copy = df_chunk[[contest_id_col_name] + actual_dezena_cols].copy()
    try:
        df_chunk_copy[contest_id_col_name] = pd.to_numeric(df_chunk_copy[contest_id_col_name])
    except Exception as e:
        logger.error(f"Não converter '{contest_id_col_name}' para numérico: {e}")
        return pd.DataFrame(0, index=all_contests_in_chunk_range, columns=config.ALL_NUMBERS, dtype=int)

    melted_df = df_chunk_copy.melt(id_vars=[contest_id_col_name], value_vars=actual_dezena_cols, value_name='Dezena_val_temp')
    melted_df.dropna(subset=['Dezena_val_temp'], inplace=True)
    if melted_df.empty:
        return pd.DataFrame(0, index=all_contests_in_chunk_range, columns=config.ALL_NUMBERS, dtype=int)

    melted_df['Dezena'] = pd.to_numeric(melted_df['Dezena_val_temp'], errors='coerce')
    melted_df.dropna(subset=['Dezena'], inplace=True)
    if melted_df.empty:
        return pd.DataFrame(0, index=all_contests_in_chunk_range, columns=config.ALL_NUMBERS, dtype=int)
    melted_df['Dezena'] = melted_df['Dezena'].astype(int)
    melted_df['presente'] = 1

    try:
        draw_matrix = melted_df.pivot_table(index=contest_id_col_name, columns='Dezena', values='presente', fill_value=0)
        draw_matrix = draw_matrix.reindex(index=all_contests_in_chunk_range, columns=config.ALL_NUMBERS, fill_value=0)
    except Exception as e:
        logger.error(f"Erro ao pivotar draw_matrix no chunk C{chunk_start_contest}-C{chunk_end_contest}: {e}")
        return pd.DataFrame(0, index=all_contests_in_chunk_range, columns=config.ALL_NUMBERS, dtype=int)
    return draw_matrix.astype(int)

def calculate_delays_for_matrix(draw_matrix: pd.DataFrame, chunk_start_contest: int, chunk_end_contest: int, config: Any) -> Dict[str, pd.Series]:
    chunk_duration_calc = chunk_end_contest - chunk_start_contest + 1
    results: Dict[str, pd.Series] = {
        "final": pd.Series(chunk_duration_calc, index=config.ALL_NUMBERS, dtype='Int64', name="atraso_final_no_bloco"),
        "mean": pd.Series(np.nan, index=config.ALL_NUMBERS, dtype='float', name="atraso_medio_no_bloco"),
        "max": pd.Series(chunk_duration_calc, index=config.ALL_NUMBERS, dtype='Int64', name="atraso_maximo_no_bloco"),
        "std_dev": pd.Series(np.nan, index=config.ALL_NUMBERS, dtype='float', name="delay_std_dev")
    }
    if draw_matrix.empty:
        logger.debug(f"Matriz de sorteios vazia para atrasos C{chunk_start_contest}-C{chunk_end_contest}.")
        return results

    draw_matrix.index = pd.to_numeric(draw_matrix.index)
    draw_matrix = draw_matrix.sort_index()

    if not draw_matrix.index.unique().tolist(): # Verificação se há algum índice após conversão
        return results

    for dezena_val_loop in config.ALL_NUMBERS:
        if dezena_val_loop not in draw_matrix.columns:
            logger.debug(f"Dezena {dezena_val_loop} não na draw_matrix para C{chunk_start_contest}-C{chunk_end_contest}.")
            continue

        col_dezena = draw_matrix[dezena_val_loop]
        occurrence_contests = col_dezena[col_dezena == 1].index.sort_values().tolist()

        if not occurrence_contests:
            continue # Atraso final e máximo permanecem como chunk_duration_calc

        results["final"].loc[dezena_val_loop] = chunk_end_contest - occurrence_contests[-1]

        gaps: List[int] = [occurrence_contests[0] - chunk_start_contest]
        for i in range(len(occurrence_contests) - 1):
            gaps.append(occurrence_contests[i+1] - occurrence_contests[i] - 1)
        gaps.append(chunk_end_contest - occurrence_contests[-1])

        if gaps:
            results["mean"].loc[dezena_val_loop] = np.mean(gaps) if gaps else np.nan
            results["max"].loc[dezena_val_loop] = max(gaps) if gaps else chunk_duration_calc
            current_gaps_std = pd.Series(gaps, dtype=float).std(ddof=0) if len(gaps) > 1 else np.nan
            results["std_dev"].loc[dezena_val_loop] = current_gaps_std
    return results

def calculate_block_group_summary_metrics(df_chunk: pd.DataFrame, config: Any) -> Dict[str, Optional[float]]:
    summary_metrics: Dict[str, Optional[float]] = {
        "avg_pares_no_bloco": None, "avg_impares_no_bloco": None,
        "avg_primos_no_bloco": None, "avg_soma_dezenas_no_bloco": None
    }
    if df_chunk.empty or not hasattr(config, 'BALL_NUMBER_COLUMNS') or not hasattr(config, 'NUMBERS_PER_DRAW'):
        return summary_metrics

    ball_cols = config.BALL_NUMBER_COLUMNS
    actual_ball_cols = [col for col in ball_cols if col in df_chunk.columns]
    if not actual_ball_cols: return summary_metrics

    contest_properties_list: List[Dict[str, Any]] = []
    for _, row in df_chunk.iterrows():
        try:
            draw_numbers_raw = [row[col] for col in actual_ball_cols if pd.notna(row[col])]
            draw = [int(float(d_num)) for d_num in draw_numbers_raw if str(d_num).replace('.0','',1).isdigit()]

            if len(draw) == config.NUMBERS_PER_DRAW:
                properties = analyze_draw_properties(draw, config)
                contest_properties_list.append(properties)
            elif len(draw_numbers_raw) > 0:
                 logger.debug(f"Sorteio incompleto no concurso {row.get(config.CONTEST_ID_COLUMN_NAME, 'N/A')} (bloco). Dezenas: {draw_numbers_raw}.")
        except ValueError as e_val:
            logger.warning(f"Erro converter dezenas para concurso {row.get(config.CONTEST_ID_COLUMN_NAME, 'Desconhecido')} (grupo): {e_val}.")
            continue
        except Exception as e_gen:
             logger.error(f"Erro inesperado em group_summary para concurso {row.get(config.CONTEST_ID_COLUMN_NAME, 'Desconhecido')}: {e_gen}")
             continue

    if not contest_properties_list: return summary_metrics
    df_contest_properties = pd.DataFrame(contest_properties_list)

    property_to_final_col_map = {
        'pares': 'avg_pares_no_bloco', 'impares': 'avg_impares_no_bloco',
        'primos': 'avg_primos_no_bloco', 'soma_dezenas': 'avg_soma_dezenas_no_bloco'
    }
    for prop_key, final_col_name in property_to_final_col_map.items():
        if prop_key in df_contest_properties.columns and not df_contest_properties[prop_key].empty:
            mean_val = df_contest_properties[prop_key].mean()
            summary_metrics[final_col_name] = round(mean_val, 2) if pd.notna(mean_val) else None
    return summary_metrics

def calculate_chunk_metrics_and_persist(all_data_df: pd.DataFrame, db_manager: Any, config: Any):
    logger.info("Iniciando cálculo e persistência de métricas de chunk.")
    contest_col = config.CONTEST_ID_COLUMN_NAME

    if contest_col not in all_data_df.columns:
        logger.error(f"Coluna '{contest_col}' não em all_data_df para chunk_metrics. Abortando.")
        return

    df_to_process = all_data_df.copy()
    try:
        df_to_process[contest_col] = pd.to_numeric(df_to_process[contest_col], errors='coerce')
        df_to_process.dropna(subset=[contest_col], inplace=True)
        if df_to_process.empty:
            logger.error("DataFrame vazio após limpar coluna de concurso. Abortando.")
            return
        df_to_process[contest_col] = df_to_process[contest_col].astype(int)
        if df_to_process[contest_col].empty:
             logger.error("Coluna de concurso vazia após conversão. Abortando.")
             return
        total_contests = df_to_process[contest_col].max()
    except Exception as e_conv:
        logger.error(f"Erro ao processar '{contest_col}': {e_conv}. Abortando.")
        return

    if pd.isna(total_contests) or total_contests <= 0:
        logger.error(f"Total de concursos inválido: {total_contests}. Abortando.")
        return

    for chunk_type_key, list_of_sizes in config.CHUNK_TYPES_CONFIG.items():
        for size_val_loop in list_of_sizes:
            logger.info(f"Processando chunks: tipo='{chunk_type_key}', tamanho={size_val_loop}.")
            chunk_definitions = get_chunk_definitions(int(total_contests), chunk_type_key, [size_val_loop], config)

            if not chunk_definitions:
                logger.warning(f"Nenhuma definição de chunk para {chunk_type_key}_{size_val_loop}."); continue

            all_metrics_for_db: List[Dict[str, Any]] = []
            all_group_metrics_for_db: List[Dict[str, Any]] = []

            for start_contest, end_contest, chunk_label_original, chunk_seq_id_val in chunk_definitions:
                mask = (df_to_process[contest_col] >= start_contest) & (df_to_process[contest_col] <= end_contest)
                df_current_chunk = df_to_process[mask]
                if df_current_chunk.empty:
                    logger.debug(f"Chunk C{start_contest}-C{end_contest} (SeqID {chunk_seq_id_val}) vazio. Pulando.")
                    continue
                chunk_actual_duration = end_contest - start_contest + 1
                frequency_series = calculate_frequency_in_chunk(df_current_chunk, config)
                draw_matrix_chunk = get_draw_matrix_for_chunk(df_current_chunk, start_contest, end_contest, config)
                delay_metrics_dict = calculate_delays_for_matrix(draw_matrix_chunk, start_contest, end_contest, config)
                for dezena_val_loop in config.ALL_NUMBERS:
                    frequencia_abs_dezena = frequency_series.get(dezena_val_loop, 0)
                    freq_rel_no_chunk = frequencia_abs_dezena / chunk_actual_duration if chunk_actual_duration > 0 else 0.0
                    occurrence_std_dev_val = math.sqrt(freq_rel_no_chunk * (1 - freq_rel_no_chunk)) if 0 < freq_rel_no_chunk < 1 else 0.0
                    all_metrics_for_db.append({
                        'chunk_seq_id': chunk_seq_id_val, 'chunk_start_contest': start_contest,
                        'chunk_end_contest': end_contest, 'dezena': int(dezena_val_loop),
                        'frequencia_absoluta': int(frequencia_abs_dezena),
                        'atraso_medio_no_bloco': float(delay_metrics_dict["mean"].get(dezena_val_loop, np.nan)) if pd.notna(delay_metrics_dict["mean"].get(dezena_val_loop, np.nan)) else None,
                        'atraso_maximo_no_bloco': int(delay_metrics_dict["max"].get(dezena_val_loop, chunk_actual_duration)) if pd.notna(delay_metrics_dict["max"].get(dezena_val_loop, chunk_actual_duration)) else None,
                        'atraso_final_no_bloco': int(delay_metrics_dict["final"].get(dezena_val_loop, chunk_actual_duration)) if pd.notna(delay_metrics_dict["final"].get(dezena_val_loop, chunk_actual_duration)) else None,
                        'occurrence_std_dev': round(occurrence_std_dev_val, 6) if pd.notna(occurrence_std_dev_val) else None,
                        'delay_std_dev': round(float(delay_metrics_dict["std_dev"].get(dezena_val_loop, np.nan)), 6) if pd.notna(delay_metrics_dict["std_dev"].get(dezena_val_loop, np.nan)) else None
                    })
                group_metrics = calculate_block_group_summary_metrics(df_current_chunk, config)
                all_group_metrics_for_db.append({'chunk_seq_id': chunk_seq_id_val, 'chunk_start_contest': start_contest, 'chunk_end_contest': end_contest, **group_metrics})

            if all_metrics_for_db:
                metrics_df_long_combined = pd.DataFrame(all_metrics_for_db)
                base_cols_dezena = ['chunk_seq_id', 'chunk_start_contest', 'chunk_end_contest', 'dezena']
                metrics_to_save_map = {
                    config.EVOL_METRIC_FREQUENCY_BLOCK_PREFIX: "frequencia_absoluta",
                    config.EVOL_METRIC_ATRASO_MEDIO_BLOCK_PREFIX: "atraso_medio_no_bloco",
                    config.EVOL_METRIC_ATRASO_MAXIMO_BLOCK_PREFIX: "atraso_maximo_no_bloco",
                    config.EVOL_METRIC_ATRASO_FINAL_BLOCK_PREFIX: "atraso_final_no_bloco",
                    config.EVOL_METRIC_OCCURRENCE_STD_DEV_BLOCK_PREFIX: "occurrence_std_dev",
                    config.EVOL_METRIC_DELAY_STD_DEV_BLOCK_PREFIX: "delay_std_dev"
                }
                for table_prefix_from_config, value_col_name_in_df in metrics_to_save_map.items():
                    if value_col_name_in_df in metrics_df_long_combined.columns:
                        df_to_save_metric = metrics_df_long_combined[base_cols_dezena + [value_col_name_in_df]].copy()
                        if pd.api.types.is_float_dtype(df_to_save_metric[value_col_name_in_df]):
                             df_to_save_metric.dropna(subset=[value_col_name_in_df], inplace=True)
                        if not df_to_save_metric.empty:
                            table_name = f"{table_prefix_from_config}_{chunk_type_key}_{size_val_loop}"
                            # CORREÇÃO APLICADA: removido index=False
                            db_manager.save_dataframe(df_to_save_metric, table_name, if_exists='replace')
                            logger.info(f"Métricas ({value_col_name_in_df}) salvas em '{table_name}'. {len(df_to_save_metric)} regs.")
                        else:
                            logger.debug(f"Nenhum dado para métrica '{value_col_name_in_df}' no chunk {chunk_type_key}_{size_val_loop}.")
            if all_group_metrics_for_db:
                group_metrics_df = pd.DataFrame(all_group_metrics_for_db)
                cols_to_check_for_nan_group = [col for col in group_metrics_df.columns if col.startswith('avg_')]
                if cols_to_check_for_nan_group :
                     group_metrics_df.dropna(subset=cols_to_check_for_nan_group, how='all', inplace=True)
                if not group_metrics_df.empty:
                    group_table_name = f"{config.EVOL_BLOCK_GROUP_METRICS_PREFIX}_{chunk_type_key}_{size_val_loop}"
                    # CORREÇÃO APLICADA: removido index=False
                    db_manager.save_dataframe(group_metrics_df, group_table_name, if_exists='replace')
                    logger.info(f"Métricas de grupo de chunk salvas em '{group_table_name}'. {len(group_metrics_df)} regs.")
                else:
                    logger.info(f"Nenhuma métrica de grupo de chunk para {chunk_type_key}_{size_val_loop}.")
    logger.info("Cálculo e persistência de métricas de chunk concluído.")

--------------------------------------------------------------------------------
# Arquivo: src/analysis/combination_analysis.py
--------------------------------------------------------------------------------
# Lotofacil_Analysis/src/analysis/combination_analysis.py
import logging
import pandas as pd
from itertools import combinations
from collections import Counter
from typing import List, Dict, Any, Set, Tuple, Union 

from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules

logger = logging.getLogger(__name__)

class CombinationAnalyzer:
    def __init__(self, all_numbers: List[int]):
        self.all_numbers = sorted(all_numbers)
        logger.debug("CombinationAnalyzer instanciado com %d números.", len(self.all_numbers))

    def _format_frozenset_to_str(self, itemset_frozenset: frozenset) -> str:
        return "-".join(map(str, sorted([int(item) for item in itemset_frozenset])))

    def analyze_pairs(
        self, 
        all_draws_df: pd.DataFrame, 
        drawn_numbers_col: str = 'drawn_numbers',
        contest_id_col: str = 'contest_id'
    ) -> pd.DataFrame:
        # ... (código existente do analyze_pairs, sem alterações) ...
        logger.info("Iniciando análise de pares.")
        
        if drawn_numbers_col not in all_draws_df.columns:
            msg = f"Coluna '{drawn_numbers_col}' não encontrada no DataFrame de sorteios."
            logger.error(msg)
            raise ValueError(msg)
        if contest_id_col not in all_draws_df.columns:
            msg = f"Coluna '{contest_id_col}' não encontrada no DataFrame de sorteios."
            logger.error(msg)
            raise ValueError(msg)

        pair_counts: Counter[Tuple[int, int]] = Counter()
        last_occurrence_pair: Dict[Tuple[int, int], int] = {}
        
        draws_data = all_draws_df[[contest_id_col, drawn_numbers_col]].copy()
        try:
            draws_data[contest_id_col] = draws_data[contest_id_col].astype(int)
            draws_data[drawn_numbers_col] = draws_data[drawn_numbers_col].apply(
                lambda x: sorted([int(n) for n in x]) if isinstance(x, (list, set, tuple)) else []
            )
        except Exception as e:
            logger.error(f"Erro ao converter colunas para o tipo esperado: {e}")
            raise

        max_contest_id_in_history = draws_data[contest_id_col].max() if not draws_data.empty else 0

        for _, row in draws_data.iterrows():
            contest_id = row[contest_id_col]
            drawn_numbers = row[drawn_numbers_col]
            
            if len(drawn_numbers) < 2:
                continue

            for pair_tuple in combinations(drawn_numbers, 2):
                pair_counts[pair_tuple] += 1
                last_occurrence_pair[pair_tuple] = contest_id
        
        all_possible_pairs = list(combinations(self.all_numbers, 2))
        
        results = []
        for pair_tuple in all_possible_pairs:
            pair_str = f"{pair_tuple[0]}-{pair_tuple[1]}"
            frequency = pair_counts.get(pair_tuple, 0)
            last_contest = last_occurrence_pair.get(pair_tuple, 0)
            
            if frequency > 0:
                current_delay = max_contest_id_in_history - last_contest
            else:
                current_delay = max_contest_id_in_history 

            results.append({
                'pair_str': pair_str,
                'frequency': frequency,
                'last_contest': last_contest,
                'current_delay': current_delay
            })
            
        pairs_df = pd.DataFrame(results)
        pairs_df = pairs_df.sort_values(by=['frequency', 'current_delay'], ascending=[False, True]).reset_index(drop=True)
        
        logger.info(f"Análise de pares concluída. {len(pairs_df)} pares processados.")
        return pairs_df

    def analyze_frequent_itemsets(
        self, 
        all_draws_df: pd.DataFrame, 
        min_support: float, 
        min_len: int = 3, 
        max_len: int = 10,
        drawn_numbers_col: str = 'drawn_numbers'
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Analisa os sorteios para encontrar conjuntos frequentes de dezenas (itemsets)
        usando o algoritmo Apriori.

        Retorna dois DataFrames:
        1. df_for_db: Formatado para salvar no banco de dados (com 'itemset_str'),
                      filtrado por min_len e max_len.
        2. frequent_itemsets_raw_mlxtend: Contém 'itemsets' como frozensets e 'support',
                                         resultado direto do apriori, *antes* da filtragem
                                         por min_len/max_len, para ser usado na geração de regras.
        """
        logger.info(f"Iniciando análise de itemsets frequentes com min_support={min_support}, min_len={min_len}, max_len={max_len}")

        empty_cols_db = ['itemset_str', 'support', 'length', 'frequency_count']
        empty_cols_rules_lookup = ['support', 'itemsets'] # Colunas típicas do apriori

        if drawn_numbers_col not in all_draws_df.columns:
            msg = f"Coluna '{drawn_numbers_col}' não encontrada no DataFrame de sorteios."
            logger.error(msg)
            raise ValueError(msg)
        
        if all_draws_df.empty:
            logger.warning("DataFrame de sorteios está vazio. Retornando DataFrames de itemsets vazios.")
            return pd.DataFrame(columns=empty_cols_db), pd.DataFrame(columns=empty_cols_rules_lookup)

        try:
            transactions = all_draws_df[drawn_numbers_col].apply(
                lambda x: [int(n) for n in x] if isinstance(x, (list, set, tuple)) and x else []
            ).tolist()
        except Exception as e:
            logger.error(f"Erro ao processar a coluna '{drawn_numbers_col}': {e}")
            raise

        if not any(transactions):
            logger.warning("Nenhuma dezena encontrada nas transações. Retornando DataFrames de itemsets vazios.")
            return pd.DataFrame(columns=empty_cols_db), pd.DataFrame(columns=empty_cols_rules_lookup)

        te = TransactionEncoder()
        try:
            te_ary = te.fit_transform(transactions)
        except ValueError as e:
            logger.error(f"Erro no TransactionEncoder: {e}")
            logger.error(f"Primeiras 5 transações: {transactions[:5]}")
            return pd.DataFrame(columns=empty_cols_db), pd.DataFrame(columns=empty_cols_rules_lookup)
            
        df_one_hot = pd.DataFrame(te_ary, columns=te.columns_)
        logger.debug(f"DataFrame one-hot criado com {df_one_hot.shape[0]} transações e {df_one_hot.shape[1]} itens.")

        if df_one_hot.empty:
            logger.warning("DataFrame one-hot está vazio. Retornando DataFrames de itemsets vazios.")
            return pd.DataFrame(columns=empty_cols_db), pd.DataFrame(columns=empty_cols_rules_lookup)

        # DataFrame bruto do apriori - este será usado para gerar as regras
        frequent_itemsets_raw_mlxtend = apriori(df_one_hot, min_support=min_support, use_colnames=True, verbose=0)
        logger.debug(f"Apriori encontrou {len(frequent_itemsets_raw_mlxtend)} itemsets frequentes (bruto).")

        if frequent_itemsets_raw_mlxtend.empty:
            logger.info("Nenhum itemset frequente encontrado com o min_support fornecido (bruto).")
            return pd.DataFrame(columns=empty_cols_db), frequent_itemsets_raw_mlxtend.copy()

        # Prepara o DataFrame para o banco de dados (df_for_db)
        # Faz uma cópia para não alterar o DataFrame bruto que será usado para as regras
        df_to_filter_for_db = frequent_itemsets_raw_mlxtend.copy()
        df_to_filter_for_db['length'] = df_to_filter_for_db['itemsets'].apply(lambda x: len(x))

        filtered_itemsets_df = df_to_filter_for_db[
            (df_to_filter_for_db['length'] >= min_len) &
            (df_to_filter_for_db['length'] <= max_len)
        ].copy()
        logger.debug(f"Encontrados {len(filtered_itemsets_df)} itemsets para DB após filtragem por tamanho ({min_len}-{max_len}).")

        if filtered_itemsets_df.empty:
            logger.info(f"Nenhum itemset frequente encontrado com tamanho entre {min_len} e {max_len} para o formato de DB.")
            df_for_db = pd.DataFrame(columns=empty_cols_db)
        else:
            total_contests = len(all_draws_df)
            df_for_db = filtered_itemsets_df.copy()
            df_for_db['frequency_count'] = (df_for_db['support'] * total_contests).round().astype(int)
            df_for_db['itemset_str'] = df_for_db['itemsets'].apply(self._format_frozenset_to_str)
            
            df_for_db = df_for_db[['itemset_str', 'support', 'length', 'frequency_count']]
            df_for_db = df_for_db.sort_values(by=['length', 'support'], ascending=[True, False]).reset_index(drop=True)
        
        logger.info(f"Análise de itemsets frequentes concluída. Retornando {len(df_for_db)} itemsets para DB e {len(frequent_itemsets_raw_mlxtend)} itemsets brutos para regras.")
        
        return df_for_db, frequent_itemsets_raw_mlxtend # Retorna o formatado e o bruto


    def generate_association_rules(
        self,
        frequent_itemsets_mlxtend_df: pd.DataFrame, 
        metric: str = "confidence",
        min_threshold: float = 0.5,
        min_lift: float = 0.0
    ) -> pd.DataFrame:
        """
        Gera regras de associação a partir de um DataFrame de itemsets frequentes (formato mlxtend).
        """
        logger.info(f"Gerando regras de associação com métrica='{metric}', limiar={min_threshold}, min_lift={min_lift}")

        if frequent_itemsets_mlxtend_df.empty:
            logger.warning("DataFrame de itemsets frequentes (formato mlxtend) está vazio. Nenhuma regra será gerada.")
            return pd.DataFrame()
        
        if 'itemsets' not in frequent_itemsets_mlxtend_df.columns or 'support' not in frequent_itemsets_mlxtend_df.columns:
            logger.error("DataFrame de itemsets frequentes (formato mlxtend) não contém as colunas 'itemsets' ou 'support'.")
            return pd.DataFrame()

        try:
            # A função association_rules precisa dos itemsets que atendem ao min_support,
            # incluindo aqueles de tamanho menor que min_len, para calcular corretamente os suportes
            # dos antecedentes/consequentes.
            rules_df = association_rules(
                frequent_itemsets_mlxtend_df, # Usa o DataFrame bruto do apriori
                metric=metric, 
                min_threshold=min_threshold
            )
        except KeyError as ke:
             # Este erro pode acontecer se, mesmo passando o DF bruto do apriori, algum subconjunto necessário
             # para uma regra não tiver sido encontrado pelo apriori (talvez por um min_support muito alto para ele).
             # Ou se o DataFrame não estiver como esperado.
            logger.error(f"KeyError ao gerar regras de associação com mlxtend: {ke}", exc_info=True)
            logger.error(f"Primeiras linhas do DataFrame de itemsets passado para association_rules:\n{frequent_itemsets_mlxtend_df.head()}")
            return pd.DataFrame()
        except Exception as e:
            logger.error(f"Erro inesperado ao gerar regras de associação com mlxtend: {e}", exc_info=True)
            return pd.DataFrame()

        if rules_df.empty:
            logger.info(f"Nenhuma regra de associação encontrada com os critérios: métrica='{metric}', limiar={min_threshold}.")
            return rules_df
        
        logger.info(f"Geradas {len(rules_df)} regras antes da filtragem por lift.")

        if min_lift > 0.0: # Aplica filtro de lift se min_lift for maior que 0
            rules_df = rules_df[rules_df['lift'] >= min_lift].copy() # .copy() para evitar SettingWithCopyWarning
            logger.info(f"{len(rules_df)} regras restantes após filtro de lift >= {min_lift}.")

        if rules_df.empty:
            logger.info(f"Nenhuma regra restante após filtro de lift.")
            return rules_df

        rules_df['antecedents_str'] = rules_df['antecedents'].apply(self._format_frozenset_to_str)
        rules_df['consequents_str'] = rules_df['consequents'].apply(self._format_frozenset_to_str)
        
        cols_to_keep = [
            'antecedents_str', 'consequents_str', 
            'antecedent support', 'consequent support', 'support', 
            'confidence', 'lift', 'leverage', 'conviction'
        ]
        final_cols = [col for col in cols_to_keep if col in rules_df.columns]
        
        rules_df = rules_df[final_cols]
        rules_df = rules_df.sort_values(by=['lift', 'confidence', 'support'], ascending=[False, False, False]).reset_index(drop=True)

        logger.info(f"Geração de regras de associação concluída. {len(rules_df)} regras finais.")
        return rules_df

--------------------------------------------------------------------------------
# Arquivo: src/analysis/cycle_analysis.py
--------------------------------------------------------------------------------
# src/analysis/cycle_analysis.py
import pandas as pd
from typing import List, Dict, Tuple, Set, Optional, Any
import logging
import numpy as np

# from src.config import ALL_NUMBERS # Acessado via config
from src.analysis.chunk_analysis import (
    calculate_frequency_in_chunk, # Já espera config
    get_draw_matrix_for_chunk,    # Já espera config
    calculate_delays_for_matrix   # Já espera config
)
from src.analysis.number_properties_analysis import analyze_draw_properties 

logger = logging.getLogger(__name__)
# ALL_NUMBERS_SET será definido usando config.ALL_NUMBERS quando config estiver disponível

def identify_and_process_cycles(all_data_df: pd.DataFrame, config: Any) -> Dict[str, Optional[pd.DataFrame]]:
    logger.info("Iniciando análise completa de ciclos.")
    results: Dict[str, Optional[pd.DataFrame]] = {'ciclos_detalhe': None, 'ciclos_sumario_estatisticas': None}
    if all_data_df is None or all_data_df.empty: # ... (como antes)
        return results

    contest_col = config.CONTEST_ID_COLUMN_NAME
    ball_cols = config.BALL_NUMBER_COLUMNS
    current_all_numbers_set = set(config.ALL_NUMBERS)

    required_cols = [contest_col] + ball_cols
    missing_cols = [col for col in required_cols if col not in all_data_df.columns]
    if missing_cols: # ... (como antes)
        return results

    df_sorted = all_data_df.copy()
    try:
        df_sorted[contest_col] = pd.to_numeric(df_sorted[contest_col])
    except Exception as e: # ... (como antes)
        return results
    df_sorted = df_sorted.sort_values(by=contest_col).reset_index(drop=True)
    # ... (resto da lógica de identify_and_process_cycles como na sua versão mais recente/corrigida anteriormente) ...
    # ... garantindo que use config.CONTEST_ID_COLUMN_NAME, config.BALL_NUMBER_COLUMNS, config.ALL_NUMBERS ...
    cycles_data: List[Dict[str, Any]] = []
    current_cycle_numbers_needed = current_all_numbers_set.copy()
    current_cycle_start_contest = int(df_sorted.loc[0, contest_col]) if not df_sorted.empty else 0
    cycle_count = 0
    last_processed_contest_num_for_open_cycle = int(df_sorted[contest_col].max()) if not df_sorted.empty else 0
    for index, row in df_sorted.iterrows():
        contest_number = int(row[contest_col])
        try:
            drawn_numbers_in_this_contest = set(int(row[col_b]) for col_b in ball_cols if pd.notna(row[col_b]))
        except ValueError: logger.warning(f"Erro converter dezenas concurso {contest_number}."); continue
        if current_cycle_numbers_needed == current_all_numbers_set and contest_number != current_cycle_start_contest:
             current_cycle_start_contest = contest_number
        current_cycle_numbers_needed.difference_update(drawn_numbers_in_this_contest)
        if not current_cycle_numbers_needed: 
            cycle_count += 1
            cycles_data.append({'ciclo_num': cycle_count, 'concurso_inicio': current_cycle_start_contest, 'concurso_fim': contest_number, 'duracao_concursos': contest_number - current_cycle_start_contest + 1, 'numeros_faltantes': None, 'qtd_faltantes': 0})
            current_cycle_numbers_needed = current_all_numbers_set.copy()
            if index + 1 < len(df_sorted): current_cycle_start_contest = int(df_sorted.loc[index + 1, contest_col])
            else: current_cycle_start_contest = contest_number + 1
    if current_cycle_numbers_needed and current_cycle_numbers_needed != current_all_numbers_set:
        if not df_sorted.empty and current_cycle_start_contest <= last_processed_contest_num_for_open_cycle:
            cycles_data.append({'ciclo_num': cycle_count + 1, 'concurso_inicio': current_cycle_start_contest, 'concurso_fim': pd.NA, 'duracao_concursos': pd.NA, 'numeros_faltantes': ",".join(map(str, sorted(list(current_cycle_numbers_needed)))), 'qtd_faltantes': len(current_cycle_numbers_needed)})
    elif cycle_count == 0 and not df_sorted.empty and current_cycle_numbers_needed and current_cycle_numbers_needed != current_all_numbers_set : 
            cycles_data.append({'ciclo_num': 1, 'concurso_inicio': current_cycle_start_contest, 'concurso_fim': pd.NA, 'duracao_concursos': pd.NA, 'numeros_faltantes': ",".join(map(str, sorted(list(current_cycle_numbers_needed)))), 'qtd_faltantes': len(current_cycle_numbers_needed)})
    if cycles_data:
        df_cycles_detail = pd.DataFrame(cycles_data)
        for col_int in ['concurso_fim', 'duracao_concursos', 'qtd_faltantes', 'ciclo_num', 'concurso_inicio']:
            if col_int in df_cycles_detail.columns: df_cycles_detail[col_int] = pd.to_numeric(df_cycles_detail[col_int], errors='coerce').astype('Int64')
        results['ciclos_detalhe'] = df_cycles_detail
        if 'duracao_concursos' in df_cycles_detail.columns:
            df_closed_cycles = df_cycles_detail[df_cycles_detail['duracao_concursos'].notna()].copy()
            if not df_closed_cycles.empty: 
                df_closed_cycles['duracao_concursos'] = pd.to_numeric(df_closed_cycles['duracao_concursos'])
                summary_stats_data = {'total_ciclos_fechados': int(len(df_closed_cycles)), 'duracao_media_ciclo': float(df_closed_cycles['duracao_concursos'].mean()) if len(df_closed_cycles) > 0 else np.nan, 'duracao_min_ciclo': int(df_closed_cycles['duracao_concursos'].min()) if len(df_closed_cycles) > 0 else pd.NA, 'duracao_max_ciclo': int(df_closed_cycles['duracao_concursos'].max()) if len(df_closed_cycles) > 0 else pd.NA, 'duracao_mediana_ciclo': float(df_closed_cycles['duracao_concursos'].median()) if len(df_closed_cycles) > 0 else np.nan}
                df_summary = pd.DataFrame([summary_stats_data])
                for col_int_sum in ['total_ciclos_fechados', 'duracao_min_ciclo', 'duracao_max_ciclo']:
                     if col_int_sum in df_summary.columns: df_summary[col_int_sum] = df_summary[col_int_sum].astype('Int64')
                results['ciclos_sumario_estatisticas'] = df_summary
    logger.info("Análise de identificação de ciclos e sumário concluída.")
    return results

def calculate_detailed_metrics_per_closed_cycle(
    all_data_df: pd.DataFrame, 
    df_ciclos_detalhe: Optional[pd.DataFrame],
    config: Any 
) -> Dict[str, Optional[pd.DataFrame]]:
    logger.info("Iniciando cálculo de métricas detalhadas por dezena/ciclo.")
    results_data_lists: Dict[str, List[Dict[str, Any]]] = {"frequency": [], "mean_delay": [], "max_delay": [], "final_delay": [], "rank_frequency": [], "group_metrics_cycle": []}
    output_dfs: Dict[str, Optional[pd.DataFrame]] = {"ciclo_metric_frequency": None, "ciclo_metric_atraso_medio": None, "ciclo_metric_atraso_maximo": None, "ciclo_metric_atraso_final": None, "ciclo_rank_frequency": None, "ciclo_group_metrics": None}

    if df_ciclos_detalhe is None or df_ciclos_detalhe.empty: return output_dfs
    if all_data_df is None or all_data_df.empty: return output_dfs

    df_closed_cycles = df_ciclos_detalhe[df_ciclos_detalhe['concurso_fim'].notna() & df_ciclos_detalhe['duracao_concursos'].notna() & (pd.to_numeric(df_ciclos_detalhe['duracao_concursos'], errors='coerce') > 0)].copy()
    if df_closed_cycles.empty: return output_dfs
    
    contest_col = config.CONTEST_ID_COLUMN_NAME
    ball_cols = config.BALL_NUMBER_COLUMNS

    for _, cycle_row in df_closed_cycles.iterrows():
        ciclo_num = int(cycle_row['ciclo_num'])
        start_contest = int(cycle_row['concurso_inicio'])
        end_contest = int(cycle_row['concurso_fim'])

        df_current_cycle_contests_filtered = all_data_df.copy()
        try:
            df_current_cycle_contests_filtered[contest_col] = pd.to_numeric(df_current_cycle_contests_filtered[contest_col], errors='coerce')
            df_current_cycle_contests_filtered.dropna(subset=[contest_col], inplace=True)
            df_current_cycle_contests_filtered[contest_col] = df_current_cycle_contests_filtered[contest_col].astype(int)
        except Exception as e_conv_detail:
            logger.error(f"Erro ao converter {contest_col} para int no ciclo {ciclo_num}: {e_conv_detail}")
            continue
        
        mask = (df_current_cycle_contests_filtered[contest_col] >= start_contest) & (df_current_cycle_contests_filtered[contest_col] <= end_contest)
        df_current_cycle_contests = df_current_cycle_contests_filtered[mask]

        chunk_duration = end_contest - start_contest + 1
        default_freq_val = 0; default_delay_float_val = float(chunk_duration); default_delay_int_val = int(chunk_duration); default_rank_val = np.nan; default_avg_group_val = np.nan

        if df_current_cycle_contests.empty: # ... (lógica de defaults como antes) ...
            for d_val in config.ALL_NUMBERS:
                results_data_lists["frequency"].append({'ciclo_num': ciclo_num, 'dezena': d_val, 'frequencia_no_ciclo': default_freq_val})
                results_data_lists["mean_delay"].append({'ciclo_num': ciclo_num, 'dezena': d_val, 'atraso_medio_no_ciclo': default_delay_float_val})
                # ... etc ...
            results_data_lists["group_metrics_cycle"].append({'ciclo_num': ciclo_num, 'avg_pares_no_ciclo': default_avg_group_val, 'avg_impares_no_ciclo': default_avg_group_val, 'avg_primos_no_ciclo': default_avg_group_val})
            continue

        # CORRIGIDO: Passa config para as funções de chunk_analysis
        freq_series = calculate_frequency_in_chunk(df_current_cycle_contests, config) 
        # ... (lógica de rank e freq como antes) ...
        temp_cycle_freq_data_for_rank: List[Dict[str, Any]] = []
        for d_item, v_item in freq_series.items():
            results_data_lists["frequency"].append({'ciclo_num': ciclo_num, 'dezena': int(d_item), 'frequencia_no_ciclo': int(v_item)})
            temp_cycle_freq_data_for_rank.append({'dezena': int(d_item), 'frequencia_no_ciclo': int(v_item)})
        if temp_cycle_freq_data_for_rank:
            df_temp_freq = pd.DataFrame(temp_cycle_freq_data_for_rank)
            if not df_temp_freq.empty and 'frequencia_no_ciclo' in df_temp_freq.columns:
                 df_temp_freq['rank_freq_no_ciclo'] = df_temp_freq['frequencia_no_ciclo'].rank(method='dense', ascending=False).astype(int)
                 for _, rank_row in df_temp_freq.iterrows(): results_data_lists["rank_frequency"].append({'ciclo_num': ciclo_num, 'dezena': int(rank_row['dezena']), 'frequencia_no_ciclo': int(rank_row['frequencia_no_ciclo']), 'rank_freq_no_ciclo': int(rank_row['rank_freq_no_ciclo'])})
            else: 
                for d_val in config.ALL_NUMBERS: results_data_lists["rank_frequency"].append({'ciclo_num': ciclo_num, 'dezena': int(d_val), 'frequencia_no_ciclo': 0, 'rank_freq_no_ciclo': default_rank_val})
        else: 
            for d_val in config.ALL_NUMBERS: results_data_lists["rank_frequency"].append({'ciclo_num': ciclo_num, 'dezena': int(d_val), 'frequencia_no_ciclo': 0, 'rank_freq_no_ciclo': default_rank_val})
        
        draw_matrix_cycle = get_draw_matrix_for_chunk(df_current_cycle_contests, start_contest, end_contest, config) 
        if not draw_matrix_cycle.empty:
            delay_metrics_dict = calculate_delays_for_matrix(draw_matrix_cycle, start_contest, end_contest, config)
            for d_val in config.ALL_NUMBERS: 
                results_data_lists["mean_delay"].append({'ciclo_num': ciclo_num, 'dezena': int(d_val), 'atraso_medio_no_ciclo': float(delay_metrics_dict["mean"].get(d_val, np.nan)) if pd.notna(delay_metrics_dict["mean"].get(d_val, np.nan)) else None})
                results_data_lists["max_delay"].append({'ciclo_num': ciclo_num, 'dezena': int(d_val), 'atraso_maximo_no_ciclo': int(delay_metrics_dict["max"].get(d_val, default_delay_int_val))})
                results_data_lists["final_delay"].append({'ciclo_num': ciclo_num, 'dezena': int(d_val), 'atraso_final_no_ciclo': int(delay_metrics_dict["final"].get(d_val, default_delay_int_val))})
        else: 
            for d_val in config.ALL_NUMBERS: # ... (defaults como antes)
                results_data_lists["mean_delay"].append({'ciclo_num': ciclo_num, 'dezena': int(d_val), 'atraso_medio_no_ciclo': default_delay_float_val})
                # ... etc ...
            
        actual_ball_cols_for_props = [col for col in ball_cols if col in df_current_cycle_contests.columns]
        if not actual_ball_cols_for_props :
            results_data_lists["group_metrics_cycle"].append({'ciclo_num': ciclo_num, 'avg_pares_no_ciclo': default_avg_group_val, 'avg_impares_no_ciclo': default_avg_group_val, 'avg_primos_no_ciclo': default_avg_group_val})
        else:
            cycle_contest_properties_list: List[Dict[str, Any]] = []
            for _, contest_row_in_cycle in df_current_cycle_contests.iterrows():
                try:
                    draw_in_cycle = [int(contest_row_in_cycle[col]) for col in actual_ball_cols_for_props if pd.notna(contest_row_in_cycle[col])]
                    if len(draw_in_cycle) == config.NUMBERS_PER_DRAW: 
                        properties = analyze_draw_properties(draw_in_cycle, config) 
                        cycle_contest_properties_list.append(properties)
                except ValueError: logger.warning(f"Erro converter dezenas concurso {contest_row_in_cycle.get(contest_col, 'Desconhecido')} ciclo {ciclo_num}."); continue
            if cycle_contest_properties_list: # ... (lógica como antes)
                 df_cycle_props = pd.DataFrame(cycle_contest_properties_list)
                 results_data_lists["group_metrics_cycle"].append({
                    'ciclo_num': ciclo_num,
                    'avg_pares_no_ciclo': df_cycle_props['pares'].mean() if 'pares' in df_cycle_props and not df_cycle_props['pares'].empty else default_avg_group_val,
                    'avg_impares_no_ciclo': df_cycle_props['impares'].mean() if 'impares' in df_cycle_props and not df_cycle_props['impares'].empty else default_avg_group_val,
                    'avg_primos_no_ciclo': df_cycle_props['primos'].mean() if 'primos' in df_cycle_props and not df_cycle_props['primos'].empty else default_avg_group_val
                })
            else: results_data_lists["group_metrics_cycle"].append({'ciclo_num': ciclo_num, 'avg_pares_no_ciclo': default_avg_group_val, 'avg_impares_no_ciclo': default_avg_group_val, 'avg_primos_no_ciclo': default_avg_group_val})

    if results_data_lists["frequency"]: output_dfs["ciclo_metric_frequency"] = pd.DataFrame(results_data_lists["frequency"])
    if results_data_lists["mean_delay"]: output_dfs["ciclo_metric_atraso_medio"] = pd.DataFrame(results_data_lists["mean_delay"])
    if results_data_lists["max_delay"]: output_dfs["ciclo_metric_atraso_maximo"] = pd.DataFrame(results_data_lists["max_delay"])
    if results_data_lists["final_delay"]: output_dfs["ciclo_metric_atraso_final"] = pd.DataFrame(results_data_lists["final_delay"])
    if results_data_lists["rank_frequency"]: output_dfs["ciclo_rank_frequency"] = pd.DataFrame(results_data_lists["rank_frequency"])
    if results_data_lists["group_metrics_cycle"]: output_dfs["ciclo_group_metrics"] = pd.DataFrame(results_data_lists["group_metrics_cycle"])
        
    logger.info("Cálculo de métricas detalhadas por dezena/ciclo concluído.")
    return output_dfs

# Wrappers
def identify_cycles(all_data_df: pd.DataFrame, config: Any) -> Optional[pd.DataFrame]:
    logger.info("Chamando identify_cycles (wrapper).")
    results_dict = identify_and_process_cycles(all_data_df, config)
    return results_dict.get('ciclos_detalhe')

def calculate_cycle_stats(df_cycles_detail: Optional[pd.DataFrame], config: Any) -> Optional[pd.DataFrame]:
    logger.info("Chamando calculate_cycle_stats (wrapper).")
    # Esta função pode ser apenas para pegar o sumário do dict retornado por identify_and_process_cycles
    # ou recalcular com base no df_cycles_detail se necessário.
    # Se identify_and_process_cycles já retorna o sumário, o step cycle_stats pode pegar de lá.
    # Vou assumir que ele pega o sumário que já foi calculado.
    if df_cycles_detail is None or df_cycles_detail.empty: return None

    # Re-calcula o sumário a partir dos detalhes do ciclo
    # (Esta lógica está duplicada de identify_and_process_cycles, idealmente seria uma função separada)
    if 'duracao_concursos' not in df_cycles_detail.columns: return None
    df_closed_cycles = df_cycles_detail[pd.to_numeric(df_cycles_detail['duracao_concursos'], errors='coerce').notna()].copy()
    if not df_closed_cycles.empty: 
        df_closed_cycles['duracao_concursos'] = pd.to_numeric(df_closed_cycles['duracao_concursos'])
        if not df_closed_cycles.empty : 
            summary_stats = {'total_ciclos_fechados': int(len(df_closed_cycles)), 'duracao_media_ciclo': float(df_closed_cycles['duracao_concursos'].mean()) if len(df_closed_cycles) > 0 else np.nan, 'duracao_min_ciclo': int(df_closed_cycles['duracao_concursos'].min()) if len(df_closed_cycles) > 0 else pd.NA, 'duracao_max_ciclo': int(df_closed_cycles['duracao_concursos'].max()) if len(df_closed_cycles) > 0 else pd.NA, 'duracao_mediana_ciclo': float(df_closed_cycles['duracao_concursos'].median()) if len(df_closed_cycles) > 0 else np.nan}
            df_summary = pd.DataFrame([summary_stats])
            for col_int_sum in ['total_ciclos_fechados', 'duracao_min_ciclo', 'duracao_max_ciclo']:
                 if col_int_sum in df_summary.columns: df_summary[col_int_sum] = df_summary[col_int_sum].astype('Int64')
            return df_summary
    return None

--------------------------------------------------------------------------------
# Arquivo: src/analysis/cycle_closing_analysis.py
--------------------------------------------------------------------------------
# Lotofacil_Analysis/src/analysis/cycle_closing_analysis.py
import pandas as pd
from collections import Counter
import numpy as np
import logging
from typing import List, Dict, Optional, Tuple, Set, Any

logger = logging.getLogger(__name__)

def _get_draw_numbers_from_row(draw_row: pd.Series, ball_columns: List[str]) -> Set[int]:
    drawn_numbers = set()
    try:
        # Garante que está tentando converter apenas valores que parecem números
        valid_numbers = [int(n) for n in draw_row[ball_columns].dropna().values if str(n).replace('.0','',1).isdigit()]
        drawn_numbers.update(valid_numbers)
    except ValueError:
        logger.warning(f"Valor não numérico encontrado nas dezenas ao processar linha: {draw_row.to_dict()}")
    return drawn_numbers

def get_cycles_df_corrected( # Esta função parece carregar do DB e pode não ser usada se cycles_df vem do shared_context
    db_manager: Any,
    config: Any,
    concurso_maximo: Optional[int] = None
) -> pd.DataFrame:
    cycles_table_name = config.ANALYSIS_CYCLES_DETAIL_TABLE_NAME
    logger.info(f"Buscando dados da tabela de detalhes de ciclos: '{cycles_table_name}'")

    if not db_manager.table_exists(cycles_table_name):
        logger.warning(f"Tabela '{cycles_table_name}' não existe. Retornando DataFrame vazio.")
        return pd.DataFrame()

    ciclo_num_col_in_db = getattr(config, 'CICLO_NUM_COLUMN_NAME', 'ciclo_num')
    # Certifique-se que as colunas na query SELECT correspondem às colunas reais na tabela
    # E que a renomeação para 'duracao' esteja correta se a coluna original é 'duracao_concursos'
    sql_query = f"""
        SELECT 
            {ciclo_num_col_in_db} AS ciclo_num, 
            concurso_inicio, 
            concurso_fim, 
            duracao_concursos -- Mantém o nome original da tabela
        FROM {cycles_table_name}
    """
    params: List[Any] = []
    if concurso_maximo is not None:
        if db_manager.column_exists(cycles_table_name, 'concurso_fim'):
             sql_query += " WHERE concurso_fim <= ?"
             params.append(concurso_maximo)
        else:
            logger.warning(f"Coluna 'concurso_fim' não existe na tabela '{cycles_table_name}' para filtro.")
    sql_query += f" ORDER BY {ciclo_num_col_in_db} ASC;"
    
    df = db_manager.load_dataframe(table_name=cycles_table_name, query=sql_query, params=tuple(params))

    if df.empty:
        logger.info(f"Nenhum ciclo encontrado na '{cycles_table_name}'.")
        return pd.DataFrame()
        
    cols_to_convert = ['ciclo_num', 'concurso_inicio', 'concurso_fim', 'duracao_concursos']
    for col in cols_to_convert:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')
    
    logger.info(f"{len(df)} ciclos lidos da tabela '{cycles_table_name}'.")
    return df

def get_draw_numbers_for_contest(
    db_manager: Any,
    config: Any,
    contest_id: int
) -> Optional[Set[int]]:
    main_draws_table = config.MAIN_DRAWS_TABLE_NAME
    ball_cols_str = ", ".join(config.BALL_NUMBER_COLUMNS)
    query = f"SELECT {ball_cols_str} FROM {main_draws_table} WHERE {config.CONTEST_ID_COLUMN_NAME} = ?"
    df_draw = db_manager.execute_query(query, params=(contest_id,))
    
    if df_draw is not None and not df_draw.empty:
        try:
            draw_series = df_draw.iloc[0]
            drawn_numbers = set(int(n) for n in draw_series.dropna().values if pd.notna(n) and str(n).replace('.0','',1).isdigit())
            return drawn_numbers
        except ValueError as ve:
            logger.error(f"Erro ao converter dezenas para int no concurso {contest_id}: {ve}")
            return None
    logger.warning(f"Nenhum dado encontrado para o concurso {contest_id} ao buscar dezenas.")
    return None

def calculate_closing_number_stats(
    db_manager: Any,
    config: Any,
    cycles_df: pd.DataFrame # Este DF vem do shared_context e tem 'ciclo_num' e 'duracao_concursos'
) -> pd.DataFrame:
    logger.info("Calculando estatísticas de frequência de fechamento de ciclo...")

    default_stats_df = pd.DataFrame({
        'closing_freq': 0,
        'sole_closing_freq': 0
    }, index=pd.Index(config.ALL_NUMBERS, name=config.DEZENA_COLUMN_NAME))

    if cycles_df is None or cycles_df.empty:
        logger.warning("DataFrame de ciclos (cycles_df) está vazio ou é None. Retornando stats default.")
        return default_stats_df

    # CORREÇÃO: Usar 'ciclo_num' e 'duracao_concursos' que são as colunas em cycles_df
    required_cols_in_cycles_df = ['ciclo_num', 'concurso_inicio', 'concurso_fim', 'duracao_concursos']
    missing_cols = [col for col in required_cols_in_cycles_df if col not in cycles_df.columns]
    if missing_cols:
        logger.error(f"DataFrame de ciclos (cycles_df) não possui as colunas: {missing_cols}. Presentes: {cycles_df.columns.tolist()}. Retornando stats default.")
        return default_stats_df

    closed_cycles_df = cycles_df[
        pd.to_numeric(cycles_df['concurso_fim'], errors='coerce').notna() &
        pd.to_numeric(cycles_df['duracao_concursos'], errors='coerce').notna() &
        (pd.to_numeric(cycles_df['duracao_concursos'], errors='coerce') > 0)
    ].copy()

    if closed_cycles_df.empty:
        logger.warning("Nenhum ciclo fechado válido para análise de fechamento. Retornando stats default.")
        return default_stats_df

    df_len = len(closed_cycles_df)
    logger.info(f"Analisando {df_len} ciclos fechados para estatísticas de fechamento...")
    
    closing_counter = Counter()
    sole_closing_counter = Counter()
    processed_valid_cycles = 0
    
    main_draws_table = config.MAIN_DRAWS_TABLE_NAME
    ball_cols_for_query_str = ", ".join(config.BALL_NUMBER_COLUMNS)

    for index, cycle_row in closed_cycles_df.iterrows():
        try:
            cycle_num = int(cycle_row['ciclo_num']) # Usando 'ciclo_num'
            start_c = int(cycle_row['concurso_inicio'])
            end_c = int(cycle_row['concurso_fim'])
        except Exception as e:
            logger.error(f"Erro ao processar dados da linha do ciclo {index} (num: {cycle_row.get('ciclo_num', 'N/A')}): {e}")
            continue
            
        concurso_fim_menos_1 = end_c - 1
        seen_in_cycle_before_closing_draw: Set[int] = set()

        if start_c <= concurso_fim_menos_1:
            query_seen_before = f"""
                SELECT {ball_cols_for_query_str} 
                FROM {main_draws_table} 
                WHERE {config.CONTEST_ID_COLUMN_NAME} BETWEEN ? AND ?
            """
            df_seen_before = db_manager.execute_query(query_seen_before, params=(start_c, concurso_fim_menos_1))
            
            if df_seen_before is not None and not df_seen_before.empty:
                for _, draw_row_seen in df_seen_before.iterrows():
                    seen_in_cycle_before_closing_draw.update(_get_draw_numbers_from_row(draw_row_seen, config.BALL_NUMBER_COLUMNS))
        
        drawn_at_closing_contest = get_draw_numbers_for_contest(db_manager, config, end_c)
        if drawn_at_closing_contest is None:
            logger.warning(f"Não obter dezenas para concurso de fechamento {end_c} do ciclo {cycle_num}. Pulando.")
            continue
            
        missing_for_cycle_closure = set(config.ALL_NUMBERS) - seen_in_cycle_before_closing_draw
        actual_closing_numbers = drawn_at_closing_contest.intersection(missing_for_cycle_closure)
        
        if not actual_closing_numbers:
            logger.warning(
                f"Ciclo {cycle_num} (final {end_c}): Nenhuma dezena de fechamento. "
                f"Faltavam: {sorted(list(missing_for_cycle_closure)) if missing_for_cycle_closure else 'Nenhuma'}. "
                f"Sorteadas em {end_c}: {sorted(list(drawn_at_closing_contest))}."
            )
            if not missing_for_cycle_closure:
                 logger.debug(f"Ciclo {cycle_num}, todas dezenas vistas antes do concurso {end_c}.")
            continue 
        
        closing_counter.update(actual_closing_numbers)
        if len(actual_closing_numbers) == 1:
            sole_closing_counter.update(actual_closing_numbers) 
        
        processed_valid_cycles += 1
        if processed_valid_cycles > 0 and processed_valid_cycles % 20 == 0:
            logger.info(f"{processed_valid_cycles}/{df_len} ciclos processados para análise de fechamento...")

    stats_df_final = pd.DataFrame(index=pd.Index(config.ALL_NUMBERS, name=config.DEZENA_COLUMN_NAME))
    stats_df_final['closing_freq'] = stats_df_final.index.map(closing_counter).fillna(0).astype(int)
    stats_df_final['sole_closing_freq'] = stats_df_final.index.map(sole_closing_counter).fillna(0).astype(int)
    
    logger.info(f"Cálculo de stats de fechamento concluído. {processed_valid_cycles} ciclos válidos analisados.")
    return stats_df_final

--------------------------------------------------------------------------------
# Arquivo: src/analysis/cycle_progression_analysis.py
--------------------------------------------------------------------------------
# src/analysis/cycle_progression_analysis.py
import pandas as pd
from typing import List, Dict, Set, Any, Optional
import logging
import numpy as np

# from src.config import ALL_NUMBERS # Será de config.ALL_NUMBERS
# Também precisará de CONTEST_ID_COLUMN_NAME, DATE_COLUMN_NAME, BALL_NUMBER_COLUMNS

logger = logging.getLogger(__name__)

def calculate_cycle_progression(all_data_df: pd.DataFrame, config: Any) -> Optional[pd.DataFrame]: # Recebe config
    logger.info("Iniciando cálculo da progressão dos ciclos concurso a concurso.")
    if all_data_df is None or all_data_df.empty:
        logger.warning("DataFrame de entrada para calculate_cycle_progression está vazio.")
        return None

    ALL_NUMBERS_SET: Set[int] = set(config.ALL_NUMBERS) # Usa config

    # Usa nomes de colunas do config
    required_cols = [config.CONTEST_ID_COLUMN_NAME, config.DATE_COLUMN_NAME] + config.BALL_NUMBER_COLUMNS
    missing_cols = [col for col in required_cols if col not in all_data_df.columns]
    if missing_cols:
        logger.error(f"Colunas essenciais ausentes no DataFrame: {missing_cols} (Esperado: {required_cols}). Não é possível calcular progressão de ciclo.")
        logger.debug(f"Colunas disponíveis: {all_data_df.columns.tolist()}")
        return None

    df_sorted = all_data_df.sort_values(by=config.CONTEST_ID_COLUMN_NAME).reset_index(drop=True)
    dezena_cols = config.BALL_NUMBER_COLUMNS # Usa config

    progression_data: List[Dict[str, Any]] = []
    current_cycle_numbers_to_find = ALL_NUMBERS_SET.copy()
    current_cycle_num = 1
    
    logger.debug(f"Progressão de ciclo: Iniciando loop por {len(df_sorted)} concursos.")
    for index, row in df_sorted.iterrows():
        contest_number = int(row[config.CONTEST_ID_COLUMN_NAME]) # Usa config
        contest_date = row[config.DATE_COLUMN_NAME] # Usa config
        
        numbers_needed_before_this_draw = current_cycle_numbers_to_find.copy()
        qty_needed_before_this_draw = len(numbers_needed_before_this_draw)

        try:
            drawn_numbers_in_this_contest = set()
            for col in dezena_cols:
                if pd.notna(row[col]):
                    drawn_numbers_in_this_contest.add(int(row[col]))
        except ValueError:
            logger.warning(f"Erro ao converter dezenas para int no concurso {contest_number}. Pulando registro deste concurso.")
            continue
        
        dezenas_sorteadas_str = ",".join(map(str, sorted(list(drawn_numbers_in_this_contest))))
        hit_this_contest = numbers_needed_before_this_draw.intersection(drawn_numbers_in_this_contest)
        hit_this_contest_str = ",".join(map(str, sorted(list(hit_this_contest)))) if hit_this_contest else None
        qty_hit_this_contest = len(hit_this_contest)
        current_cycle_numbers_to_find.difference_update(drawn_numbers_in_this_contest)
        
        numbers_needed_after_this_draw_str = None
        qty_needed_after_this_draw = 0
        cycle_closed_this_contest = False

        if not current_cycle_numbers_to_find:
            cycle_closed_this_contest = True
            numbers_needed_after_this_draw_str = None 
            qty_needed_after_this_draw = 0
        else:
            numbers_needed_after_this_draw_str = ",".join(map(str, sorted(list(current_cycle_numbers_to_find))))
            qty_needed_after_this_draw = len(current_cycle_numbers_to_find)

        progression_data.append({
            config.CONTEST_ID_COLUMN_NAME: contest_number, # Usa config
            config.DATE_COLUMN_NAME: contest_date, # Usa config
            'ciclo_num_associado': current_cycle_num,
            'dezenas_sorteadas_neste_concurso': dezenas_sorteadas_str,
            'numeros_que_faltavam_antes_deste_concurso': ",".join(map(str, sorted(list(numbers_needed_before_this_draw)))) if numbers_needed_before_this_draw else None,
            'qtd_faltavam_antes_deste_concurso': qty_needed_before_this_draw,
            'dezenas_apuradas_neste_concurso': hit_this_contest_str,
            'qtd_apuradas_neste_concurso': qty_hit_this_contest,
            'numeros_faltantes_apos_este_concurso': numbers_needed_after_this_draw_str,
            'qtd_faltantes_apos_este_concurso': qty_needed_after_this_draw,
            'ciclo_fechou_neste_concurso': 1 if cycle_closed_this_contest else 0
        })
        
        if cycle_closed_this_contest:
            logger.debug(f"Ciclo {current_cycle_num} FECHOU no concurso {contest_number}.")
            current_cycle_num += 1
            current_cycle_numbers_to_find = ALL_NUMBERS_SET.copy()

    if not progression_data:
        logger.warning("Nenhum dado de progressão de ciclo foi gerado.")
        return None
        
    df_progression = pd.DataFrame(progression_data)
    # Renomear colunas para nomes padronizados se eles foram usados como chaves de dicionário
    df_progression.rename(columns={
        config.CONTEST_ID_COLUMN_NAME: "Concurso", # Se a tabela final espera "Concurso"
        config.DATE_COLUMN_NAME: "Data"           # Se a tabela final espera "Data"
    }, inplace=True)

    logger.info(f"Cálculo da progressão de ciclo concluído. {len(df_progression)} registros gerados.")
    return df_progression

--------------------------------------------------------------------------------
# Arquivo: src/analysis/delay_analysis.py
--------------------------------------------------------------------------------
# src/analysis/delay_analysis.py
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional 
import logging

logger = logging.getLogger(__name__)

# get_draw_matrix permanece como está, pois será chamada uma vez por iteração no execute_delay.py
def get_draw_matrix(all_data_df: pd.DataFrame, config: Any) -> pd.DataFrame:
    logger.debug("Interno: get_draw_matrix iniciando.")
    if all_data_df.empty or config.CONTEST_ID_COLUMN_NAME not in all_data_df.columns:
        logger.warning("DataFrame de entrada para get_draw_matrix está vazio ou sem coluna de concurso.")
        return pd.DataFrame(columns=config.ALL_NUMBERS) 

    df_for_matrix = all_data_df.copy()
    try:
        df_for_matrix[config.CONTEST_ID_COLUMN_NAME] = pd.to_numeric(df_for_matrix[config.CONTEST_ID_COLUMN_NAME])
    except Exception as e:
        logger.error(f"Não foi possível converter '{config.CONTEST_ID_COLUMN_NAME}' para numérico: {e}")
        return pd.DataFrame(columns=config.ALL_NUMBERS)
        
    df_sorted = df_for_matrix.sort_values(by=config.CONTEST_ID_COLUMN_NAME).set_index(config.CONTEST_ID_COLUMN_NAME)
    
    actual_ball_cols = [col for col in config.BALL_NUMBER_COLUMNS if col in df_sorted.columns]
    if not actual_ball_cols:
        logger.error(f"Nenhuma coluna de bola encontrada em all_data_df para get_draw_matrix.")
        return pd.DataFrame(columns=config.ALL_NUMBERS, index=df_sorted.index if not df_sorted.empty else None)

    draw_matrix_list = []
    for contest_id_loop, row in df_sorted.iterrows(): # Renomeado contest_id para evitar conflito
        try:
            drawn_numbers_for_row = [int(n) for n in row[actual_ball_cols].dropna().unique()]
            drawn_numbers = set(drawn_numbers_for_row)
            contest_presence = {number: 1 if number in drawn_numbers else 0 for number in config.ALL_NUMBERS}
            draw_matrix_list.append(contest_presence)
        except ValueError:
            logger.warning(f"Valor não numérico encontrado nas dezenas do concurso {contest_id_loop}. Adicionando linha de zeros na matriz.")
            contest_presence = {number: 0 for number in config.ALL_NUMBERS} 
            draw_matrix_list.append(contest_presence) # Garante que a lista tenha o mesmo tamanho do índice
            continue 
    
    if not draw_matrix_list: # Deve ser raro se df_sorted não for vazio
        logger.warning("Nenhum dado processado para a matriz de sorteios.")
        return pd.DataFrame(columns=config.ALL_NUMBERS, index=df_sorted.index if not df_sorted.empty else None)

    # Se draw_matrix_list foi populado, seu tamanho deve corresponder a df_sorted.index
    draw_matrix_df = pd.DataFrame(draw_matrix_list, index=df_sorted.index) 
    
    missing_num_cols = set(config.ALL_NUMBERS) - set(draw_matrix_df.columns)
    for col in missing_num_cols:
        draw_matrix_df[col] = 0
    
    logger.debug("Interno: get_draw_matrix concluído.")
    return draw_matrix_df[config.ALL_NUMBERS].astype(int)


# MODIFICADO: Recebe draw_matrix e last_contest_id_in_matrix
def calculate_current_delay(draw_matrix: pd.DataFrame, config: Any, last_contest_id_in_matrix: int) -> pd.DataFrame:
    logger.debug("Interno: Iniciando calculate_current_delay (com matriz pré-calculada).")
    if draw_matrix.empty:
        logger.warning("Matriz de sorteios vazia em calculate_current_delay.")
        return pd.DataFrame({'Dezena': config.ALL_NUMBERS, 'Atraso Atual': last_contest_id_in_matrix if last_contest_id_in_matrix else 0})

    current_delays = {}
    for dezena_val in config.ALL_NUMBERS:
        if dezena_val not in draw_matrix.columns:
            current_delays[dezena_val] = len(draw_matrix) 
            continue
            
        series_dezena = draw_matrix[dezena_val]
        last_occurrence_indices = series_dezena[series_dezena == 1].index
        
        if not last_occurrence_indices.empty:
            last_occurrence_idx = last_occurrence_indices.max()
            current_delays[dezena_val] = last_contest_id_in_matrix - last_occurrence_idx
        else:
            current_delays[dezena_val] = len(draw_matrix)
            
    delay_df = pd.DataFrame(list(current_delays.items()), columns=['Dezena', 'Atraso Atual'])
    delay_df['Dezena'] = delay_df['Dezena'].astype(int)
    delay_df['Atraso Atual'] = delay_df['Atraso Atual'].astype(int)
    logger.debug(f"Atraso atual calculado para {len(delay_df)} dezenas.")
    return delay_df.sort_values(by=['Atraso Atual', 'Dezena'], ascending=[False, True])


# MODIFICADO: Recebe draw_matrix, first_contest_id, last_contest_id
def calculate_max_delay(draw_matrix: pd.DataFrame, config: Any, first_contest_id_in_matrix: int, last_contest_id_in_matrix: int) -> pd.DataFrame:
    logger.debug("Interno: Iniciando calculate_max_delay (com matriz pré-calculada).")
    if draw_matrix.empty:
        logger.warning("Matriz de sorteios vazia em calculate_max_delay.")
        return pd.DataFrame({'Dezena': config.ALL_NUMBERS, 'Atraso Maximo': len(draw_matrix) if not draw_matrix.empty else 0})

    max_delays = {}
    for dezena_val in config.ALL_NUMBERS:
        if dezena_val not in draw_matrix.columns:
            max_delays[dezena_val] = len(draw_matrix)
            continue
            
        series_dezena = draw_matrix[dezena_val]
        occurrences = sorted(series_dezena[series_dezena == 1].index.tolist())
        
        if not occurrences:
            max_delays[dezena_val] = len(draw_matrix)
            continue
        
        gaps = [occurrences[0] - first_contest_id_in_matrix] 
        for i in range(len(occurrences) - 1):
            gaps.append(occurrences[i+1] - occurrences[i] - 1)
        gaps.append(last_contest_id_in_matrix - occurrences[-1])
        
        max_delays[dezena_val] = max(gaps) if gaps else 0
        
    delay_df = pd.DataFrame(list(max_delays.items()), columns=['Dezena', 'Atraso Maximo'])
    delay_df['Dezena'] = delay_df['Dezena'].astype(int)
    delay_df['Atraso Maximo'] = delay_df['Atraso Maximo'].astype(int)
    logger.debug(f"Atraso máximo calculado para {len(delay_df)} dezenas.")
    return delay_df.sort_values(by=['Atraso Maximo', 'Dezena'], ascending=[False, True])


# MODIFICADO: Recebe draw_matrix
def calculate_mean_delay(draw_matrix: pd.DataFrame, config: Any) -> pd.DataFrame:
    logger.debug("Interno: Iniciando calculate_mean_delay (com matriz pré-calculada).")
    if draw_matrix.empty:
        logger.warning("Matriz de sorteios vazia em calculate_mean_delay.")
        return pd.DataFrame({'Dezena': config.ALL_NUMBERS, 'Atraso Medio': np.nan})

    mean_delays = {}
    for dezena_val in config.ALL_NUMBERS:
        if dezena_val not in draw_matrix.columns:
            mean_delays[dezena_val] = np.nan
            continue
            
        series_dezena = draw_matrix[dezena_val]
        occurrences = sorted(series_dezena[series_dezena == 1].index.tolist())
        
        if len(occurrences) < 2:
            mean_delays[dezena_val] = np.nan
            continue
            
        gaps_between_occurrences = []
        for i in range(len(occurrences) - 1):
            gaps_between_occurrences.append(occurrences[i+1] - occurrences[i] - 1)
            
        if gaps_between_occurrences:
            mean_delays[dezena_val] = np.mean(gaps_between_occurrences)
        else:
            mean_delays[dezena_val] = np.nan
            
    delay_df = pd.DataFrame(list(mean_delays.items()), columns=['Dezena', 'Atraso Medio'])
    delay_df['Dezena'] = delay_df['Dezena'].astype(int)
    delay_df['Atraso Medio'] = pd.to_numeric(delay_df['Atraso Medio'], errors='coerce').round(4)
    logger.debug(f"Atraso médio calculado para {len(delay_df)} dezenas.")
    return delay_df.sort_values(by=['Atraso Medio', 'Dezena'], ascending=[False, True])

--------------------------------------------------------------------------------
# Arquivo: src/analysis/frequency_analysis.py
--------------------------------------------------------------------------------
# src/analysis/frequency_analysis.py
import pandas as pd
import logging
from typing import Dict, Optional, Any # Adicionado Any

logger = logging.getLogger(__name__)

def calculate_frequency(
    all_data_df: pd.DataFrame, 
    config: Any, # Espera config_obj
    # target_contest_id: Optional[int] = None # Removido, pois o df já vem filtrado
    # run_all: bool = True # Removido, pois o df já vem filtrado
    specific_numbers: Optional[list[int]] = None
) -> Optional[pd.DataFrame]:
    """
    Calcula a frequência absoluta das dezenas.
    O DataFrame de entrada 'all_data_df' já deve estar filtrado até o concurso desejado.
    """
    # logger.info("Interno: Iniciando calculate_frequency (analysis).") # MUDADO PARA DEBUG ou REMOVIDO
    logger.debug("Interno: Iniciando calculate_frequency (analysis).")

    if all_data_df.empty:
        logger.warning("DataFrame vazio fornecido para calculate_frequency.")
        # Retorna um DataFrame com a estrutura esperada, mas com frequência zero
        dezenas_para_retorno = specific_numbers if specific_numbers else config.ALL_NUMBERS
        return pd.DataFrame({'Dezena': dezenas_para_retorno, 'Frequencia Absoluta': 0})

    # Usa as colunas de bolas definidas no config
    ball_columns_to_use = [col for col in config.BALL_NUMBER_COLUMNS if col in all_data_df.columns]
    if not ball_columns_to_use:
        logger.error("Nenhuma coluna de bola encontrada no DataFrame para calculate_frequency.")
        return None # Ou DataFrame com zeros

    # Concatena todas as colunas de bolas em uma única série
    all_drawn_numbers = pd.concat([all_data_df[col] for col in ball_columns_to_use], ignore_index=True)
    all_drawn_numbers.dropna(inplace=True) # Remove NaNs que podem surgir de colunas de bolas parcialmente preenchidas
    
    try:
        # Tenta converter para inteiro, tratando erros
        all_drawn_numbers = pd.to_numeric(all_drawn_numbers, errors='coerce')
        all_drawn_numbers.dropna(inplace=True) # Remove NaNs após conversão
        all_drawn_numbers = all_drawn_numbers.astype(int)
    except Exception as e:
        logger.error(f"Erro ao converter dezenas para inteiro em calculate_frequency: {e}")
        return None

    # Calcula a contagem de frequência
    frequency_counts = all_drawn_numbers.value_counts().sort_index()
    
    # Prepara o DataFrame de resultado
    dezenas_para_analise = specific_numbers if specific_numbers else config.ALL_NUMBERS
    df_frequency = pd.DataFrame({
        'Dezena': dezenas_para_analise # Usa a constante DEZENA_COLUMN_NAME do config
    })
    
    # Mapeia as frequências calculadas, preenchendo com 0 para dezenas não sorteadas
    df_frequency['Frequencia Absoluta'] = df_frequency['Dezena'].map(frequency_counts).fillna(0).astype(int)
    
    # logger.info(f"Frequência absoluta calculada para {len(df_frequency)} dezenas em calculate_frequency (analysis).") # MUDADO PARA DEBUG ou REMOVIDO
    logger.debug(f"Frequência absoluta calculada para {len(df_frequency)} dezenas em calculate_frequency (analysis).")
    return df_frequency


def calculate_relative_frequency(
    df_frequency_abs: pd.DataFrame, 
    total_draws: int, 
    config: Any # Espera config_obj
) -> Optional[pd.DataFrame]:
    """
    Calcula a frequência relativa das dezenas.
    Espera um DataFrame com colunas 'Dezena' e 'Frequencia Absoluta'.
    """
    # logger.info("Interno: Iniciando calculate_relative_frequency (analysis).") # MUDADO PARA DEBUG ou REMOVIDO
    logger.debug("Interno: Iniciando calculate_relative_frequency (analysis).")

    if df_frequency_abs.empty:
        logger.warning("DataFrame de frequência absoluta vazio para calculate_relative_frequency.")
        return pd.DataFrame({'Dezena': config.ALL_NUMBERS, 'Frequencia Relativa': 0.0})
    
    if 'Dezena' not in df_frequency_abs.columns or 'Frequencia Absoluta' not in df_frequency_abs.columns:
        logger.error("Colunas 'Dezena' ou 'Frequencia Absoluta' não encontradas no DataFrame de entrada.")
        return None

    if total_draws == 0:
        logger.warning("Total de sorteios é zero. Frequência relativa será zero.")
        df_relative_frequency = df_frequency_abs[['Dezena']].copy()
        df_relative_frequency['Frequencia Relativa'] = 0.0
        return df_relative_frequency

    df_relative_frequency = df_frequency_abs.copy()
    df_relative_frequency['Frequencia Relativa'] = (df_relative_frequency['Frequencia Absoluta'] / total_draws).round(6)
    
    # logger.info(f"Frequência relativa calculada para {len(df_relative_frequency)} dezenas em calculate_relative_frequency (analysis).") # MUDADO PARA DEBUG ou REMOVIDO
    logger.debug(f"Frequência relativa calculada para {len(df_relative_frequency)} dezenas em calculate_relative_frequency (analysis).")
    return df_relative_frequency

--------------------------------------------------------------------------------
# Arquivo: src/analysis/frequent_itemset_metrics_analysis.py
--------------------------------------------------------------------------------
# src/analysis/frequent_itemset_metrics_analysis.py
import pandas as pd
import numpy as np
import json 
from typing import List, Dict, Any, Set

try:
    # Importar CONTEST_ID_COLUMN_NAME do config para consistência
    from ..config import Config, CONTEST_ID_COLUMN_NAME 
except ImportError:
    from src.config import Config, CONTEST_ID_COLUMN_NAME

import logging
logger = logging.getLogger(__name__)

def parse_itemset_str(itemset_str: str) -> Set[int]:
    """Converte uma string de itemset (ex: '01-05-12') para um conjunto de inteiros."""
    if not itemset_str or not isinstance(itemset_str, str):
        logger.warning(f"Tentativa de parsear itemset_str inválido: {itemset_str}")
        return set()
    try:
        return set(map(int, itemset_str.split('-')))
    except ValueError:
        logger.warning(f"Erro ao converter partes de itemset_str para int: {itemset_str}")
        return set()

def calculate_frequent_itemset_delay_metrics(
    all_draws_df: pd.DataFrame, 
    frequent_itemsets_df: pd.DataFrame, 
    latest_contest_id: int, # <<< PARÂMETRO RENOMEADO
    config: Config # Tipagem mais específica para config
) -> pd.DataFrame:
    """
    Calcula métricas de atraso e outras informações para cada itemset frequente.

    Args:
        all_draws_df (pd.DataFrame): DataFrame com todos os sorteios. 
                                     Deve conter CONTEST_ID_COLUMN_NAME (ex: 'contest_id') e 'numbers_drawn_set'.
        frequent_itemsets_df (pd.DataFrame): DataFrame da tabela 'frequent_itemsets'.
                                             Deve conter 'itemset_str', 'length', 'support', 'frequency_count'.
        latest_contest_id (int): O ID do concurso mais recente no histórico. # <<< ATUALIZADO
        config (Config): Objeto de configuração.

    Returns:
        pd.DataFrame: DataFrame com métricas de atraso para cada itemset.
    """
    logger.info(f"Iniciando cálculo de métricas de atraso para {len(frequent_itemsets_df)} itemsets frequentes, usando '{CONTEST_ID_COLUMN_NAME}' como ID.")
    
    if CONTEST_ID_COLUMN_NAME not in all_draws_df.columns:
        msg = f"DataFrame all_draws_df deve conter a coluna de ID do concurso: '{CONTEST_ID_COLUMN_NAME}'."
        logger.error(msg)
        raise ValueError(msg)

    # Garantir que 'numbers_drawn_set' exista
    # (seu main.py e data_loader já criam 'drawn_numbers' que é uma lista, precisamos do set)
    if 'numbers_drawn_set' not in all_draws_df.columns:
        if 'drawn_numbers' in all_draws_df.columns:
            logger.info("Coluna 'numbers_drawn_set' não encontrada em all_draws_df, criando a partir de 'drawn_numbers'.")
            try:
                # Garante que os elementos da lista são convertidos para set
                all_draws_df['numbers_drawn_set'] = all_draws_df['drawn_numbers'].apply(lambda x: set(x) if isinstance(x, list) else set())
            except Exception as e_set_conversion:
                logger.error(f"Erro ao converter 'drawn_numbers' para 'numbers_drawn_set': {e_set_conversion}", exc_info=True)
                raise ValueError("Falha ao preparar a coluna 'numbers_drawn_set'.")
        else:
            msg = "DataFrame all_draws_df deve conter 'numbers_drawn_set' (Set[int]) ou 'drawn_numbers' (List[int])."
            logger.error(msg)
            raise ValueError(msg)


    if frequent_itemsets_df.empty:
        logger.warning("DataFrame de itemsets frequentes está vazio. Retornando DataFrame vazio.")
        cols = ['itemset_str', 'length', 'support', 'frequency_count', 
                'last_occurrence_contest_id', 'current_delay', 
                'mean_delay', 'max_delay', 'std_dev_delay', 'occurrences_draw_ids']
        return pd.DataFrame(columns=cols)

    processed_metrics = []

    # Certifique-se de que a coluna de ID do concurso em all_draws_df é do tipo int para comparações
    all_draws_df[CONTEST_ID_COLUMN_NAME] = all_draws_df[CONTEST_ID_COLUMN_NAME].astype(int)


    for _, row in frequent_itemsets_df.iterrows():
        itemset_str = row['itemset_str']
        itemset_set = parse_itemset_str(itemset_str)

        if not itemset_set:
            continue

        occurrences_mask = all_draws_df['numbers_drawn_set'].apply(lambda s: itemset_set.issubset(s))
        occurrence_draws_df = all_draws_df[occurrences_mask]
        
        occurrence_contest_ids = sorted(list(occurrence_draws_df[CONTEST_ID_COLUMN_NAME].unique()))

        # Inicialização das métricas
        last_occurrence_contest_id_val = None
        current_delay_val = None
        mean_delay_val = np.nan
        max_delay_val = np.nan
        std_dev_delay_val = np.nan
        occurrences_json = json.dumps([])

        if not occurrence_contest_ids:
            # Se o itemset nunca ocorreu, o atraso atual é desde o "início" até o último concurso.
            # O primeiro concurso no dataset pode ser usado como referência se necessário, ou latest_contest_id.
            # Por simplicidade, se não houver ocorrências, current_delay é a "idade" total dos dados.
            if not all_draws_df.empty:
                 first_contest_id_in_data = all_draws_df[CONTEST_ID_COLUMN_NAME].min()
                 current_delay_val = latest_contest_id - first_contest_id_in_data + 1 # Ou apenas latest_contest_id se o start for 0
                 max_delay_val = current_delay_val # O atraso máximo é a própria duração se nunca ocorreu
            else: # Caso extremo de all_draws_df ser vazio, mas já foi checado antes
                current_delay_val = latest_contest_id 
                max_delay_val = latest_contest_id

        else:
            last_occurrence_contest_id_val = int(occurrence_contest_ids[-1])
            current_delay_val = latest_contest_id - last_occurrence_contest_id_val
            
            gaps = []
            if len(occurrence_contest_ids) > 1:
                # Gap inicial (desde o primeiro concurso geral até a primeira ocorrência do itemset)
                # Se quiser incluir o gap desde o início do dataset até a primeira ocorrência do itemset:
                # if not all_draws_df.empty:
                #     first_contest_id_in_data = all_draws_df[CONTEST_ID_COLUMN_NAME].min()
                #     gaps.append(occurrence_contest_ids[0] - first_contest_id_in_data)
                
                for i in range(len(occurrence_contest_ids) - 1):
                    gaps.append(occurrence_contest_ids[i+1] - occurrence_contest_ids[i] - 1)
            
            # Gap final (desde a última ocorrência do itemset até o último concurso geral)
            # Se quiser incluir o gap desde a última ocorrência até o final do dataset:
            # gaps.append(latest_contest_id - occurrence_contest_ids[-1])


            if gaps: 
                mean_delay_val = np.mean(gaps)
                max_delay_val = np.max(gaps) 
                std_dev_delay_val = pd.Series(gaps).std(ddof=1) 
            elif len(occurrence_contest_ids) == 1: # Apenas uma ocorrência
                # O que fazer com média, max, std de gaps?
                # Atraso atual já foi calculado.
                # Gaps entre ocorrências não existem.
                mean_delay_val = np.nan
                max_delay_val = np.nan # Ou 0 se considerarmos que não houve "intervalo"
                std_dev_delay_val = np.nan
            
            occurrences_json = json.dumps([int(cid) for cid in occurrence_contest_ids]) # Garante que são ints para JSON

        processed_metrics.append({
            'itemset_str': itemset_str,
            'length': int(row['length']),
            'support': float(row['support']),
            'frequency_count': int(row['frequency_count']),
            'last_occurrence_contest_id': last_occurrence_contest_id_val,
            'current_delay': current_delay_val,
            'mean_delay': mean_delay_val,
            'max_delay': max_delay_val,
            'std_dev_delay': std_dev_delay_val,
            'occurrences_draw_ids': occurrences_json
        })
        
        if len(processed_metrics) % 10000 == 0: # Log de progresso menos frequente
             logger.info(f"Processadas métricas de atraso para {len(processed_metrics)}/{len(frequent_itemsets_df)} itemsets...")

    logger.info(f"Cálculo de métricas de atraso para itemsets frequentes concluído. {len(processed_metrics)} itemsets processados.")
    
    final_cols = ['itemset_str', 'length', 'support', 'frequency_count', 
                  'last_occurrence_contest_id', 'current_delay', 
                  'mean_delay', 'max_delay', 'std_dev_delay', 'occurrences_draw_ids']
    
    result_df = pd.DataFrame(processed_metrics, columns=final_cols)
    
    return result_df

--------------------------------------------------------------------------------
# Arquivo: src/analysis/grid_analysis.py
--------------------------------------------------------------------------------
# src/analysis/grid_analysis.py
import pandas as pd
import logging
from typing import List, Dict, Any, Set
from collections import Counter

logger = logging.getLogger(__name__)

def analyze_grid_distribution(
    all_draws_df: pd.DataFrame, 
    config: Any
) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    Analisa a distribuição de quantas dezenas são sorteadas por linha e por coluna do volante.

    Args:
        all_draws_df (pd.DataFrame): DataFrame com todos os sorteios.
                                     Deve conter a coluna config.DRAWN_NUMBERS_COLUMN_NAME.
        config (Any): Objeto de configuração, que deve ter os atributos:
                      DRAWN_NUMBERS_COLUMN_NAME, LOTOFACIL_GRID_LINES,
                      LOTOFACIL_GRID_COLUMNS.

    Returns:
        tuple[pd.DataFrame, pd.DataFrame]: 
            - line_distribution_df: DataFrame com a distribuição para as linhas.
                Colunas: 'Linha', 'Qtd_Dezenas_Sorteadas', 'Frequencia_Absoluta', 'Frequencia_Relativa'.
            - column_distribution_df: DataFrame com a distribuição para as colunas.
                Colunas: 'Coluna', 'Qtd_Dezenas_Sorteadas', 'Frequencia_Absoluta', 'Frequencia_Relativa'.
    """
    step_name = "Análise de Distribuição por Linhas e Colunas"
    logger.info(f"Iniciando {step_name}.")

    drawn_numbers_col = config.DRAWN_NUMBERS_COLUMN_NAME
    grid_lines = config.LOTOFACIL_GRID_LINES
    grid_columns = config.LOTOFACIL_GRID_COLUMNS

    if drawn_numbers_col not in all_draws_df.columns:
        logger.error(f"Coluna '{drawn_numbers_col}' não encontrada no DataFrame de sorteios.")
        empty_df = pd.DataFrame(columns=['Elemento', 'Qtd_Dezenas_Sorteadas', 'Frequencia_Absoluta', 'Frequencia_Relativa'])
        return empty_df.rename(columns={'Elemento':'Linha'}), empty_df.rename(columns={'Elemento':'Coluna'})

    # Contadores para linhas e colunas
    # A chave será o nome da linha/coluna (ex: "L1", "C1")
    # O valor será um Counter para as quantidades (0 a 5 dezenas)
    line_counts_by_draw: Dict[str, List[int]] = {line_name: [] for line_name in grid_lines.keys()}
    column_counts_by_draw: Dict[str, List[int]] = {col_name: [] for col_name in grid_columns.keys()}

    total_draws = len(all_draws_df)
    if total_draws == 0:
        logger.warning("DataFrame de sorteios está vazio. Nenhuma análise de grid será realizada.")
        empty_df = pd.DataFrame(columns=['Elemento', 'Qtd_Dezenas_Sorteadas', 'Frequencia_Absoluta', 'Frequencia_Relativa'])
        return empty_df.rename(columns={'Elemento':'Linha'}), empty_df.rename(columns={'Elemento':'Coluna'})

    for _, row in all_draws_df.iterrows():
        drawn_numbers: Set[int] = set(row[drawn_numbers_col]) if isinstance(row[drawn_numbers_col], list) else set()

        # Contagem por Linhas
        for line_name, line_dezenas in grid_lines.items():
            count_in_line = len(drawn_numbers.intersection(set(line_dezenas)))
            line_counts_by_draw[line_name].append(count_in_line)

        # Contagem por Colunas
        for col_name, col_dezenas in grid_columns.items():
            count_in_col = len(drawn_numbers.intersection(set(col_dezenas)))
            column_counts_by_draw[col_name].append(count_in_col)

    # Processar e agregar resultados para Linhas
    line_distribution_results = []
    for line_name, counts_list in line_counts_by_draw.items():
        counts_summary = Counter(counts_list)
        for qtd_dezenas, freq_abs in counts_summary.items():
            freq_rel = freq_abs / total_draws if total_draws > 0 else 0.0
            line_distribution_results.append({
                'Linha': line_name,
                'Qtd_Dezenas_Sorteadas': qtd_dezenas,
                'Frequencia_Absoluta': freq_abs,
                'Frequencia_Relativa': round(freq_rel, 6)
            })
    line_distribution_df = pd.DataFrame(line_distribution_results)
    if not line_distribution_df.empty:
        line_distribution_df = line_distribution_df.sort_values(by=['Linha', 'Qtd_Dezenas_Sorteadas']).reset_index(drop=True)

    # Processar e agregar resultados para Colunas
    column_distribution_results = []
    for col_name, counts_list in column_counts_by_draw.items():
        counts_summary = Counter(counts_list)
        for qtd_dezenas, freq_abs in counts_summary.items():
            freq_rel = freq_abs / total_draws if total_draws > 0 else 0.0
            column_distribution_results.append({
                'Coluna': col_name,
                'Qtd_Dezenas_Sorteadas': qtd_dezenas,
                'Frequencia_Absoluta': freq_abs,
                'Frequencia_Relativa': round(freq_rel, 6)
            })
    column_distribution_df = pd.DataFrame(column_distribution_results)
    if not column_distribution_df.empty:
        column_distribution_df = column_distribution_df.sort_values(by=['Coluna', 'Qtd_Dezenas_Sorteadas']).reset_index(drop=True)
        
    logger.info(f"{step_name} concluída.")
    return line_distribution_df, column_distribution_df

--------------------------------------------------------------------------------
# Arquivo: src/analysis/group_trend_analysis.py
--------------------------------------------------------------------------------
# src/analysis/group_trend_analysis.py

import pandas as pd
import numpy as np
from typing import Dict, List, Optional

# Importa funções e constantes necessárias
from src.config import logger, ALL_NUMBERS
from src.analysis.frequency_analysis import calculate_windowed_frequency

# Define os grupos
NUMBER_GROUPS: Dict[str, List[int]] = {
    'G1 (1-5)':   list(range(1, 6)),
    'G2 (6-10)':  list(range(6, 11)),
    'G3 (11-15)': list(range(11, 16)),
    'G4 (16-20)': list(range(16, 21)),
    'G5 (21-25)': list(range(21, 26)),
}
# Fallback (caso config falhe)
try: from src.config import DEFAULT_GROUP_WINDOWS
except ImportError: DEFAULT_GROUP_WINDOWS = [25, 100]
if 'ALL_NUMBERS' not in globals(): ALL_NUMBERS = list(range(1, 26))


def calculate_group_freq_stats(concurso_maximo: Optional[int] = None,
                               windows: List[int] = DEFAULT_GROUP_WINDOWS
                               ) -> Optional[pd.DataFrame]: # <<< Retorna DataFrame
    """
    Calcula a frequência MÉDIA das dezenas dentro de cada grupo definido
    para diferentes janelas recentes.

    Returns:
        Optional[pd.DataFrame]: DataFrame indexado pelo NOME do grupo, com colunas
                                 como 'W25_avg_freq', 'W100_avg_freq'.
    """
    if not windows: return pd.DataFrame(index=list(NUMBER_GROUPS.keys()))
    logger.info(f"Calculando stats de freq. média por grupo (W: {windows}) até {concurso_maximo or 'último'}...")

    # Cache para não recalcular janelas inteiras
    window_freq_cache: Dict[int, Optional[pd.Series]] = {}
    for w in windows: window_freq_cache[w] = calculate_windowed_frequency(w, concurso_maximo)

    group_avg_data = {} # Dict para construir o DataFrame final

    # Calcula a média da frequência para os números de cada grupo
    for group_name, numbers_in_group in NUMBER_GROUPS.items():
        group_avg_data[group_name] = {} # Inicia dict para este grupo
        for window_size in windows:
            col_name = f'W{window_size}_avg_freq'
            freq_series = window_freq_cache.get(window_size)

            if freq_series is None:
                group_avg_data[group_name][col_name] = np.nan # Usa NaN se janela falhou
                logger.warning(f"Freq W{window_size} indisponível p/ grupo {group_name}.")
            else:
                group_frequencies = freq_series.reindex(numbers_in_group)
                avg_freq = group_frequencies.mean(skipna=True)
                # Usa NaN se média falhar (ex: grupo vazio - não deve acontecer)
                group_avg_data[group_name][col_name] = avg_freq if pd.notna(avg_freq) else np.nan
                logger.debug(f"Grupo {group_name} - W{window_size}: Média Freq = {avg_freq:.4f}")

    logger.info("Cálculo de stats de freq. média por grupo concluído.")
    results_df = pd.DataFrame.from_dict(group_avg_data, orient='index')
    results_df.index.name = 'grupo'
    # Preenche NaNs restantes com 0 (ex: se janela inteira falhou)
    results_df.fillna(0, inplace=True)
    return results_df

--------------------------------------------------------------------------------
# Arquivo: src/analysis/number_properties_analysis.py
--------------------------------------------------------------------------------
# src/analysis/number_properties_analysis.py
import pandas as pd
import numpy as np
from typing import List, Dict, Any
import logging

# Importa ALL_NUMBERS para definir PRIMES_UP_TO_25 globalmente
# Se preferir, PRIMES_UP_TO_25 pode ser um atributo de config também.
from src.config import ALL_NUMBERS as CONFIG_ALL_NUMBERS # Renomeado para evitar conflito se config for passado

logger = logging.getLogger(__name__)

_PRIMES_UP_TO_25_CACHE: List[int] = []

def get_prime_numbers(limit: int) -> List[int]:
    global _PRIMES_UP_TO_25_CACHE
    if limit == 25 and _PRIMES_UP_TO_25_CACHE:
        return _PRIMES_UP_TO_25_CACHE
    primes = []
    if limit < 2: return primes
    sieve = [True] * (limit + 1)
    for p in range(2, int(limit**0.5) + 1):
        if sieve[p]:
            for multiple in range(p*p, limit + 1, p): sieve[multiple] = False
    for p in range(2, limit + 1):
        if sieve[p]: primes.append(p)
    if limit == 25: _PRIMES_UP_TO_25_CACHE = primes
    return primes

limit_for_primes = max(CONFIG_ALL_NUMBERS) if CONFIG_ALL_NUMBERS else 25
PRIMES_UP_TO_25: List[int] = get_prime_numbers(limit_for_primes)
logger.debug(f"PRIMES_UP_TO_25 (módulo): {PRIMES_UP_TO_25}")

def analyze_draw_properties(draw: List[int], config_obj_param: Any) -> Dict[str, Any]: # Renomeado config para evitar conflito
    properties: Dict[str, Any] = {}
    if not draw or len(draw) != config_obj_param.NUMBERS_PER_DRAW:
        logger.warning(f"Sorteio inválido ou número incorreto de dezenas: {draw}")
        return {'soma_dezenas': 0, 'pares': 0, 'impares': 0, 'primos': 0}

    draw_series = pd.Series(draw)
    properties['soma_dezenas'] = int(draw_series.sum())
    properties['pares'] = int(draw_series.apply(lambda x: x % 2 == 0).sum())
    properties['impares'] = int(draw_series.apply(lambda x: x % 2 != 0).sum())
    properties['primos'] = int(draw_series.apply(lambda x: x in PRIMES_UP_TO_25).sum())
    return properties

def analyze_number_properties(all_data_df: pd.DataFrame, config: Any) -> pd.DataFrame: # Recebe config
    logger.info("Iniciando análise de propriedades numéricas dos sorteios.")
    if all_data_df is None or all_data_df.empty:
        logger.warning("DataFrame para analyze_number_properties está vazio.")
        return pd.DataFrame()

    contest_col = config.CONTEST_ID_COLUMN_NAME
    ball_cols = config.BALL_NUMBER_COLUMNS
    numbers_per_draw_val = config.NUMBERS_PER_DRAW

    if contest_col not in all_data_df.columns:
        logger.error(f"Coluna '{contest_col}' não encontrada. Colunas: {all_data_df.columns.tolist()}")
        return pd.DataFrame()

    actual_ball_cols = [col for col in ball_cols if col in all_data_df.columns]
    if len(actual_ball_cols) < numbers_per_draw_val :
        logger.warning(f"Esperava {numbers_per_draw_val} colunas de bolas, encontrou {len(actual_ball_cols)}. Usando: {actual_ball_cols}")
        if not actual_ball_cols:
            logger.error("Nenhuma coluna de bola encontrada."); return pd.DataFrame()

    results = []
    for index, row in all_data_df.iterrows():
        try:
            draw = [int(row[col]) for col in actual_ball_cols if pd.notna(row[col])]
            if len(draw) != numbers_per_draw_val:
                logger.debug(f"Sorteio {row[contest_col]} tem {len(draw)} dezenas, esperado {numbers_per_draw_val}. Pulando.")
                continue
            properties = analyze_draw_properties(draw, config) # Passa config
            properties_entry: Dict[str,Any] = {contest_col: int(row[contest_col])}
            properties_entry.update(properties)
            results.append(properties_entry)
        except Exception as e:
            logger.error(f"Erro processar props concurso {row.get(contest_col, 'UKN')}: {e}", exc_info=False)
    
    if not results: logger.warning("Nenhuma propriedade calculada."); return pd.DataFrame()
    properties_df = pd.DataFrame(results)
    
    final_col_order = [contest_col] + [col for col in properties_df.columns if col != contest_col]
    # Garante que apenas colunas existentes sejam selecionadas
    final_col_order = [col for col in final_col_order if col in properties_df.columns]
    properties_df = properties_df[final_col_order]
    
    # Renomeia a coluna de concurso para "Concurso" se a tabela final no DB espera esse nome
    properties_df.rename(columns={contest_col: "Concurso"}, inplace=True, errors='ignore')

    logger.info(f"Análise de propriedades numéricas concluída para {len(properties_df)} concursos.")
    return properties_df

--------------------------------------------------------------------------------
# Arquivo: src/analysis/positional_analysis.py
--------------------------------------------------------------------------------
# src/analysis/positional_analysis.py
import pandas as pd
import logging
from typing import List, Any
# Importa Config de forma a ser compatível com a estrutura do projeto
# Se config_obj é globalmente acessível ou passado via contexto, ajuste conforme necessário.
# Para este módulo, assumiremos que um objeto config será passado para a função.

logger = logging.getLogger(__name__)

def analyze_draw_position_frequency(all_draws_df: pd.DataFrame, config: Any) -> pd.DataFrame:
    """
    Analisa a frequência de cada dezena (1-25) em cada uma das 15 posições de sorteio.

    Args:
        all_draws_df: DataFrame contendo todos os sorteios históricos.
                      Esperado que tenha colunas como 'ball_1', 'ball_2', ..., 'ball_15'
                      conforme definido em config.BALL_NUMBER_COLUMNS.
        config: O objeto de configuração (instância da classe Config).

    Returns:
        Um DataFrame com dezenas 1-25 como índice ('Dezena') e colunas 'Posicao_1'
        até 'Posicao_15' contendo as contagens de frequência.
        O DataFrame é retornado com 'Dezena' como uma coluna regular para fácil salvamento no BD.
    """
    logger.info("Iniciando análise de frequência posicional das dezenas.")

    if all_draws_df.empty:
        logger.warning("DataFrame de sorteios está vazio. Retornando DataFrame de frequência posicional vazio.")
        pos_cols_names = [f"Posicao_{i+1}" for i in range(config.NUMBERS_PER_DRAW)]
        empty_df = pd.DataFrame(columns=['Dezena'] + pos_cols_names)
        # Para consistência, se esperamos 'Dezena' como coluna, não definir como índice aqui.
        # Se a tabela do BD tem 'Dezena' como PK e o df tem que ter essa coluna, então está ok.
        return empty_df

    ball_cols = config.BALL_NUMBER_COLUMNS
    if not all(col in all_draws_df.columns for col in ball_cols):
        missing_cols = [col for col in ball_cols if col not in all_draws_df.columns]
        logger.error(f"Colunas de bolas esperadas ({ball_cols}) ausentes no DataFrame: {missing_cols}.")
        # Retorna um DataFrame vazio com a estrutura esperada, mas sem dados.
        pos_cols_names = [f"Posicao_{i+1}" for i in range(config.NUMBERS_PER_DRAW)]
        empty_df_structure = pd.DataFrame(0, index=config.ALL_NUMBERS, columns=pos_cols_names)
        empty_df_structure.index.name = 'Dezena'
        return empty_df_structure.reset_index() # 'Dezena' como coluna

    numbers_range = config.ALL_NUMBERS 
    position_columns = [f"Posicao_{i+1}" for i in range(config.NUMBERS_PER_DRAW)] 
    
    # Inicializa o DataFrame com dezenas como índice para facilitar o incremento
    positional_freq_df = pd.DataFrame(0, index=numbers_range, columns=position_columns)
    positional_freq_df.index.name = 'Dezena'

    for _, row in all_draws_df.iterrows():
        for i, ball_col_name in enumerate(ball_cols): # ball_cols é ['ball_1', ..., 'ball_15']
            try:
                # As colunas de bolas no DataFrame limpo já devem ser numéricas (int)
                # Se vierem como string do CSV, data_loader.py deve converter.
                # Se ainda assim puderem ser string ou float, a conversão é necessária.
                number_drawn = row[ball_col_name]
                if pd.isna(number_drawn): # Checa por NaN ou NaT
                    logger.warning(f"Valor NaN encontrado na coluna {ball_col_name} no concurso {row.get(config.CONTEST_ID_COLUMN_NAME, 'N/A')}. Pulando esta entrada.")
                    continue
                
                number_drawn = int(number_drawn) # Garante que é int

                if number_drawn in positional_freq_df.index:
                    # As colunas de posição são 'Posicao_1', 'Posicao_2', ...
                    # O 'i' do enumerate(ball_cols) vai de 0 a 14.
                    col_pos_name = f"Posicao_{i+1}" 
                    positional_freq_df.loc[number_drawn, col_pos_name] += 1
                else:
                    logger.warning(f"Dezena {number_drawn} da coluna {ball_col_name} no concurso {row.get(config.CONTEST_ID_COLUMN_NAME, 'N/A')} está fora do range esperado ({numbers_range}).")

            except ValueError:
                logger.warning(f"Valor não numérico ou não conversível para int encontrado na coluna {ball_col_name} no concurso {row.get(config.CONTEST_ID_COLUMN_NAME, 'N/A')}. Valor: {row[ball_col_name]}. Pulando esta entrada.")
            except Exception as e:
                logger.error(f"Erro inesperado ao processar {ball_col_name} no concurso {row.get(config.CONTEST_ID_COLUMN_NAME, 'N/A')}: {e}", exc_info=True)
    
    logger.info("Análise de frequência posicional concluída.")
    # Retorna com 'Dezena' como uma coluna para facilitar o salvamento no banco de dados,
    # onde 'Dezena' é a chave primária na tabela.
    return positional_freq_df.reset_index()

--------------------------------------------------------------------------------
# Arquivo: src/analysis/rank_trend_analysis.py
--------------------------------------------------------------------------------
# Lotofacil_Analysis/src/analysis/rank_trend_analysis.py
import pandas as pd
import numpy as np
import logging
from typing import List, Dict, Optional, Any
from scipy.stats import linregress

logger = logging.getLogger(__name__)

def calculate_and_persist_rank_per_chunk(db_manager: Any, config: Any) -> None:
    logger.info("Iniciando cálculo e persistência de ranking de frequência por chunk.")

    freq_table_prefix = config.EVOL_METRIC_FREQUENCY_BLOCK_PREFIX
    rank_table_prefix = config.EVOL_RANK_FREQUENCY_BLOCK_PREFIX
    chunk_seq_id_col = 'chunk_seq_id'
    dezena_col = config.DEZENA_COLUMN_NAME
    freq_col = 'frequencia_absoluta'
    rank_col = 'rank_no_bloco'

    if not hasattr(config, 'CHUNK_TYPES_CONFIG'):
        logger.error("CHUNK_TYPES_CONFIG não encontrado no objeto de configuração.")
        return

    for chunk_type, sizes in config.CHUNK_TYPES_CONFIG.items():
        for size in sizes:
            logger.info(f"Processando ranking para chunks: tipo='{chunk_type}', tamanho={size}")
            freq_table_name = f"{freq_table_prefix}_{chunk_type}_{size}"
            rank_table_name = f"{rank_table_prefix}_{chunk_type}_{size}"

            if not db_manager.table_exists(freq_table_name):
                logger.warning(f"Tabela de frequência '{freq_table_name}' não encontrada. Pulando rank para {chunk_type}_{size}.")
                continue

            df_freq_chunk = db_manager.load_dataframe(freq_table_name)

            if df_freq_chunk is None or df_freq_chunk.empty:
                logger.info(f"DataFrame de frequência para '{freq_table_name}' vazio. Pulando rank.")
                empty_rank_df_cols = [chunk_seq_id_col, dezena_col, rank_col]
                if 'chunk_start_contest' in df_freq_chunk.columns:
                     empty_rank_df_cols.insert(1, 'chunk_start_contest')
                if 'chunk_end_contest' in df_freq_chunk.columns:
                     empty_rank_df_cols.insert(2, 'chunk_end_contest')
                empty_rank_df = pd.DataFrame(columns=empty_rank_df_cols)
                try:
                    # CORREÇÃO: Removido index=False (se existia aqui, mas o erro principal é abaixo)
                    db_manager.save_dataframe(empty_rank_df, rank_table_name, if_exists='replace')
                except Exception as e_save_empty:
                    logger.error(f"Erro ao salvar tabela de rank vazia '{rank_table_name}': {e_save_empty}")
                continue

            if freq_col not in df_freq_chunk.columns:
                logger.error(f"Coluna de frequência '{freq_col}' não encontrada em '{freq_table_name}'. Pulando rank.")
                continue
            
            df_freq_chunk[rank_col] = df_freq_chunk.groupby(chunk_seq_id_col)[freq_col].rank(method='dense', ascending=False).astype(int)
            
            cols_to_save = [chunk_seq_id_col]
            if 'chunk_start_contest' in df_freq_chunk.columns:
                cols_to_save.append('chunk_start_contest')
            if 'chunk_end_contest' in df_freq_chunk.columns:
                cols_to_save.append('chunk_end_contest')
            cols_to_save.extend([dezena_col, rank_col])
            
            df_to_save = df_freq_chunk[[col for col in cols_to_save if col in df_freq_chunk.columns]].copy()

            try:
                # CORREÇÃO APLICADA: removido index=False
                db_manager.save_dataframe(df_to_save, rank_table_name, if_exists='replace')
                logger.info(f"Dados de ranking salvos na tabela '{rank_table_name}'.")
            except Exception as e_save:
                logger.error(f"Erro ao salvar dados de ranking na tabela '{rank_table_name}': {e_save}", exc_info=True)
                
    logger.info("Cálculo e persistência de ranking de frequência por chunk concluídos.")


def calculate_historical_rank_trends(
    db_manager: Any,
    config: Any,
    aggregated_block_table_name: str,
    rank_analysis_type_filter: str,
    rank_value_column_name: str, 
    trend_window_blocks: int,
    slope_improving_threshold: float,
    slope_worsening_threshold: float
) -> Optional[pd.DataFrame]:
    logger.info(f"Iniciando cálculo de tendências de rank da tabela: {aggregated_block_table_name} filtrando por tipo_analise='{rank_analysis_type_filter}'.")

    if not db_manager.table_exists(aggregated_block_table_name):
        logger.error(f"Tabela agregada de blocos '{aggregated_block_table_name}' não encontrada.")
        return None

    df_agg_blocks = db_manager.load_dataframe(aggregated_block_table_name)
    if df_agg_blocks is None or df_agg_blocks.empty:
        logger.warning(f"DataFrame da tabela agregada de blocos '{aggregated_block_table_name}' está vazio.")
        return None

    df_ranks = df_agg_blocks[df_agg_blocks['tipo_analise'] == rank_analysis_type_filter].copy()
    
    if df_ranks.empty:
        logger.warning(f"Nenhum dado encontrado para tipo_analise='{rank_analysis_type_filter}' na tabela '{aggregated_block_table_name}'.")
        return None
        
    all_trend_data: List[Dict[str, Any]] = []
    
    required_trend_cols = ['chunk_seq_id', 'chunk_end_contest']
    if not all(col in df_ranks.columns for col in required_trend_cols):
        logger.error(f"Colunas {required_trend_cols} não encontradas no DataFrame de ranks. Não é possível calcular tendências.")
        return None
        
    for dezena_num in config.ALL_NUMBERS:
        dezena_col_for_rank_value = f"dezena_{dezena_num}"
        
        if dezena_col_for_rank_value not in df_ranks.columns:
            logger.debug(f"Coluna de rank '{dezena_col_for_rank_value}' não encontrada para dezena {dezena_num}. Pulando.")
            last_chunk_end_contest = df_ranks['chunk_end_contest'].max() if not df_ranks.empty else None
            if last_chunk_end_contest is not None:
                all_trend_data.append({
                    config.CONTEST_ID_COLUMN_NAME: int(last_chunk_end_contest),
                    config.DEZENA_COLUMN_NAME: int(dezena_num),
                    config.RANK_SLOPE_COLUMN_NAME: np.nan,
                    config.TREND_STATUS_COLUMN_NAME: "sem_dados_rank"
                })
            continue

        df_dezena_rank_history = df_ranks[['chunk_seq_id', 'chunk_end_contest', dezena_col_for_rank_value]].copy()
        df_dezena_rank_history.rename(columns={dezena_col_for_rank_value: 'rank_value'}, inplace=True)
        df_dezena_rank_history.dropna(subset=['rank_value'], inplace=True)
        df_dezena_rank_history = df_dezena_rank_history.sort_values(by='chunk_seq_id')
        
        slope = np.nan
        trend_status = "indefinido"
        target_contest_id = None
        
        if not df_dezena_rank_history.empty:
            target_contest_id = int(df_dezena_rank_history['chunk_end_contest'].iloc[-1]) if pd.notna(df_dezena_rank_history['chunk_end_contest'].iloc[-1]) else None

            if len(df_dezena_rank_history) >= 2:
                window_to_use = min(trend_window_blocks, len(df_dezena_rank_history))
                if window_to_use < 2:
                     trend_status = "insuficiente_janela"
                else:
                    df_window = df_dezena_rank_history.tail(window_to_use)
                    x_values = df_window['chunk_seq_id'].values
                    y_values = pd.to_numeric(df_window['rank_value'], errors='coerce').values
                    
                    valid_mask = ~np.isnan(y_values)
                    x_values_clean = x_values[valid_mask]
                    y_values_clean = y_values[valid_mask]

                    if len(x_values_clean) >= 2:
                        regression_result = linregress(x_values_clean, y_values_clean)
                        slope = regression_result.slope
                        
                        if pd.notna(slope):
                            if slope < slope_improving_threshold: 
                                trend_status = "melhorando"
                            elif slope > slope_worsening_threshold: 
                                trend_status = "piorando"
                            else:
                                trend_status = "estavel"
                        else:
                            trend_status = "indefinido_slope_nan"
                    else:
                        trend_status = "insuficiente_apos_limpeza_nan"
            else: 
                trend_status = "insuficiente_pontos_historico"
        else: 
             trend_status = "sem_historico_de_rank_para_dezena"
             if not df_ranks.empty and 'chunk_end_contest' in df_ranks.columns:
                 target_contest_id = int(df_ranks['chunk_end_contest'].max()) if pd.notna(df_ranks['chunk_end_contest'].max()) else None

        if target_contest_id is not None:
            all_trend_data.append({
                config.CONTEST_ID_COLUMN_NAME: target_contest_id,
                config.DEZENA_COLUMN_NAME: int(dezena_num),
                config.RANK_SLOPE_COLUMN_NAME: round(slope, 4) if pd.notna(slope) else None,
                config.TREND_STATUS_COLUMN_NAME: trend_status
            })
        
    if not all_trend_data:
        logger.warning("Nenhum dado de tendência de rank foi gerado após processar todas as dezenas.")
        return pd.DataFrame(columns=[config.CONTEST_ID_COLUMN_NAME, config.DEZENA_COLUMN_NAME, config.RANK_SLOPE_COLUMN_NAME, config.TREND_STATUS_COLUMN_NAME])
        
    df_final_trends = pd.DataFrame(all_trend_data)
    df_final_trends.dropna(subset=[config.CONTEST_ID_COLUMN_NAME], inplace=True)
    if not df_final_trends.empty:
        df_final_trends[config.CONTEST_ID_COLUMN_NAME] = df_final_trends[config.CONTEST_ID_COLUMN_NAME].astype(int)

    logger.info(f"Cálculo de tendências de rank (slope/status) concluído. {len(df_final_trends)} registros gerados.")
    return df_final_trends

--------------------------------------------------------------------------------
# Arquivo: src/analysis/recurrence_analysis.py
--------------------------------------------------------------------------------
# src/analysis/recurrence_analysis.py
import pandas as pd
import numpy as np
import json
import logging
from typing import List, Dict, Any, Tuple, Optional

logger = logging.getLogger(__name__)

def get_gaps_for_all_numbers(
    draw_matrix: pd.DataFrame,
    config: Any 
) -> Dict[int, List[int]]:
    logger.debug("Interno: Calculando gaps históricos para todas as dezenas.") # Mantido DEBUG
    if not hasattr(config, 'ALL_NUMBERS'):
        logger.error("Atributo 'ALL_NUMBERS' não encontrado no config para get_gaps_for_all_numbers.")
        return {i: [] for i in range(1, 26)} 

    if draw_matrix.empty:
        logger.warning("Matriz de sorteios para get_gaps_for_all_numbers está vazia.")
        return {dezena: [] for dezena in config.ALL_NUMBERS}

    all_gaps: Dict[int, List[int]] = {dezena: [] for dezena in config.ALL_NUMBERS}
    draw_matrix = draw_matrix.sort_index()

    for dezena_col in draw_matrix.columns:
        try:
            dezena = int(dezena_col)
            if dezena not in config.ALL_NUMBERS: continue
        except ValueError:
            logger.warning(f"Coluna não numérica '{dezena_col}' na draw_matrix. Pulando.")
            continue
        
        occurrences = draw_matrix.index[draw_matrix.loc[:, dezena] == 1].tolist()
        
        if len(occurrences) > 1:
            for i in range(len(occurrences) - 1):
                gap = occurrences[i+1] - occurrences[i] - 1
                all_gaps[dezena].append(gap)
            
    logger.debug("Interno: Cálculo de gaps históricos concluído.") # Mantido DEBUG
    return all_gaps

def calculate_recurrence_stats_for_number(
    gaps_list: List[int],
    current_delay: int 
) -> Tuple[Optional[float], int, Optional[float], Optional[int], Optional[float], Optional[int], str]:
    total_gaps = len(gaps_list)
    gaps_series = pd.Series(gaps_list, dtype=float)

    cdf_current_delay: Optional[float] = None
    mean_gaps: Optional[float] = None
    median_gaps_int: Optional[int] = None
    std_dev_gaps: Optional[float] = None
    max_gap_observed_int: Optional[int] = None

    if total_gaps > 0:
        count_le_delay = gaps_series[gaps_series <= current_delay].count()
        cdf_current_delay = round(count_le_delay / total_gaps, 6) if total_gaps > 0 else None
        mean_gaps = round(gaps_series.mean(), 2) if not gaps_series.empty else None
        median_val = gaps_series.median()
        median_gaps_int = int(median_val) if pd.notna(median_val) else None
        std_dev_gaps = round(gaps_series.std(ddof=0), 2) if total_gaps > 1 else (0.0 if total_gaps == 1 else None)
        max_gap_val = gaps_series.max()
        max_gap_observed_int = int(max_gap_val) if pd.notna(max_gap_val) else None
    
    gaps_for_json = [int(g) for g in gaps_list]
    gaps_json_str = json.dumps(gaps_for_json)

    return (
        cdf_current_delay, total_gaps, mean_gaps, median_gaps_int,
        std_dev_gaps, max_gap_observed_int, gaps_json_str
    )

def analyze_recurrence(
    draw_matrix: pd.DataFrame,
    current_delays_df: pd.DataFrame, 
    config: Any 
) -> pd.DataFrame:
    # logger.info("Iniciando análise de recorrência das dezenas.") # MUDADO PARA DEBUG
    logger.debug("Interno: Iniciando analyze_recurrence.")
    
    # ... (validações como antes) ...
    if not hasattr(config, 'ALL_NUMBERS'): # etc.
        # ...
        return pd.DataFrame() # ou com estrutura default
    if draw_matrix.empty: # etc.
        # ...
        # Exemplo de retorno com estrutura para consistência
        empty_cols = ['Atraso_Atual_Input', 'CDF_Atraso_Atual', 'Total_Gaps_Observados', 'Media_Gaps', 'Mediana_Gaps', 'Std_Dev_Gaps', 'Max_Gap_Observado', 'Gaps_Observados_json']
        empty_data = {col: (0 if 'Input' in col or 'Total' in col else (json.dumps([]) if 'json' in col else pd.NA)) for col in empty_cols}
        empty_data[config.DEZENA_COLUMN_NAME] = config.ALL_NUMBERS
        return pd.DataFrame(empty_data)


    all_historical_gaps = get_gaps_for_all_numbers(draw_matrix, config)
    recurrence_data = []
    dezena_col_input = config.DEZENA_COLUMN_NAME # Usar constante
    current_delay_col_input = 'current_delay' # Nome da coluna no current_delays_df

    if dezena_col_input not in current_delays_df.columns or current_delay_col_input not in current_delays_df.columns:
        logger.error(f"Colunas '{dezena_col_input}' ou '{current_delay_col_input}' não encontradas em current_delays_df.")
        return pd.DataFrame() # Retornar DataFrame vazio ou com estrutura default

    for dezena_val_loop in config.ALL_NUMBERS:
        gaps_list = all_historical_gaps.get(dezena_val_loop, [])
        
        current_delay_series = current_delays_df[current_delays_df[dezena_col_input] == dezena_val_loop][current_delay_col_input]
        
        current_delay_as_int: int
        if current_delay_series.empty or pd.isna(current_delay_series.iloc[0]):
             current_delay_as_int = len(draw_matrix) 
        else:
            try:
                current_delay_as_int = int(current_delay_series.iloc[0])
            except ValueError:
                logger.warning(f"Não convertível para int: atraso atual para dezena {dezena_val_loop}. Usando atraso máximo.")
                current_delay_as_int = len(draw_matrix)

        (cdf_val, total_gaps, mean_g, med_g, std_g, max_g, gaps_j_str) = calculate_recurrence_stats_for_number(
            gaps_list, current_delay_as_int
        )
        
        recurrence_data.append({
            config.DEZENA_COLUMN_NAME: dezena_val_loop,
            'Atraso_Atual_Input': current_delay_as_int, # Mantido para possível uso futuro, mas não vai para a tabela final
            'CDF_Atraso_Atual': cdf_val, 
            'Total_Gaps_Observados': total_gaps,
            'Media_Gaps': mean_g,
            'Mediana_Gaps': med_g,
            'Std_Dev_Gaps': std_g,
            'Max_Gap_Observado': max_g,
            'Gaps_Observados_json': gaps_j_str
        })

    result_df = pd.DataFrame(recurrence_data)
    # logger.info(f"Análise de recorrência concluída. {len(result_df)} dezenas processadas.") # MUDADO PARA DEBUG
    logger.debug(f"Interno: Análise de recorrência concluída. {len(result_df)} dezenas processadas.")
    return result_df

--------------------------------------------------------------------------------
# Arquivo: src/analysis/repetition_analysis.py
--------------------------------------------------------------------------------
# src/analysis/repetition_analysis.py
import pandas as pd
from typing import List, Dict, Any, Set 
import logging

logger = logging.getLogger(__name__)

def calculate_previous_draw_repetitions(all_data_df: pd.DataFrame, config: Any) -> pd.DataFrame: # Recebe config
    logger.info("Iniciando análise de repetição de dezenas do concurso anterior.")
    
    contest_col = config.CONTEST_ID_COLUMN_NAME
    date_col = config.DATE_COLUMN_NAME
    ball_cols = config.BALL_NUMBER_COLUMNS

    # Define colunas default para o DataFrame de resultado
    default_cols_result = [contest_col, 'QtdDezenasRepetidas', 'DezenasRepetidas']
    if date_col in all_data_df.columns: # Adiciona coluna de data se existir no input
        default_cols_result.insert(1, date_col)

    if all_data_df is None or all_data_df.empty or len(all_data_df) < 2:
        logger.warning("DataFrame de entrada insuficiente para análise de repetição (necessário >= 2 concursos).")
        return pd.DataFrame(columns=default_cols_result)

    required_cols = [contest_col] + ball_cols
    missing_cols = [col for col in required_cols if col not in all_data_df.columns]
    if missing_cols:
        logger.error(f"Colunas essenciais ausentes: {missing_cols} (Esperado: {required_cols}). Colunas disponíveis: {all_data_df.columns.tolist()}")
        return pd.DataFrame(columns=default_cols_result)

    df_sorted = all_data_df.copy()
    try:
        df_sorted[contest_col] = pd.to_numeric(df_sorted[contest_col])
    except Exception as e_conv:
        logger.error(f"Não foi possível converter '{contest_col}' para numérico: {e_conv}")
        return pd.DataFrame(columns=default_cols_result)
    df_sorted = df_sorted.sort_values(by=contest_col).reset_index(drop=True)
    
    repetition_data: List[Dict[str, Any]] = []

    for i in range(1, len(df_sorted)):
        current_row = df_sorted.iloc[i]
        previous_row = df_sorted.iloc[i-1]
        current_contest_id = int(current_row[contest_col])

        try:
            current_draw_numbers = set(int(num) for col_name in ball_cols if pd.notna(current_row[col_name]) for num in [current_row[col_name]])
            previous_draw_numbers = set(int(num) for col_name in ball_cols if pd.notna(previous_row[col_name]) for num in [previous_row[col_name]])

            repeated_numbers = current_draw_numbers.intersection(previous_draw_numbers)
            repeated_count = len(repeated_numbers)
            repeated_numbers_str = ",".join(map(str, sorted(list(repeated_numbers)))) if repeated_numbers else None
            
            data_entry: Dict[str, Any] = {
                contest_col: current_contest_id,
                'QtdDezenasRepetidas': repeated_count,
                'DezenasRepetidas': repeated_numbers_str
            }
            if date_col in current_row and pd.notna(current_row[date_col]):
                data_entry[date_col] = current_row[date_col]
            repetition_data.append(data_entry)
        except Exception as e:
            logger.error(f"Erro ao processar repetição para o concurso {current_contest_id}: {e}", exc_info=True)
            error_entry: Dict[str,Any] = {contest_col: current_contest_id, 'QtdDezenasRepetidas': 0, 'DezenasRepetidas': None}
            if date_col in current_row and pd.notna(current_row[date_col]): error_entry[date_col] = current_row[date_col]
            repetition_data.append(error_entry)

    if not repetition_data:
        logger.warning("Nenhum dado de repetição foi gerado.")
        return pd.DataFrame(columns=default_cols_result)
        
    df_repetitions = pd.DataFrame(repetition_data)
    
    # Reordenar e renomear colunas para o padrão da tabela final
    final_ordered_cols = ["Concurso"]
    if "Data" in default_cols_result or date_col in df_repetitions.columns : # Se a coluna de data deve estar no resultado
        final_ordered_cols.append("Data")
    final_ordered_cols.extend(['QtdDezenasRepetidas', 'DezenasRepetidas'])
    
    # Renomeia as colunas do config para os nomes fixos da tabela
    rename_map_final = {
        contest_col: "Concurso",
        date_col: "Data" # Se date_col foi usado e é diferente de "Data"
    }
    df_repetitions.rename(columns=rename_map_final, inplace=True, errors='ignore')
    
    # Garante que todas as colunas de final_ordered_cols existam e estejam na ordem correta
    existing_final_cols = [col for col in final_ordered_cols if col in df_repetitions.columns]
    df_repetitions = df_repetitions[existing_final_cols]
    
    logger.info(f"Análise de repetição concluída. {len(df_repetitions)} registros gerados.")
    return df_repetitions

--------------------------------------------------------------------------------
# Arquivo: src/analysis/seasonality_analysis.py
--------------------------------------------------------------------------------
# src/analysis/seasonality_analysis.py
import pandas as pd
import numpy as np
import logging
from typing import Any, Dict, List # Adicionado List
from collections import Counter

logger = logging.getLogger(__name__)

def analyze_monthly_number_frequency(
    all_draws_df: pd.DataFrame, 
    config: Any
) -> pd.DataFrame:
    """
    Analisa a frequência de ocorrência de cada dezena por mês, agregando todos os anos.
    """
    step_name = "Análise de Frequência Mensal de Dezenas" # Corrigido para nome da sub-análise
    logger.info(f"Iniciando {step_name}.")

    date_col = config.DATE_COLUMN_NAME
    drawn_numbers_col = config.DRAWN_NUMBERS_COLUMN_NAME
    all_numbers = config.ALL_NUMBERS

    # Validações de colunas
    if date_col not in all_draws_df.columns:
        logger.error(f"Coluna de data '{date_col}' não encontrada no DataFrame de sorteios.")
        return pd.DataFrame(columns=['Dezena', 'Mes', 'Frequencia_Absoluta_Total_Mes', 'Total_Sorteios_Considerados_Mes', 'Frequencia_Relativa_Mes'])
    if drawn_numbers_col not in all_draws_df.columns:
        logger.error(f"Coluna de dezenas sorteadas '{drawn_numbers_col}' não encontrada.")
        return pd.DataFrame(columns=['Dezena', 'Mes', 'Frequencia_Absoluta_Total_Mes', 'Total_Sorteios_Considerados_Mes', 'Frequencia_Relativa_Mes'])

    try:
        df_analysis = all_draws_df[[date_col, drawn_numbers_col]].copy()
        df_analysis[date_col] = pd.to_datetime(df_analysis[date_col], errors='coerce')
        df_analysis.dropna(subset=[date_col], inplace=True)
    except Exception as e:
        logger.error(f"Erro ao processar a coluna de data '{date_col}': {e}", exc_info=True)
        return pd.DataFrame(columns=['Dezena', 'Mes', 'Frequencia_Absoluta_Total_Mes', 'Total_Sorteios_Considerados_Mes', 'Frequencia_Relativa_Mes'])

    if df_analysis.empty:
        logger.warning("DataFrame vazio após processamento da coluna de data. Nenhuma análise de frequência mensal será realizada.")
        return pd.DataFrame(columns=['Dezena', 'Mes', 'Frequencia_Absoluta_Total_Mes', 'Total_Sorteios_Considerados_Mes', 'Frequencia_Relativa_Mes'])

    df_analysis['Mes'] = df_analysis[date_col].dt.month
    total_draws_overall = len(df_analysis) # Usado para logs ou outras métricas, não para a freq relativa mensal direta

    # Contar total de sorteios por mês (agregando todos os anos)
    total_draws_per_month_map = df_analysis['Mes'].value_counts().sort_index().to_dict()

    monthly_freq_data = []
    for month in range(1, 13):
        draws_in_month_df = df_analysis[df_analysis['Mes'] == month]
        total_draws_this_month_all_years = total_draws_per_month_map.get(month, 0)
        
        if not draws_in_month_df.empty and total_draws_this_month_all_years > 0:
            all_numbers_in_month_list = []
            for numbers_list in draws_in_month_df[drawn_numbers_col]:
                if isinstance(numbers_list, list):
                    all_numbers_in_month_list.extend(numbers_list)
            
            month_counts = pd.Series(all_numbers_in_month_list).value_counts()
            
            for dezena in all_numbers:
                abs_freq = month_counts.get(dezena, 0)
                # Frequencia_Relativa_Mes = Proporção de sorteios DO MÊS em que a dezena apareceu
                rel_freq = (abs_freq / total_draws_this_month_all_years) if total_draws_this_month_all_years > 0 else 0.0
                
                monthly_freq_data.append({
                    'Dezena': dezena,
                    'Mes': month,
                    'Frequencia_Absoluta_Total_Mes': abs_freq,
                    'Total_Sorteios_Considerados_Mes': total_draws_this_month_all_years,
                    'Frequencia_Relativa_Mes': round(rel_freq, 6)
                })
        else:
            for dezena in all_numbers:
                monthly_freq_data.append({
                    'Dezena': dezena,
                    'Mes': month,
                    'Frequencia_Absoluta_Total_Mes': 0,
                    'Total_Sorteios_Considerados_Mes': 0,
                    'Frequencia_Relativa_Mes': 0.0
                })

    result_df = pd.DataFrame(monthly_freq_data)
    if not result_df.empty:
        result_df = result_df.sort_values(by=['Dezena', 'Mes']).reset_index(drop=True)
        
    logger.info(f"{step_name} concluída. {len(result_df)} registros gerados.")
    return result_df

# --- NOVA FUNÇÃO PARA PROPRIEDADES NUMÉRICAS MENSAIS ---
def analyze_monthly_draw_properties(
    all_draws_df: pd.DataFrame, # Usado para obter a coluna de data e fazer o merge
    properties_df: pd.DataFrame, # DataFrame da tabela 'propriedades_numericas_por_concurso'
    config: Any
) -> pd.DataFrame:
    """
    Analisa o sumário de propriedades numéricas dos sorteios (soma, pares, ímpares, primos)
    agregados por mês.

    Args:
        all_draws_df (pd.DataFrame): DataFrame com todos os sorteios, deve conter
                                     config.DATE_COLUMN_NAME e config.CONTEST_ID_COLUMN_NAME.
        properties_df (pd.DataFrame): DataFrame com as propriedades numéricas por concurso.
                                      Deve conter config.CONTEST_ID_COLUMN_NAME e as colunas
                                      de propriedades (ex: 'soma_dezenas', 'pares', 'impares', 'primos').
        config (Any): Objeto de configuração.

    Returns:
        pd.DataFrame: DataFrame com o sumário de propriedades médias por mês.
                      Colunas: 'Mes', 'Total_Sorteios_Mes', 'Soma_Media_Mensal', 
                               'Media_Pares_Mensal', 'Media_Impares_Mensal', 'Media_Primos_Mensal'.
    """
    step_name = "Análise de Sumário Mensal de Propriedades Numéricas"
    logger.info(f"Iniciando {step_name}.")

    date_col = config.DATE_COLUMN_NAME
    contest_id_col = config.CONTEST_ID_COLUMN_NAME

    # Colunas de propriedades que esperamos em properties_df e queremos agregar
    # A coluna 'Concurso' em properties_df corresponde a contest_id_col
    property_cols_to_aggregate = ['soma_dezenas', 'pares', 'impares', 'primos']

    if date_col not in all_draws_df.columns:
        logger.error(f"Coluna de data '{date_col}' não encontrada em all_draws_df.")
        return pd.DataFrame(columns=['Mes', 'Total_Sorteios_Mes'] + [f"Media_{col}_Mensal" for col in property_cols_to_aggregate]) # Ajustado nome da coluna soma
    if contest_id_col not in all_draws_df.columns:
        logger.error(f"Coluna de ID do concurso '{contest_id_col}' não encontrada em all_draws_df.")
        return pd.DataFrame(columns=['Mes', 'Total_Sorteios_Mes'] + [f"Media_{col}_Mensal" for col in property_cols_to_aggregate])
    
    if properties_df.empty:
        logger.warning("DataFrame de propriedades numéricas está vazio.")
        return pd.DataFrame(columns=['Mes', 'Total_Sorteios_Mes'] + [f"Media_{col}_Mensal" for col in property_cols_to_aggregate])
    
    # Verifica se a coluna de ID e as colunas de propriedades existem em properties_df
    # A coluna de ID em properties_df é "Concurso" (hardcoded no _create_table)
    properties_contest_id_col = "Concurso" # Nome da coluna em propriedades_numericas_por_concurso
    if properties_contest_id_col not in properties_df.columns:
        logger.error(f"Coluna de ID do concurso '{properties_contest_id_col}' não encontrada em properties_df.")
        return pd.DataFrame(columns=['Mes', 'Total_Sorteios_Mes'] + [f"Media_{col}_Mensal" for col in property_cols_to_aggregate])
    
    missing_prop_cols = [p_col for p_col in property_cols_to_aggregate if p_col not in properties_df.columns]
    if missing_prop_cols:
        logger.error(f"Colunas de propriedades ausentes em properties_df: {missing_prop_cols}.")
        return pd.DataFrame(columns=['Mes', 'Total_Sorteios_Mes'] + [f"Media_{col}_Mensal" for col in property_cols_to_aggregate])


    # Prepara df_dates com [contest_id, Mes]
    try:
        df_dates = all_draws_df[[contest_id_col, date_col]].copy()
        df_dates[date_col] = pd.to_datetime(df_dates[date_col], errors='coerce')
        df_dates.dropna(subset=[date_col], inplace=True)
        df_dates['Mes'] = df_dates[date_col].dt.month
        # Garante que contest_id_col seja do mesmo tipo que properties_contest_id_col para o merge
        df_dates[contest_id_col] = df_dates[contest_id_col].astype(properties_df[properties_contest_id_col].dtype)
    except Exception as e:
        logger.error(f"Erro ao processar datas e IDs de concurso: {e}", exc_info=True)
        return pd.DataFrame(columns=['Mes', 'Total_Sorteios_Mes'] + [f"Media_{col}_Mensal" for col in property_cols_to_aggregate])

    if df_dates.empty:
        logger.warning("DataFrame de datas vazio após processamento. Nenhuma análise será realizada.")
        return pd.DataFrame(columns=['Mes', 'Total_Sorteios_Mes'] + [f"Media_{col}_Mensal" for col in property_cols_to_aggregate])

    # Faz o merge para adicionar o Mês ao properties_df
    # Renomeia a coluna de ID em properties_df para corresponder a contest_id_col para o merge, se necessário,
    # ou usa left_on e right_on.
    # Vamos assumir que config.CONTEST_ID_COLUMN_NAME em all_draws_df é 'contest_id'
    # e a coluna em properties_df é 'Concurso'.
    merged_df = pd.merge(
        df_dates[[contest_id_col, 'Mes']], 
        properties_df[[properties_contest_id_col] + property_cols_to_aggregate],
        left_on=contest_id_col,
        right_on=properties_contest_id_col,
        how='inner' # Usa inner para garantir que apenas concursos com ambas as informações sejam usados
    )

    if merged_df.empty:
        logger.warning("DataFrame merged_df vazio após merge de datas e propriedades. Nenhuma análise será realizada.")
        return pd.DataFrame(columns=['Mes', 'Total_Sorteios_Mes'] + [f"Media_{col}_Mensal" for col in property_cols_to_aggregate])

    # Agrupa por Mês e calcula as médias e contagens
    # Primeiro, a contagem de sorteios por mês
    monthly_summary_counts = merged_df.groupby('Mes')[contest_id_col].count().reset_index(name='Total_Sorteios_Mes')

    # Depois, as médias das propriedades
    aggregation_dict = {prop_col: 'mean' for prop_col in property_cols_to_aggregate}
    monthly_summary_means = merged_df.groupby('Mes').agg(aggregation_dict).reset_index()
    
    # Renomeia as colunas de médias
    rename_map_means = {prop_col: f"Media_{prop_col}_Mensal" for prop_col in property_cols_to_aggregate}
    # Ajuste para a coluna de soma, para manter o padrão:
    if 'soma_dezenas' in rename_map_means:
        rename_map_means['soma_dezenas'] = 'Soma_Media_Mensal' # Nome da coluna como definido no DB
    
    monthly_summary_means.rename(columns=rename_map_means, inplace=True)

    # Junta as contagens com as médias
    final_summary_df = pd.merge(monthly_summary_counts, monthly_summary_means, on='Mes', how='outer')
    
    # Garante que todos os meses (1-12) estejam presentes, preenchendo com 0 ou NaN se não houver dados
    all_months_df = pd.DataFrame({'Mes': range(1, 13)})
    final_summary_df = pd.merge(all_months_df, final_summary_df, on='Mes', how='left')
    
    # Preenche NaNs nas contagens com 0 e nas médias com np.nan (ou 0.0 se preferir)
    if 'Total_Sorteios_Mes' in final_summary_df.columns:
        final_summary_df['Total_Sorteios_Mes'] = final_summary_df['Total_Sorteios_Mes'].fillna(0).astype(int)
    
    for col in final_summary_df.columns:
        if col != 'Mes' and col != 'Total_Sorteios_Mes':
            final_summary_df[col] = final_summary_df[col].fillna(np.nan) # ou 0.0
            # Arredondar as médias para melhor apresentação
            if pd.api.types.is_numeric_dtype(final_summary_df[col]):
                 final_summary_df[col] = final_summary_df[col].round(2)


    final_summary_df = final_summary_df.sort_values(by='Mes').reset_index(drop=True)

    logger.info(f"{step_name} concluída. {len(final_summary_df)} registros gerados.")
    return final_summary_df

--------------------------------------------------------------------------------
# Arquivo: src/analysis/sequence_analysis.py
--------------------------------------------------------------------------------
# src/analysis/sequence_analysis.py
import pandas as pd
from typing import List, Dict, Tuple, Any
import logging
from collections import defaultdict

try:
    from ..config import Config, CONTEST_ID_COLUMN_NAME, \
                         SEQUENCE_ANALYSIS_CONFIG, SEQUENCE_METRICS_TABLE_NAME 
except ImportError:
    from src.config import Config, CONTEST_ID_COLUMN_NAME, \
                         SEQUENCE_ANALYSIS_CONFIG, SEQUENCE_METRICS_TABLE_NAME

logger = logging.getLogger(__name__)

def _find_consecutive_sequences_in_draw(
    draw_numbers_sorted: List[int], 
    min_len: int, 
    max_len: int
) -> Dict[int, List[Tuple[int, ...]]]:
    found_sequences_by_length: Dict[int, List[Tuple[int, ...]]] = {
        length: [] for length in range(min_len, max_len + 1)
    }
    n = len(draw_numbers_sorted)
    if n == 0 or n < min_len:
        return found_sequences_by_length
    for length_to_check in range(min_len, max_len + 1):
        if n < length_to_check:
            continue
        for i in range(n - length_to_check + 1):
            current_subsequence = draw_numbers_sorted[i : i + length_to_check]
            is_consecutive_flag = True 
            for k in range(len(current_subsequence) - 1):
                if current_subsequence[k+1] - current_subsequence[k] != 1:
                    is_consecutive_flag = False
                    break
            if is_consecutive_flag:
                found_sequences_by_length[length_to_check].append(tuple(current_subsequence))
    return found_sequences_by_length

# <<< NOVA FUNÇÃO AUXILIAR PARA SEQUÊNCIAS ARITMÉTICAS >>>
def _find_arithmetic_sequences_in_draw(
    draw_numbers_sorted: List[int],
    min_len: int,
    max_len: int,
    step_value: int
) -> Dict[int, List[Tuple[int, ...]]]:
    """
    Encontra todas as subsequências aritméticas com um 'step_value' específico
    de comprimentos entre min_len e max_len em uma lista ordenada de números de um sorteio.
    """
    found_sequences_by_length: Dict[int, List[Tuple[int, ...]]] = {
        length: [] for length in range(min_len, max_len + 1)
    }
    n = len(draw_numbers_sorted)

    if n == 0 or n < min_len or step_value <= 0:
        return found_sequences_by_length

    for length_to_check in range(min_len, max_len + 1):
        if n < length_to_check:
            continue
        
        # Para encontrar sequências aritméticas, precisamos de uma abordagem diferente da janela deslizante simples.
        # Podemos iterar sobre todas as combinações de 'length_to_check' números do sorteio
        # e verificar se formam uma progressão aritmética com o 'step_value'.
        # No entanto, isso pode ser computacionalmente caro (C(15, k)).
        # Uma abordagem mais eficiente é iterar sobre os números e tentar construir sequências.

        # Abordagem: Iterar sobre cada número como um potencial início de sequência
        for i in range(n):
            potential_sequence = [draw_numbers_sorted[i]]
            current_num = draw_numbers_sorted[i]
            
            # Tenta construir uma sequência do tamanho 'length_to_check'
            for _ in range(1, length_to_check):
                next_expected_num = current_num + step_value
                # Otimização: buscar 'next_expected_num' no restante da lista 'draw_numbers_sorted'
                # Isso é mais eficiente do que iterar sobre todas as subsequências.
                # Como draw_numbers_sorted é ordenada, podemos fazer uma busca eficiente.
                found_next = False
                for j in range(i + len(potential_sequence), n): # Busca a partir do próximo índice
                    if draw_numbers_sorted[j] == next_expected_num:
                        potential_sequence.append(next_expected_num)
                        current_num = next_expected_num
                        found_next = True
                        break # Encontrou o próximo número esperado, para este passo da sequência
                    elif draw_numbers_sorted[j] > next_expected_num:
                        break # Passou do ponto onde o número poderia estar
                
                if not found_next: # Se não encontrou o próximo número esperado para a sequência
                    break # Quebra a tentativa de construir esta sequência particular
            
            if len(potential_sequence) == length_to_check:
                found_sequences_by_length[length_to_check].append(tuple(potential_sequence))
                
    return found_sequences_by_length
# <<< FIM DA NOVA FUNÇÃO AUXILIAR >>>

def analyze_sequences(
    all_draws_df: pd.DataFrame, 
    config_obj_instance: Config 
) -> pd.DataFrame:
    logger.info("==== INICIANDO ANÁLISE DE SEQUÊNCIAS NUMÉRICAS ====")
    
    # logger.info(f"Usando SEQUENCE_ANALYSIS_CONFIG do módulo config: {SEQUENCE_ANALYSIS_CONFIG}")
    
    results_list = [] 
    
    # --- Análise de Sequências Consecutivas ---
    consecutive_config = SEQUENCE_ANALYSIS_CONFIG.get("consecutive", {})
    min_len_consecutive = consecutive_config.get("min_len", 3)
    max_len_consecutive = consecutive_config.get("max_len", 5)
    is_active_consecutive = consecutive_config.get("active", False) 

    logger.info(f"Config para 'consecutive': min_len={min_len_consecutive}, max_len={max_len_consecutive}, active={is_active_consecutive} (Tipo: {type(is_active_consecutive)})")

    if 'drawn_numbers' not in all_draws_df.columns:
        msg = "DataFrame 'all_draws_df' deve conter a coluna 'drawn_numbers' com listas de dezenas ordenadas."
        logger.error(msg)
        # Se a coluna crucial faltar e QUALQUER análise de sequência estiver ativa, melhor parar.
        if is_active_consecutive or SEQUENCE_ANALYSIS_CONFIG.get("arithmetic_steps", {}).get("active", False):
             raise ValueError(msg)
        else:
            logger.warning(msg + " Nenhuma análise de sequência ativa que dependa desta coluna.")

    if is_active_consecutive:
        if 'drawn_numbers' in all_draws_df.columns:
            logger.info(f"Analisando sequências CONSECUTIVAS de comprimento {min_len_consecutive} a {max_len_consecutive}.")
            sequence_counts_consecutive: Dict[int, Dict[Tuple[int, ...], int]] = {
                length: defaultdict(int) for length in range(min_len_consecutive, max_len_consecutive + 1)
            }
            draws_with_consecutive_sequence_of_length: Dict[int, int] = defaultdict(int)
            total_draws = len(all_draws_df)

            if total_draws > 0:
                for _, row in all_draws_df.iterrows():
                    draw_numbers_sorted = row.get('drawn_numbers')
                    if not isinstance(draw_numbers_sorted, list) or not all(isinstance(n, int) for n in draw_numbers_sorted):
                        contest_id_val = row.get(CONTEST_ID_COLUMN_NAME, "Desconhecido") 
                        logger.warning(f"Sorteio {contest_id_val} tem 'drawn_numbers' inválido. Pulando para sequências. Conteúdo: {draw_numbers_sorted}")
                        continue
                    draw_numbers_sorted.sort() 
                    sequences_found_in_this_draw = _find_consecutive_sequences_in_draw(draw_numbers_sorted, min_len_consecutive, max_len_consecutive)
                    for length, sequences_list_found in sequences_found_in_this_draw.items(): 
                        if sequences_list_found: 
                            draws_with_consecutive_sequence_of_length[length] += 1
                            for seq_tuple in sequences_list_found:
                                sequence_counts_consecutive[length][seq_tuple] += 1
                
                for length_iter in range(min_len_consecutive, max_len_consecutive + 1):
                    total_draws_with_this_len_seq = draws_with_consecutive_sequence_of_length[length_iter]
                    support_for_len = (total_draws_with_this_len_seq / total_draws) if total_draws > 0 else 0.0
                    results_list.append({
                        "sequence_description": f"Qualquer sequência consecutiva de {length_iter} dezenas",
                        "sequence_type": "consecutive_any", "length": length_iter, "step": 1, 
                        "specific_sequence": "N/A", 
                        "frequency_count": total_draws_with_this_len_seq, "support": round(support_for_len, 6) 
                    })
                    for seq_tuple, count in sorted(sequence_counts_consecutive[length_iter].items()):
                        seq_str = "-".join(map(str, seq_tuple))
                        support_specific = (count / total_draws) if total_draws > 0 else 0.0
                        results_list.append({
                            "sequence_description": f"Sequência específica: {seq_str}",
                            "sequence_type": "consecutive_specific", "length": length_iter, "step": 1,
                            "specific_sequence": seq_str, "frequency_count": count, "support": round(support_specific, 6)
                        })
            else: 
                logger.warning("Nenhum sorteio para analisar sequências consecutivas.")
        else: 
            logger.warning("Coluna 'drawn_numbers' não encontrada, pulando análise de sequências consecutivas.")
    else:
        logger.warning("Análise de sequências CONSECUTIVAS está DESATIVADA na configuração (active=False).")

    # <<< INÍCIO DA LÓGICA PARA "arithmetic_steps" >>>
    arithmetic_config = SEQUENCE_ANALYSIS_CONFIG.get("arithmetic_steps", {}) 
    is_arithmetic_active = arithmetic_config.get("active", False)
    logger.info(f"Config para 'arithmetic_steps': active={is_arithmetic_active}")

    if is_arithmetic_active:
        if 'drawn_numbers' not in all_draws_df.columns:
            logger.error("Coluna 'drawn_numbers' necessária para análise aritmética, mas ausente. Pulando análise aritmética.")
        else:
            steps_to_check = arithmetic_config.get("steps_to_check", [])
            min_len_arith = arithmetic_config.get("min_len", 3)
            max_len_arith = arithmetic_config.get("max_len", 4)
            
            logger.info(f"Analisando sequências ARITMÉTICAS com steps {steps_to_check}, comprimentos de {min_len_arith} a {max_len_arith}.")

            total_draws = len(all_draws_df) # Já definido, mas para clareza se este bloco for isolado

            for step_value in steps_to_check:
                if step_value <= 0:
                    logger.warning(f"Step inválido ({step_value}) na configuração. Pulando este step.")
                    continue
                
                logger.info(f"Analisando para step: {step_value}")
                sequence_counts_arith: Dict[int, Dict[Tuple[int, ...], int]] = {
                    length: defaultdict(int) for length in range(min_len_arith, max_len_arith + 1)
                }
                draws_with_arithmetic_sequence_of_length: Dict[int, int] = defaultdict(int)

                if total_draws > 0:
                    for _, row in all_draws_df.iterrows():
                        draw_numbers_sorted = row.get('drawn_numbers')
                        if not isinstance(draw_numbers_sorted, list) or not all(isinstance(n, int) for n in draw_numbers_sorted):
                            continue # Já logado acima
                        draw_numbers_sorted.sort()
                        
                        sequences_found_this_draw_step = _find_arithmetic_sequences_in_draw(
                            draw_numbers_sorted, min_len_arith, max_len_arith, step_value
                        )
                        for length, sequences_list_found in sequences_found_this_draw_step.items():
                            if sequences_list_found:
                                draws_with_arithmetic_sequence_of_length[length] += 1
                                for seq_tuple in sequences_list_found:
                                    sequence_counts_arith[length][seq_tuple] += 1
                    
                    for length_iter in range(min_len_arith, max_len_arith + 1):
                        total_draws_with_this_len_step_seq = draws_with_arithmetic_sequence_of_length[length_iter]
                        support_for_len_step = (total_draws_with_this_len_step_seq / total_draws) if total_draws > 0 else 0.0
                        results_list.append({
                            "sequence_description": f"Qualquer sequência aritmética (step {step_value}) de {length_iter} dezenas",
                            "sequence_type": f"arithmetic_step_{step_value}_any",
                            "length": length_iter,
                            "step": step_value,
                            "specific_sequence": "N/A",
                            "frequency_count": total_draws_with_this_len_step_seq,
                            "support": round(support_for_len_step, 6)
                        })
                        for seq_tuple, count in sorted(sequence_counts_arith[length_iter].items()):
                            seq_str = "-".join(map(str, seq_tuple))
                            support_specific_step = (count / total_draws) if total_draws > 0 else 0.0
                            results_list.append({
                                "sequence_description": f"Sequência aritmética específica (step {step_value}): {seq_str}",
                                "sequence_type": f"arithmetic_step_{step_value}_specific",
                                "length": length_iter,
                                "step": step_value,
                                "specific_sequence": seq_str,
                                "frequency_count": count,
                                "support": round(support_specific_step, 6)
                            })
                else: # total_draws == 0
                    logger.warning(f"Nenhum sorteio para analisar sequências aritméticas com step {step_value}.")
    else: # not is_arithmetic_active
        logger.warning("Análise de sequências ARITMÉTICAS está DESATIVADA na configuração (active=False).")
    # <<< FIM DA LÓGICA PARA "arithmetic_steps" >>>

    logger.info("Análise de sequências numéricas concluída.")
    if not results_list: 
        logger.info("Nenhuma métrica de sequência (ativa e implementada) foi gerada.")
        return pd.DataFrame(columns=["sequence_description", "sequence_type", "length", "step", "specific_sequence", "frequency_count", "support"])

    return pd.DataFrame(results_list)

--------------------------------------------------------------------------------
# Arquivo: src/analysis/statistical_tests_analysis.py
--------------------------------------------------------------------------------
# src/analysis/statistical_tests_analysis.py
import pandas as pd
import numpy as np
from scipy import stats 
import logging
import json 
from typing import Dict, Any, Optional, List 

logger = logging.getLogger(__name__)

def perform_chi_square_test_number_frequencies(
    observed_frequencies_df: pd.DataFrame, 
    total_draws: int, 
    config: Any,
    alpha: float = 0.05
) -> Optional[Dict[str, Any]]:
    # ... (código existente desta função, sem alterações) ...
    test_name = "ChiSquare_NumberFrequencies_Uniformity"
    logger.info(f"Iniciando teste: {test_name}")

    if observed_frequencies_df.empty:
        logger.warning(f"{test_name}: DataFrame de frequências observadas está vazio.")
        return None
    if not all(col in observed_frequencies_df.columns for col in ['Dezena', 'Frequencia Absoluta']):
        logger.error(f"{test_name}: Colunas 'Dezena' ou 'Frequencia Absoluta' ausentes no DataFrame de frequências.")
        return None
    if total_draws <= 0:
        logger.error(f"{test_name}: Número total de sorteios ({total_draws}) deve ser positivo.")
        return None

    observed_frequencies_df = observed_frequencies_df.sort_values(by='Dezena').reset_index(drop=True)
    observed_counts = observed_frequencies_df['Frequencia Absoluta'].values
    
    if len(observed_counts) != len(config.ALL_NUMBERS):
        logger.error(f"{test_name}: Número de dezenas observadas ({len(observed_counts)}) difere do esperado ({len(config.ALL_NUMBERS)}).")
        return None

    total_drawn_ball_slots = total_draws * config.NUMBERS_PER_DRAW
    expected_frequency_per_number = total_drawn_ball_slots / len(config.ALL_NUMBERS)
    expected_counts = np.full_like(observed_counts, fill_value=expected_frequency_per_number, dtype=float)

    if np.any(expected_counts < 5):
        logger.warning(f"{test_name}: Algumas frequências esperadas são menores que 5. "
                       "O teste Qui-Quadrado pode não ser preciso. (Menor E_i: {expected_counts.min()})")

    try:
        chi2_statistic, p_value = stats.chisquare(f_obs=observed_counts, f_exp=expected_counts)
    except Exception as e:
        logger.error(f"{test_name}: Erro ao calcular o teste Qui-Quadrado: {e}", exc_info=True)
        return None

    degrees_of_freedom = len(config.ALL_NUMBERS) - 1 
    
    conclusion = ""
    if p_value < alpha:
        conclusion = (f"Rejeita H0 (p={p_value:.4f} < alpha={alpha}). "
                      "Evidência de que as dezenas NÃO são sorteadas uniformemente.")
    else:
        conclusion = (f"Não rejeita H0 (p={p_value:.4f} >= alpha={alpha}). "
                      "Sem evidência estatística contra a uniformidade das dezenas.")

    parameters_dict = {
        "total_observations": int(np.sum(observed_counts)),
        "expected_frequency_per_category": round(expected_frequency_per_number, 2),
        "number_of_categories": len(config.ALL_NUMBERS)
    }

    result = {
        "Test_Name": test_name,
        "Chi2_Statistic": round(chi2_statistic, 4),
        "P_Value": round(p_value, 6),
        "Degrees_Freedom": degrees_of_freedom,
        "Alpha_Level": alpha,
        "Conclusion": conclusion,
        "Parameters": json.dumps(parameters_dict),
        "Notes": "H0: As frequências observadas das dezenas são consistentes com uma distribuição uniforme."
    }
    logger.info(f"{test_name} concluído. P-valor: {p_value:.6f}. Conclusão (alpha={alpha}): {conclusion}")
    return result

def perform_normality_test_for_sum_of_numbers(
    sum_of_numbers_series: pd.Series,
    config: Any,
    method: str = 'chi_square_bins',
    alpha: float = 0.05
) -> Optional[Dict[str, Any]]:
    # ... (código existente desta função, sem alterações) ...
    if sum_of_numbers_series.empty or sum_of_numbers_series.nunique() < 2 :
        logger.warning(f"Série de soma das dezenas está vazia ou não tem variação suficiente para o teste de normalidade.")
        return None

    test_name_base = "NormalityTest_SumOfNumbers"
    test_result_dict: Dict[str, Any] = {}
    sample_size = len(sum_of_numbers_series)
    sample_mean = sum_of_numbers_series.mean()
    sample_std_dev = sum_of_numbers_series.std(ddof=0) 

    parameters_dict: Dict[str, Any] = {
        "sample_size": sample_size,
        "sample_mean": round(sample_mean, 2),
        "sample_std_dev": round(sample_std_dev, 2)
    }

    statistic = np.nan
    p_value = np.nan
    degrees_of_freedom = None
    notes = "H0: A distribuição da soma das dezenas é Normal."

    if method == 'chi_square_bins':
        test_name = f"{test_name_base}_ChiSquareBins"
        logger.info(f"Iniciando teste: {test_name}")
        
        num_bins = getattr(config, 'SUM_NORMALITY_TEST_BINS', 10)
        parameters_dict["num_bins"] = num_bins

        observed_freq, bin_edges = np.histogram(sum_of_numbers_series, bins=num_bins)
        
        if sample_std_dev == 0:
            logger.warning(f"{test_name}: Desvio padrão da soma das dezenas é zero. Teste não aplicável.")
            return None
            
        cdf_values = stats.norm.cdf(bin_edges, loc=sample_mean, scale=sample_std_dev)
        expected_prob = np.diff(cdf_values)
        expected_freq = expected_prob * sample_size
        
        if np.sum(expected_freq) > 0 : 
            expected_freq = (expected_freq / np.sum(expected_freq)) * np.sum(observed_freq)
        
        if np.any(expected_freq < 1):
            logger.warning(f"{test_name}: Algumas frequências esperadas são menores que 1 (idealmente >= 5). "
                           "O teste Qui-Quadrado pode não ser preciso. Considere agrupar bins ou usar outro teste. "
                           f"Menor E_i: {expected_freq.min():.2f}. Soma Observada: {np.sum(observed_freq)}, Soma Esperada Ajustada: {np.sum(expected_freq):.2f}")
            
        valid_indices = (observed_freq > 0) | (expected_freq > 0.00001) 
        observed_freq_filtered = observed_freq[valid_indices]
        expected_freq_filtered = expected_freq[valid_indices]

        if len(observed_freq_filtered) < 2:
            logger.warning(f"{test_name}: Menos de 2 bins com frequências não nulas após filtragem. Teste não pode ser realizado.")
            return None

        try:
            statistic, p_value = stats.chisquare(f_obs=observed_freq_filtered, f_exp=expected_freq_filtered)
            degrees_of_freedom = len(observed_freq_filtered) - 1 - 2 
            if degrees_of_freedom <= 0:
                logger.warning(f"{test_name}: Graus de liberdade não positivos ({degrees_of_freedom}). Teste inválido.")
                degrees_of_freedom = None 
                p_value = np.nan
        except ValueError as ve:
            logger.error(f"{test_name}: Erro de valor no cálculo do Qui-Quadrado: {ve}")
            logger.debug(f"Observed Freq (filtered): {observed_freq_filtered}, Sum: {np.sum(observed_freq_filtered)}")
            logger.debug(f"Expected Freq (filtered): {expected_freq_filtered}, Sum: {np.sum(expected_freq_filtered)}")
            return None
        notes += f" Teste Qui-Quadrado com {num_bins} bins. Estimados mu e sigma da amostra."

    elif method == 'kolmogorov_smirnov':
        test_name = f"{test_name_base}_KolmogorovSmirnov"
        logger.info(f"Iniciando teste: {test_name}")
        if sample_std_dev == 0:
            logger.warning(f"{test_name}: Desvio padrão da soma das dezenas é zero. Teste não aplicável.")
            return None
        try:
            statistic, p_value = stats.kstest(sum_of_numbers_series, 'norm', args=(sample_mean, sample_std_dev))
        except Exception as e:
            logger.error(f"{test_name}: Erro ao calcular o teste Kolmogorov-Smirnov: {e}", exc_info=True)
            return None
        notes += " Teste Kolmogorov-Smirnov contra Normal com mu e sigma estimados da amostra."
    
    else:
        logger.error(f"Método de teste de normalidade desconhecido: {method}")
        return None

    conclusion = ""
    if pd.notna(p_value):
        if p_value < alpha:
            conclusion = (f"Rejeita H0 (p={p_value:.4f} < alpha={alpha}). "
                          "Evidência de que a distribuição da soma das dezenas NÃO é Normal.")
        else:
            conclusion = (f"Não rejeita H0 (p={p_value:.4f} >= alpha={alpha}). "
                          "Sem evidência estatística contra a normalidade da soma das dezenas.")
    else:
        conclusion = "Inconclusivo devido a erro ou dados insuficientes para o teste."

    test_result_dict = {
        "Test_Name": test_name,
        "Chi2_Statistic": round(statistic, 4) if pd.notna(statistic) else None,
        "P_Value": round(p_value, 6) if pd.notna(p_value) else None,
        "Degrees_Freedom": degrees_of_freedom,
        "Alpha_Level": alpha,
        "Conclusion": conclusion,
        "Parameters": json.dumps(parameters_dict),
        "Notes": notes
    }
    logger.info(f"{test_name} concluído. P-valor: {test_result_dict['P_Value']}. Conclusão (alpha={alpha}): {conclusion}")
    return test_result_dict

# --- NOVA FUNÇÃO PARA TESTE DE ADERÊNCIA À DISTRIBUIÇÃO DE POISSON ---
def perform_poisson_distribution_test(
    observed_event_counts_series: pd.Series, # Série com as contagens do evento por unidade (ex: N.primos por sorteio)
    event_config: Dict[str, Any], # Configuração específica para este evento (ex: de POISSON_DISTRIBUTION_TEST_CONFIG)
    alpha: float = 0.05
) -> Optional[Dict[str, Any]]:
    """
    Realiza um teste Qui-Quadrado de aderência para verificar se a contagem de um evento
    (ex: número de dezenas primas por sorteio) segue uma distribuição de Poisson.

    Args:
        observed_event_counts_series (pd.Series): Série Pandas contendo as contagens
                                                  observadas do evento por unidade (ex: por sorteio).
        event_config (Dict[str, Any]): Dicionário de configuração para este teste específico,
                                       espera-se 'column_name' e 'max_observed_count_for_chi2'.
        alpha (float): Nível de significância para a conclusão do teste.

    Returns:
        Optional[Dict[str, Any]]: Dicionário com os resultados do teste ou None em caso de erro.
    """
    event_name = event_config.get("event_description", event_config.get("column_name", "EventoDesconhecido"))
    test_name = f"PoissonDistributionTest_{event_name}"
    logger.info(f"Iniciando teste: {test_name}")

    if observed_event_counts_series.empty:
        logger.warning(f"{test_name}: Série de contagens observadas do evento está vazia.")
        return None

    # 1. Calcular a taxa média observada (lambda)
    lambda_observed = observed_event_counts_series.mean()
    if pd.isna(lambda_observed) or lambda_observed < 0: # Lambda não pode ser negativo
        logger.error(f"{test_name}: Lambda observado inválido ({lambda_observed}).")
        return None

    # 2. Gerar frequências observadas de k eventos (0, 1, 2, ..., max_val_plus)
    # Agrupa contagens raras na cauda para o teste Qui-Quadrado
    max_k_for_chi2 = event_config.get("max_observed_count_for_chi2", int(observed_event_counts_series.max()))
    
    # Cria os bins/categorias para o Qui-Quadrado.
    # Categorias: 0, 1, 2, ..., max_k_for_chi2, >max_k_for_chi2 (se necessário)
    observed_counts_binned = []
    categories = []
    
    # Frequências para 0, 1, ..., max_k_for_chi2 - 1
    for k_val in range(max_k_for_chi2):
        observed_counts_binned.append((observed_event_counts_series == k_val).sum())
        categories.append(str(k_val))
    
    # Frequência para max_k_for_chi2 e acima (categoria ">= max_k_for_chi2")
    observed_counts_binned.append((observed_event_counts_series >= max_k_for_chi2).sum())
    categories.append(f">={max_k_for_chi2}")
    
    observed_freq = np.array(observed_counts_binned)
    num_categories = len(observed_freq)

    # 3. Calcular frequências esperadas segundo uma Poisson com lambda_observed
    expected_freq = np.zeros(num_categories, dtype=float)
    total_observations = len(observed_event_counts_series)

    for i in range(num_categories):
        if i < num_categories - 1: # Categorias 0, 1, ..., max_k_for_chi2 - 1
            k_val_cat = int(categories[i])
            expected_prob_k = stats.poisson.pmf(k_val_cat, lambda_observed)
            expected_freq[i] = expected_prob_k * total_observations
        else: # Última categoria: P(X >= max_k_for_chi2) = 1 - CDF(max_k_for_chi2 - 1)
            # Ou, mais simples, o restante para a soma das probabilidades ser 1.
            # Mas para Poisson, é 1 - sum(P(X=k) for k < max_k_for_chi2)
            # Usamos a probabilidade de sobrevivência (1 - CDF)
            prob_ge_max_k = 1.0 - stats.poisson.cdf(max_k_for_chi2 - 1, lambda_observed)
            expected_freq[i] = prob_ge_max_k * total_observations
            
    # Pequeno ajuste para garantir que a soma das frequências esperadas seja igual ao total de observações
    # Isso ajuda a evitar erros de precisão no teste Qui-Quadrado.
    if np.sum(expected_freq) > 0 and total_observations > 0:
        expected_freq = (expected_freq / np.sum(expected_freq)) * total_observations


    # Agrupar categorias se as frequências esperadas forem muito baixas (critério comum: < 5)
    # Esta é uma parte complexa. Para simplificar, apenas emitiremos um aviso por agora.
    # A implementação robusta de agrupamento de bins é não trivial.
    if np.any(expected_freq < 1): # Critério mais frouxo para aviso
        logger.warning(f"{test_name}: Algumas frequências esperadas são < 1 (idealmente >= 5). "
                       f"O teste Qui-Quadrado pode não ser preciso. Menor E_i: {expected_freq.min():.2f}. "
                       f"Observadas: {observed_freq.tolist()}, Esperadas: {[round(ef,2) for ef in expected_freq.tolist()]}")
        # Se houver zeros em expected_freq e não em observed_freq, stats.chisquare dará erro.
        # Vamos filtrar onde expected_freq é muito próximo de zero se observed_freq também é zero,
        # ou onde expected_freq é zero e observed não é (indicando um problema).
        
    # Filtra categorias onde a frequência esperada é zero (ou muito pequena)
    # e a observada também é zero, para evitar problemas com o teste.
    # Se a esperada é zero e a observada não é, o teste Qui-Quadrado falhará.
    valid_indices_chi2 = (expected_freq > 0.00001) # Filtra esperadas muito pequenas
    observed_freq_chi2 = observed_freq[valid_indices_chi2]
    expected_freq_chi2 = expected_freq[valid_indices_chi2]

    if len(observed_freq_chi2) < 2:
        logger.warning(f"{test_name}: Menos de 2 categorias válidas para o teste Qui-Quadrado após filtragem. Teste não pode ser realizado.")
        return None

    try:
        chi2_statistic, p_value = stats.chisquare(f_obs=observed_freq_chi2, f_exp=expected_freq_chi2)
        degrees_of_freedom = len(observed_freq_chi2) - 1 - 1 # k_categorias - 1 - num_param_estimados (lambda)
        if degrees_of_freedom <= 0:
            logger.warning(f"{test_name}: Graus de liberdade não positivos ({degrees_of_freedom}). Teste inválido.")
            degrees_of_freedom = None
            p_value = np.nan 
    except ValueError as ve:
        logger.error(f"{test_name}: Erro de valor no cálculo do Qui-Quadrado para Poisson: {ve}")
        logger.debug(f"Observed Freq (filtered for chi2): {observed_freq_chi2}, Sum: {np.sum(observed_freq_chi2)}")
        logger.debug(f"Expected Freq (filtered for chi2): {expected_freq_chi2}, Sum: {np.sum(expected_freq_chi2)}")
        return None
    except Exception as e:
        logger.error(f"{test_name}: Erro ao calcular o teste Qui-Quadrado para Poisson: {e}", exc_info=True)
        return None
        
    conclusion = ""
    if pd.notna(p_value):
        if p_value < alpha:
            conclusion = (f"Rejeita H0 (p={p_value:.4f} < alpha={alpha}). Evidência de que a "
                          f"distribuição das contagens de '{event_name}' NÃO segue uma Poisson(lambda~{lambda_observed:.2f}).")
        else:
            conclusion = (f"Não rejeita H0 (p={p_value:.4f} >= alpha={alpha}). Sem evidência contra "
                          f"a distribuição de Poisson(lambda~{lambda_observed:.2f}) para '{event_name}'.")
    else:
        conclusion = "Inconclusivo devido a erro ou dados insuficientes para o teste."

    parameters_dict = {
        "event_name": event_name,
        "estimated_lambda": round(lambda_observed, 4),
        "total_units_observed": total_observations, # Ex: número de sorteios
        "max_k_for_chi2_categories": max_k_for_chi2,
        "categories_used_in_chi2": categories # Pode ser útil saber os bins
    }

    result = {
        "Test_Name": test_name,
        "Chi2_Statistic": round(chi2_statistic, 4) if pd.notna(chi2_statistic) else None,
        "P_Value": round(p_value, 6) if pd.notna(p_value) else None,
        "Degrees_Freedom": degrees_of_freedom,
        "Alpha_Level": alpha,
        "Conclusion": conclusion,
        "Parameters": json.dumps(parameters_dict),
        "Notes": f"H0: A contagem de '{event_name}' por unidade segue uma distribuição de Poisson."
    }
    logger.info(f"{test_name} concluído. Lambda estimado: {lambda_observed:.2f}. P-valor: {result['P_Value']}. Conclusão: {conclusion}")
    return result

--------------------------------------------------------------------------------
# Arquivo: src/analysis/temporal_trend_analysis.py
--------------------------------------------------------------------------------
# src/analysis/temporal_trend_analysis.py
import pandas as pd
import numpy as np # Adicionado para np.nan
import logging
from typing import List, Any # Any para o objeto config

logger = logging.getLogger(__name__)

def get_full_draw_matrix(all_draws_df: pd.DataFrame, config: Any) -> pd.DataFrame:
    """
    Cria uma matriz binária de ocorrências de dezenas por concurso.

    Args:
        all_draws_df (pd.DataFrame): DataFrame com todos os sorteios.
                                     Deve conter config.CONTEST_ID_COLUMN_NAME e
                                     config.DRAWN_NUMBERS_COLUMN_NAME (lista de dezenas).
        config (Any): Objeto de configuração.

    Returns:
        pd.DataFrame: Matriz com Concursos como índice, Dezenas (1-25) como colunas,
                      e valores 0 (não ocorreu) ou 1 (ocorreu).
                      Retorna um DataFrame vazio se a entrada for inválida.
    """
    logger.debug("Iniciando criação da matriz completa de sorteios (ocorrências).")
    if all_draws_df.empty:
        logger.warning("DataFrame de entrada para get_full_draw_matrix está vazio.")
        return pd.DataFrame()

    contest_col = config.CONTEST_ID_COLUMN_NAME
    drawn_numbers_col = config.DRAWN_NUMBERS_COLUMN_NAME

    if contest_col not in all_draws_df.columns:
        logger.error(f"Coluna '{contest_col}' não encontrada em all_draws_df.")
        return pd.DataFrame()
    if drawn_numbers_col not in all_draws_df.columns:
        logger.error(f"Coluna '{drawn_numbers_col}' não encontrada em all_draws_df.")
        return pd.DataFrame()

    try:
        df_copy = all_draws_df[[contest_col, drawn_numbers_col]].copy()
        df_copy[contest_col] = pd.to_numeric(df_copy[contest_col], errors='coerce')
        df_copy.dropna(subset=[contest_col], inplace=True)
        df_copy[contest_col] = df_copy[contest_col].astype(int)
        df_copy.set_index(contest_col, inplace=True)
        df_copy.sort_index(inplace=True) # Garante que os concursos estão ordenados
    except Exception as e:
        logger.error(f"Erro ao processar coluna de concurso '{contest_col}': {e}", exc_info=True)
        return pd.DataFrame()

    all_contests_idx = df_copy.index
    draw_matrix = pd.DataFrame(0, index=all_contests_idx, columns=config.ALL_NUMBERS)
    draw_matrix.index.name = contest_col

    for contest_id, row in df_copy.iterrows():
        drawn_numbers = row[drawn_numbers_col]
        if isinstance(drawn_numbers, list):
            for number in drawn_numbers:
                if number in draw_matrix.columns:
                    draw_matrix.loc[contest_id, number] = 1
        else:
            logger.warning(f"Dados em '{drawn_numbers_col}' para o concurso {contest_id} não são uma lista: {drawn_numbers}")
            
    logger.debug(f"Matriz completa de sorteios criada com {draw_matrix.shape[0]} concursos e {draw_matrix.shape[1]} dezenas.")
    return draw_matrix


def calculate_moving_average_frequency(
    draw_matrix: pd.DataFrame, 
    windows: List[int], 
    config: Any
) -> pd.DataFrame:
    """
    Calcula a média móvel da frequência de ocorrência para cada dezena e cada janela especificada.
    """
    logger.info(f"Iniciando cálculo da média móvel de frequência para janelas: {windows}")
    if draw_matrix.empty:
        logger.warning("Matriz de sorteios para calculate_moving_average_frequency está vazia.")
        return pd.DataFrame(columns=['Concurso', 'Dezena', 'Janela', 'MA_Frequencia'])
    if not windows:
        logger.warning("Nenhuma janela especificada para cálculo da média móvel de frequência.")
        return pd.DataFrame(columns=['Concurso', 'Dezena', 'Janela', 'MA_Frequencia'])

    all_ma_results = []
    
    if not pd.api.types.is_numeric_dtype(draw_matrix.index):
        try:
            draw_matrix.index = pd.to_numeric(draw_matrix.index)
        except Exception as e:
            logger.error(f"Índice da draw_matrix (Concurso) não é numérico e não pôde ser convertido: {e}")
            return pd.DataFrame(columns=['Concurso', 'Dezena', 'Janela', 'MA_Frequencia'])
    
    draw_matrix = draw_matrix.sort_index()

    for dezena_col in draw_matrix.columns:
        if dezena_col not in config.ALL_NUMBERS:
            logger.warning(f"Coluna {dezena_col} na draw_matrix não está em config.ALL_NUMBERS. Pulando.")
            continue
            
        occurrence_series = draw_matrix[dezena_col]
        
        for window_size in windows:
            if window_size <= 0:
                logger.warning(f"Tamanho de janela inválido {window_size} para a dezena {dezena_col}. Pulando.")
                continue
            
            ma_series = occurrence_series.rolling(window=window_size, min_periods=1).mean()
            
            temp_df = ma_series.reset_index()
            temp_df.columns = ['Concurso', 'MA_Frequencia']
            temp_df['Dezena'] = dezena_col
            temp_df['Janela'] = window_size
            all_ma_results.append(temp_df)

    if not all_ma_results:
        logger.warning("Nenhum resultado de média móvel de frequência foi gerado.")
        return pd.DataFrame(columns=['Concurso', 'Dezena', 'Janela', 'MA_Frequencia'])

    final_df = pd.concat(all_ma_results, ignore_index=True)
    final_df = final_df[['Concurso', 'Dezena', 'Janela', 'MA_Frequencia']]
    
    logger.info(f"Cálculo da média móvel de frequência concluído. {len(final_df)} registros gerados.")
    return final_df

# --- NOVAS FUNÇÕES PARA MÉDIA MÓVEL DE ATRASO ---

def get_historical_delay_matrix(draw_matrix: pd.DataFrame, config: Any) -> pd.DataFrame:
    """
    Calcula o atraso atual histórico para cada dezena em cada concurso.

    Args:
        draw_matrix (pd.DataFrame): Matriz de ocorrências (Concursos x Dezenas, valores 0/1),
                                     com índice de Concurso ordenado.
        config (Any): Objeto de configuração (para CONTEST_ID_COLUMN_NAME).

    Returns:
        pd.DataFrame: Matriz com Concursos como índice, Dezenas como colunas,
                      e valores de atraso atual histórico.
    """
    logger.debug("Iniciando cálculo da matriz de atraso atual histórico.")
    if draw_matrix.empty:
        logger.warning("Matriz de sorteios para get_historical_delay_matrix está vazia.")
        return pd.DataFrame()

    # Assume que draw_matrix.index são os IDs dos concursos e estão ordenados.
    historical_delay_df = pd.DataFrame(index=draw_matrix.index, columns=draw_matrix.columns)
    historical_delay_df.index.name = config.CONTEST_ID_COLUMN_NAME # ou draw_matrix.index.name
    
    first_contest_id_overall = draw_matrix.index.min()

    for dezena in draw_matrix.columns:
        last_occurrence_contest = 0 # Considera 0 como "antes do primeiro concurso"
        
        # Se a dezena nunca ocorre em todo o histórico, o atraso será sempre crescente
        # desde o primeiro concurso.
        if draw_matrix[dezena].sum() == 0:
            for contest_id in draw_matrix.index:
                historical_delay_df.loc[contest_id, dezena] = contest_id - first_contest_id_overall +1 # Ou apenas contest_id
            continue

        for contest_id in draw_matrix.index:
            if last_occurrence_contest == 0: # Ainda não encontrou a primeira ocorrência
                 current_delay = contest_id - first_contest_id_overall + 1
            else:
                 current_delay = contest_id - last_occurrence_contest
            
            historical_delay_df.loc[contest_id, dezena] = current_delay
            
            if draw_matrix.loc[contest_id, dezena] == 1: # Se a dezena ocorreu neste concurso
                last_occurrence_contest = contest_id    # Atualiza o último concurso de ocorrência

    logger.debug("Matriz de atraso atual histórico calculada.")
    return historical_delay_df.astype(int)


def calculate_moving_average_delay(
    historical_delay_matrix: pd.DataFrame, 
    windows: List[int], 
    config: Any # Usado para config.ALL_NUMBERS, se necessário para validar colunas
) -> pd.DataFrame:
    """
    Calcula a média móvel do atraso atual histórico para cada dezena e cada janela.

    Args:
        historical_delay_matrix (pd.DataFrame): Matriz de atraso atual histórico
                                                (Concursos x Dezenas).
        windows (List[int]): Lista de tamanhos de janela para a média móvel.
        config (Any): Objeto de configuração.

    Returns:
        pd.DataFrame: DataFrame em formato longo com as colunas:
                      'Concurso', 'Dezena', 'Janela', 'MA_Atraso'.
    """
    logger.info(f"Iniciando cálculo da média móvel de atraso para janelas: {windows}")
    if historical_delay_matrix.empty:
        logger.warning("Matriz de atraso histórico para calculate_moving_average_delay está vazia.")
        return pd.DataFrame(columns=['Concurso', 'Dezena', 'Janela', 'MA_Atraso'])
    if not windows:
        logger.warning("Nenhuma janela especificada para cálculo da média móvel de atraso.")
        return pd.DataFrame(columns=['Concurso', 'Dezena', 'Janela', 'MA_Atraso'])

    all_ma_results = []
    
    # Garante que o índice (Concurso) é numérico e ordenado (já deve ser pela função anterior)
    if not pd.api.types.is_numeric_dtype(historical_delay_matrix.index):
        try:
            historical_delay_matrix.index = pd.to_numeric(historical_delay_matrix.index)
        except Exception as e:
            logger.error(f"Índice da historical_delay_matrix (Concurso) não é numérico e não pôde ser convertido: {e}")
            return pd.DataFrame(columns=['Concurso', 'Dezena', 'Janela', 'MA_Atraso'])

    historical_delay_matrix = historical_delay_matrix.sort_index()

    for dezena_col in historical_delay_matrix.columns:
        if hasattr(config, 'ALL_NUMBERS') and dezena_col not in config.ALL_NUMBERS:
             logger.warning(f"Coluna {dezena_col} na historical_delay_matrix não está em config.ALL_NUMBERS. Pulando.")
             continue

        delay_series = historical_delay_matrix[dezena_col].astype(float) # .rolling().mean() espera float
        
        for window_size in windows:
            if window_size <= 0:
                logger.warning(f"Tamanho de janela inválido {window_size} para a dezena {dezena_col}. Pulando.")
                continue
            
            ma_series = delay_series.rolling(window=window_size, min_periods=1).mean()
            
            temp_df = ma_series.reset_index()
            temp_df.columns = ['Concurso', 'MA_Atraso']
            temp_df['Dezena'] = dezena_col
            temp_df['Janela'] = window_size
            all_ma_results.append(temp_df)

    if not all_ma_results:
        logger.warning("Nenhum resultado de média móvel de atraso foi gerado.")
        return pd.DataFrame(columns=['Concurso', 'Dezena', 'Janela', 'MA_Atraso'])

    final_df = pd.concat(all_ma_results, ignore_index=True)
    final_df = final_df[['Concurso', 'Dezena', 'Janela', 'MA_Atraso']]
    
    logger.info(f"Cálculo da média móvel de atraso concluído. {len(final_df)} registros gerados.")
    return final_df

--------------------------------------------------------------------------------
# Arquivo: src/analysis_aggregator.py
--------------------------------------------------------------------------------
# src/analysis_aggregator.py
import pandas as pd
from typing import Optional, List, Dict, Any
import logging
from sklearn.preprocessing import MinMaxScaler

# Removido logging.basicConfig para permitir configuração externa
logger = logging.getLogger(__name__)

# Importar o objeto de configuração diretamente e o tipo Config para type hinting
from .database_manager import DatabaseManager
from .config import config_obj, Config # Importando o config_obj global e a classe Config

class AnalysisAggregator:
    def __init__(self, db_manager: DatabaseManager,
                 config_instance: Optional[Config] = None): # Recebe uma instância de Config
        self.db_manager = db_manager
        # Prioriza a instância de config passada, caso contrário usa o config_obj global
        # Isso é útil para testes onde você pode querer injetar uma configuração mockada.
        self.config_access: Config = config_instance if config_instance is not None else config_obj
        
        if not isinstance(self.config_access, Config):
            logger.error(f"AnalysisAggregator recebeu um objeto de configuração inválido. Esperava uma instância da classe 'Config', recebeu {type(self.config_access)}. Revertendo para config_obj global.")
            self.config_access = config_obj # Fallback para o global em caso de erro grave
            if not isinstance(self.config_access, Config): # Checagem final
                 logger.critical("Falha crítica: config_obj global também não é uma instância de Config. O Aggregator pode não funcionar corretamente.")
                 # Em um cenário real, poderia levantar uma exceção aqui.
                 # Por agora, ele tentará continuar, mas provavelmente falhará ao acessar atributos.

        self._all_dezenas_list = self.config_access.ALL_NUMBERS
        self._default_recent_window = self.config_access.AGGREGATOR_DEFAULT_RECENT_WINDOW # Nova constante do config.py
        
        # Nomes das tabelas agora são lidos diretamente das constantes em config_obj
        self.table_names = {
            'draws': self.config_access.MAIN_DRAWS_TABLE_NAME,
            'draws_flat': self.config_access.FLAT_DRAWS_TABLE_NAME,
            'delays': self.config_access.ANALYSIS_DELAYS_TABLE_NAME,
            'frequency_overall': self.config_access.ANALYSIS_FREQUENCY_OVERALL_TABLE_NAME,
            'recurrence_cdf': self.config_access.ANALYSIS_RECURRENCE_CDF_TABLE_NAME,
            'rank_trends': self.config_access.ANALYSIS_RANK_TREND_METRICS_TABLE_NAME,
            'cycle_status': self.config_access.ANALYSIS_CYCLE_STATUS_DEZENAS_TABLE_NAME,
            'cycle_closing_propensity': self.config_access.ANALYSIS_CYCLE_CLOSING_PROPENSITY_TABLE_NAME,
            'itemset_metrics': self.config_access.ANALYSIS_ITEMSET_METRICS_TABLE_NAME
        }

        # Colunas que o método principal get_historical_metrics_for_dezenas tentará preencher.
        # A chave 'source_table' agora reflete os nomes padronizados.
        self.metric_configs = {
            'current_delay': {'default': 0, 'type': int, 'source_table': self.table_names['delays']},
            'max_delay_observed': {'default': 0, 'type': int, 'source_table': self.table_names['delays']},
            'avg_delay': {'default': 0.0, 'type': float, 'source_table': self.table_names['delays']},
            'overall_frequency': {'default': 0, 'type': int, 'source_table': self.table_names['frequency_overall']},
            'overall_relative_frequency': {'default': 0.0, 'type': float, 'source_table': self.table_names['frequency_overall']},
            f'recent_frequency_window_{self._default_recent_window}': {'default': 0, 'type': int, 'source_table': self.table_names['draws_flat']},
            'recurrence_cdf': {'default': 0.0, 'type': float, 'source_table': self.table_names['recurrence_cdf']},
            'rank_slope': {'default': 0.0, 'type': float, 'source_table': self.table_names['rank_trends']},
            'trend_status': {'default': 'indefinido', 'type': str, 'source_table': self.table_names['rank_trends']},
            'is_missing_in_current_cycle': {'default': 0, 'type': int, 'source_table': self.table_names['cycle_status']},
            'cycle_closing_propensity_score': {'default': 0.0, 'type': float, 'source_table': self.table_names['cycle_closing_propensity']},
            'participation_score_k2': {'default': 0.0, 'type': float, 'source_table': self.table_names['itemset_metrics']},
            'participation_score_k3': {'default': 0.0, 'type': float, 'source_table': self.table_names['itemset_metrics']},
        }
        self._target_metric_columns = list(self.metric_configs.keys())

    def _get_latest_concurso_id_from_db(self) -> Optional[int]:
        try:
            # Usa CONTEST_ID_COLUMN_NAME do config para consistência
            query = f"SELECT MAX({self.config_access.CONTEST_ID_COLUMN_NAME}) FROM {self.table_names['draws']}"
            result_df = self.db_manager.execute_query(query)
            if result_df is not None and not result_df.empty and pd.notna(result_df.iloc[0, 0]):
                return int(result_df.iloc[0, 0])
            logger.warning(f"Nenhum concurso encontrado na tabela '{self.table_names['draws']}'.")
        except Exception as e:
            logger.error(f"Falha ao buscar último concurso_id: {e}", exc_info=True)
        return None

    def get_historical_metrics_for_dezenas(self, latest_concurso_id: Optional[int] = None) -> pd.DataFrame:
        if latest_concurso_id is None:
            latest_concurso_id = self._get_latest_concurso_id_from_db()
            if latest_concurso_id is None:
                logger.error("Não foi possível determinar o concurso mais recente para agregação.")
                empty_df = pd.DataFrame({'dezena': self._all_dezenas_list})
                for col, conf in self.metric_configs.items(): empty_df[col] = conf['default']
                return empty_df.astype({col: conf['type'] for col, conf in self.metric_configs.items() if conf['type'] is not object and col in empty_df}, errors='ignore')

        logger.info(f"Consolidando métricas históricas para dezenas até o concurso {latest_concurso_id}...")
        dezenas_df = pd.DataFrame({'dezena': self._all_dezenas_list})

        merge_methods_map = {
            'delays': self._merge_delay_metrics,
            'frequency': self._merge_frequency_metrics,
            'recurrence': self._merge_recurrence_metrics,
            'rank_trends': self._merge_rank_trend_metrics,
            'cycle_status': self._merge_cycle_status_metrics,
            'itemset_participation': self._merge_itemset_participation_scores
        }

        for metric_group, merge_method in merge_methods_map.items():
            try:
                dezenas_df = merge_method(dezenas_df, latest_concurso_id)
            except Exception as e:
                logger.error(f"Erro durante {merge_method.__name__} para o grupo '{metric_group}': {e}", exc_info=True)
        
        for col_name, conf in self.metric_configs.items():
            if col_name not in dezenas_df.columns:
                dezenas_df[col_name] = conf['default']
            else:
                dezenas_df[col_name] = dezenas_df[col_name].fillna(conf['default'])
            try:
                 if conf['type'] is not object and col_name in dezenas_df.columns: # Adicionada checagem se col_name existe
                    dezenas_df[col_name] = dezenas_df[col_name].astype(conf['type'])
            except Exception as e:
                 logger.warning(f"Não foi possível converter a coluna '{col_name}' para o tipo {conf['type']}: {e}")

        final_cols_ordered = ['dezena'] + [col for col in self._target_metric_columns if col in dezenas_df.columns]
        other_cols = [col for col in dezenas_df.columns if col not in final_cols_ordered]
        dezenas_df = dezenas_df[final_cols_ordered + other_cols]
        
        return dezenas_df

    def _execute_metric_query(self, base_df: pd.DataFrame, sql: str, params: tuple,
                               expected_metric_cols: List[str], metric_group_name: str) -> pd.DataFrame:
        data_df = None
        try:
            data_df = self.db_manager.execute_query(sql, params)
        except Exception as e:
            logger.error(f"Falha na execução da query para {metric_group_name}: {e}", exc_info=True)

        if data_df is not None and not data_df.empty:
            if 'dezena' not in data_df.columns:
                logger.warning(f"Coluna 'dezena' ausente no resultado da query para {metric_group_name}.")
                for col in expected_metric_cols:
                    if col != 'dezena' and col not in base_df.columns: base_df[col] = self.metric_configs.get(col, {}).get('default', pd.NA)
                return base_df
            
            # Garantir que as colunas esperadas existam no data_df antes de selecionar
            actual_cols_from_query = ['dezena'] + [col for col in expected_metric_cols if col in data_df.columns and col != 'dezena']
            
            cols_to_drop_from_base = [col for col in actual_cols_from_query if col in base_df.columns and col != 'dezena']
            base_df_for_merge = base_df.drop(columns=cols_to_drop_from_base, errors='ignore')
            
            merged_df = pd.merge(base_df_for_merge, data_df[actual_cols_from_query], on='dezena', how='left')
            return merged_df
        else:
            logger.info(f"Nenhum dado encontrado via query para {metric_group_name} (params: {params}).")
            # Preencher com defaults se a query não retornar dados ou for vazia
            for col_metric in expected_metric_cols: # Iterar sobre expected_metric_cols, não apenas 'col'
                if col_metric != 'dezena' and col_metric not in base_df.columns:
                    base_df[col_metric] = self.metric_configs.get(col_metric, {}).get('default', pd.NA)
            return base_df

    def _merge_delay_metrics(self, base_df: pd.DataFrame, concurso_id: int) -> pd.DataFrame:
        table = self.table_names['delays']
        # Usando CONTEST_ID_COLUMN_NAME do config para consistência
        cid_col = self.config_access.CONTEST_ID_COLUMN_NAME
        sql = f"""
            WITH RankedData AS (
                SELECT dezena, current_delay,
                       COALESCE(max_delay_observed, current_delay) AS max_delay_observed,
                       COALESCE(avg_delay, current_delay) AS avg_delay,
                       ROW_NUMBER() OVER (PARTITION BY dezena ORDER BY {cid_col} DESC) as rn
                FROM {table} WHERE {cid_col} <= ?
            )
            SELECT dezena, current_delay, max_delay_observed, avg_delay FROM RankedData WHERE rn = 1;
        """
        params = (concurso_id,)
        metric_cols = ['current_delay', 'max_delay_observed', 'avg_delay']
        return self._execute_metric_query(base_df, sql, params, metric_cols, "métricas de atraso")

    def _merge_frequency_metrics(self, base_df: pd.DataFrame, concurso_id: int) -> pd.DataFrame:
        merged_df = base_df.copy()
        table_overall = self.table_names['frequency_overall']
        table_flat = self.table_names['draws_flat']
        cid_col = self.config_access.CONTEST_ID_COLUMN_NAME
        
        sql_overall = f"""
            WITH RankedData AS (
                SELECT dezena, frequency AS overall_frequency, relative_frequency AS overall_relative_frequency,
                       ROW_NUMBER() OVER (PARTITION BY dezena ORDER BY {cid_col} DESC) as rn
                FROM {table_overall} WHERE {cid_col} <= ?
            )
            SELECT dezena, overall_frequency, overall_relative_frequency FROM RankedData WHERE rn = 1;
        """
        params_overall = (concurso_id,)
        metric_cols_overall = ['overall_frequency', 'overall_relative_frequency']
        merged_df = self._execute_metric_query(merged_df, sql_overall, params_overall, metric_cols_overall, "frequência geral")

        window_size = self.config_access.AGGREGATOR_DEFAULT_RECENT_WINDOW # Usando a constante do config
        start_concurso_id = max(1, concurso_id - window_size + 1)
        col_name_recent_freq = f'recent_frequency_window_{window_size}'
        
        sql_recent = f"""
            SELECT
                gen_dez.value AS dezena, COUNT(drf.dezena) AS "{col_name_recent_freq}"
            FROM json_each(json_array({','.join(map(str, self._all_dezenas_list))})) gen_dez
            LEFT JOIN {table_flat} drf ON gen_dez.value = drf.dezena AND drf.{cid_col} BETWEEN ? AND ?
            GROUP BY gen_dez.value;
        """
        params_recent = (start_concurso_id, concurso_id)
        metric_cols_recent = [col_name_recent_freq]
        
        # Adicionar dinamicamente a configuração da métrica se a janela mudar e não existir
        if col_name_recent_freq not in self.metric_configs:
            self.metric_configs[col_name_recent_freq] = {'default': 0, 'type': int, 'source_table': table_flat}
            if col_name_recent_freq not in self._target_metric_columns: 
                self._target_metric_columns.append(col_name_recent_freq)
        
        return self._execute_metric_query(merged_df, sql_recent, params_recent, metric_cols_recent, f"frequência recente (W{window_size})")

    def _merge_recurrence_metrics(self, base_df: pd.DataFrame, concurso_id: int) -> pd.DataFrame:
        table = self.table_names['recurrence_cdf']
        cid_col = self.config_access.CONTEST_ID_COLUMN_NAME
        sql = f"""
            WITH RankedData AS (
                SELECT dezena, recurrence_cdf,
                       ROW_NUMBER() OVER (PARTITION BY dezena ORDER BY {cid_col} DESC) as rn
                FROM {table} WHERE {cid_col} <= ?
            )
            SELECT dezena, recurrence_cdf FROM RankedData WHERE rn = 1;
        """
        params = (concurso_id,)
        metric_cols = ['recurrence_cdf']
        return self._execute_metric_query(base_df, sql, params, metric_cols, "métricas de recorrência")

    def _merge_rank_trend_metrics(self, base_df: pd.DataFrame, concurso_id: int) -> pd.DataFrame:
        table = self.table_names['rank_trends']
        cid_col = self.config_access.CONTEST_ID_COLUMN_NAME
        sql = f"""
            WITH RankedData AS (
                SELECT dezena, rank_slope, trend_status,
                       ROW_NUMBER() OVER (PARTITION BY dezena ORDER BY {cid_col} DESC) as rn
                FROM {table} WHERE {cid_col} <= ?
            )
            SELECT dezena, rank_slope, trend_status FROM RankedData WHERE rn = 1;
        """
        params = (concurso_id,)
        metric_cols = ['rank_slope', 'trend_status']
        return self._execute_metric_query(base_df, sql, params, metric_cols, "tendência de rank")

    def _merge_cycle_status_metrics(self, base_df: pd.DataFrame, concurso_id: int) -> pd.DataFrame:
        table_status = self.table_names['cycle_status']
        table_closing = self.table_names['cycle_closing_propensity']
        cid_col = self.config_access.CONTEST_ID_COLUMN_NAME
        merged_df = base_df.copy()

        sql_missing = f"""
            WITH RankedData AS (
                SELECT dezena, is_missing_in_current_cycle,
                       ROW_NUMBER() OVER (PARTITION BY dezena ORDER BY {cid_col} DESC) as rn
                FROM {table_status} WHERE {cid_col} <= ?
            )
            SELECT dezena, is_missing_in_current_cycle FROM RankedData WHERE rn = 1;
        """
        params_missing = (concurso_id,)
        metric_cols_missing = ['is_missing_in_current_cycle']
        merged_df = self._execute_metric_query(merged_df, sql_missing, params_missing, metric_cols_missing, "status de ciclo (faltantes)")
        
        sql_closing_score = f"""SELECT dezena, score AS cycle_closing_propensity_score FROM {table_closing};"""
        params_closing = () 
        metric_cols_closing = ['cycle_closing_propensity_score']
        return self._execute_metric_query(merged_df, sql_closing_score, params_closing, metric_cols_closing, "propensão de fechamento de ciclo")

    def _calculate_dezena_itemset_scores(self, item_data_df: pd.DataFrame, k_value: int) -> pd.DataFrame:
        if item_data_df.empty or 'itemset' not in item_data_df.columns or 'itemset_score' not in item_data_df.columns:
            return pd.DataFrame({'dezena': self._all_dezenas_list, f'participation_score_k{k_value}': 0.0})

        filtered_items = item_data_df[item_data_df['k'] == k_value]
        if filtered_items.empty:
             return pd.DataFrame({'dezena': self._all_dezenas_list, f'participation_score_k{k_value}': 0.0})

        dezena_scores_acc: Dict[int, float] = {d: 0.0 for d in self._all_dezenas_list}
        for _, row in filtered_items.iterrows():
            itemset = row['itemset']
            item_score = row.get('itemset_score', 0.0) 
            if isinstance(itemset, tuple): # Checagem adicional de tipo
                for dezena_val in itemset: # Renomeado para evitar conflito com a coluna 'dezena'
                    if dezena_val in dezena_scores_acc:
                        dezena_scores_acc[dezena_val] += item_score
        
        scores_df = pd.DataFrame(list(dezena_scores_acc.items()), columns=['dezena', f'raw_participation_score_k{k_value}'])
        
        scaler = MinMaxScaler()
        score_col_raw = f'raw_participation_score_k{k_value}'
        score_col_norm = f'participation_score_k{k_value}'
        
        # Tratar caso onde a coluna pode não ter sido criada se dezena_scores_acc estava vazio ou todos os scores eram 0
        if score_col_raw not in scores_df.columns:
             scores_df[score_col_norm] = 0.0
        elif scores_df[score_col_raw].nunique() > 1:
            scores_df[score_col_norm] = scaler.fit_transform(scores_df[[score_col_raw]])
        else: # nunique <= 1
            scores_df[score_col_norm] = 0.5 if scores_df[score_col_raw].nunique() == 1 and scores_df[score_col_raw].iloc[0] !=0 else 0.0
        
        return scores_df[['dezena', score_col_norm]]


    def _merge_itemset_participation_scores(self, base_df: pd.DataFrame, concurso_id: int) -> pd.DataFrame:
        logger.info("Calculando scores de participação em itemsets...")
        merged_df = base_df.copy()
        k_values_to_process = self.config_access.config_obj.get('itemset_k_values_for_participation_score', [2,3])


        all_item_data = self.get_itemset_analysis_data(latest_concurso_id=concurso_id, k_values=k_values_to_process)

        if all_item_data is None or all_item_data.empty:
            logger.warning("Nenhum dado de itemset retornado pelo Aggregator para cálculo de scores de participação.")
            for k_val in k_values_to_process:
                 score_col_k = f'participation_score_k{k_val}'
                 if score_col_k not in merged_df.columns: merged_df[score_col_k] = self.metric_configs.get(score_col_k, {}).get('default', 0.0)
            return merged_df
        
        for k_val in k_values_to_process:
            score_col_k = f'participation_score_k{k_val}'
            itemset_scores_k_df = self._calculate_dezena_itemset_scores(all_item_data, k_val)
            if not itemset_scores_k_df.empty:
                merged_df = pd.merge(merged_df, itemset_scores_k_df, on='dezena', how='left')
            elif score_col_k not in merged_df.columns: # Garante a coluna se o df de scores k for vazio
                 merged_df[score_col_k] = self.metric_configs.get(score_col_k, {}).get('default', 0.0)
            
            # Preencher NaNs que podem ter surgido dos merges
            if score_col_k in merged_df.columns:
                merged_df[score_col_k] = merged_df[score_col_k].fillna(self.metric_configs.get(score_col_k, {}).get('default',0.0))
            else: # Se a coluna ainda não existe por algum motivo, cria com default
                merged_df[score_col_k] = self.metric_configs.get(score_col_k, {}).get('default', 0.0)

        return merged_df

    def get_itemset_analysis_data(self, latest_concurso_id: Optional[int] = None,
                                  k_values: Optional[List[int]] = None,
                                  min_support: Optional[float] = None,
                                  min_lift: Optional[float] = None,
                                  itemset_score_metric: str = 'itemset_score'
                                  ) -> pd.DataFrame:
        table_itemsets = self.table_names['itemset_metrics']
        # Busca de config_access ao invés de self.config.get
        default_k_from_config = getattr(self.config_access, 'ITEMSET_DEFAULT_K_VALUES_AGGREGATOR', [2, 3])


        if latest_concurso_id is None:
            latest_concurso_id = self._get_latest_concurso_id_from_db()
            if latest_concurso_id is None: return pd.DataFrame()

        where_clauses = [f"im.{self.config_access.CONTEST_ID_COLUMN_NAME} <= ?"] # Usando nome da coluna do config
        params_list: List[Any] = [latest_concurso_id]

        current_k_values = k_values if k_values is not None else default_k_from_config
        if current_k_values:
            safe_k_values = [int(k) for k in current_k_values if isinstance(k, (int, float)) or (isinstance(k, str) and k.isdigit())]
            if safe_k_values:
                 where_clauses.append(f"im.k IN ({','.join(['?'] * len(safe_k_values))})")
                 params_list.extend(safe_k_values)
        
        if min_support is not None:
            where_clauses.append("im.support >= ?")
            params_list.append(min_support)
        
        if min_lift is not None:
            where_clauses.append("im.lift >= ?")
            params_list.append(min_lift)
            
        where_sql = " AND ".join(where_clauses)
        
        allowed_score_cols = ['itemset_score', 'support', 'confidence', 'lift'] 
        score_col_to_select = itemset_score_metric if itemset_score_metric in allowed_score_cols else 'itemset_score'

        cid_col_for_query = self.config_access.CONTEST_ID_COLUMN_NAME # Usando nome da coluna do config
        sql = f"""
            WITH RankedItemsets AS (
                SELECT
                    itemset_str, k, support, COALESCE(lift, 0) AS lift, 
                    COALESCE({score_col_to_select}, 0) AS itemset_score,
                    COALESCE(itemset_current_delay, 0) AS itemset_current_delay,
                    ROW_NUMBER() OVER (PARTITION BY itemset_str, k ORDER BY {cid_col_for_query} DESC) as rn
                FROM {table_itemsets} im
                WHERE {where_sql}
            )
            SELECT itemset_str AS itemset_original_str, k, support, lift, itemset_score, itemset_current_delay
            FROM RankedItemsets WHERE rn = 1;
        """
        params = tuple(params_list)
        
        try:
            item_data_df = self.db_manager.execute_query(sql, params)
            if item_data_df is not None and not item_data_df.empty:
                def parse_itemset_str_local(itemset_string_val): # Renomeado para evitar conflito
                    try:
                        if isinstance(itemset_string_val, str):
                            cleaned_str = itemset_string_val.strip("() ")
                            if not cleaned_str: return tuple()
                            # Supondo que os números no itemset_str são separados por vírgula ou hífen
                            # Ex: "(1,5,10)" ou "1-5-10"
                            separator = ',' if ',' in cleaned_str else '-'
                            return tuple(map(int, cleaned_str.split(separator)))
                        elif isinstance(itemset_string_val, (list, set)):
                             return tuple(sorted(map(int,itemset_string_val)))
                        elif isinstance(itemset_string_val, tuple):
                             return tuple(sorted(map(int,itemset_string_val)))
                        return None
                    except Exception as e_parse:
                        logger.warning(f"Falha ao parsear itemset_str: '{itemset_string_val}'. Erro: {e_parse}")
                        return None
                item_data_df['itemset'] = item_data_df['itemset_original_str'].apply(parse_itemset_str_local)
                item_data_df.dropna(subset=['itemset'], inplace=True)
                return item_data_df[['itemset', 'k', 'support', 'lift', 'itemset_score', 'itemset_current_delay']]
            else:
                logger.info(f"Nenhum dado de itemset encontrado com os critérios (concurso {latest_concurso_id}).")
        except Exception as e:
            logger.error(f"Erro ao buscar dados de itemset: {e}", exc_info=True)
        
        return pd.DataFrame(columns=['itemset', 'k', 'support', 'lift', 'itemset_score', 'itemset_current_delay'])

--------------------------------------------------------------------------------
# Arquivo: src/backtester/__init__.py
--------------------------------------------------------------------------------
# src/__init__.py
# Este arquivo pode ficar vazio.

--------------------------------------------------------------------------------
# Arquivo: src/backtester/evaluator.py
--------------------------------------------------------------------------------
# src/backtester/evaluator.py

from typing import Set, Dict
from collections import Counter
from src.config import logger

def evaluate_hits(chosen_numbers: Set[int], actual_numbers: Set[int]) -> int:
    """
    Compara o conjunto de números escolhidos com os números sorteados
    e retorna a quantidade de acertos.

    Args:
        chosen_numbers (Set[int]): Conjunto de números escolhidos pela estratégia.
        actual_numbers (Set[int]): Conjunto de números realmente sorteados.

    Returns:
        int: O número de acertos (interseção entre os dois conjuntos).
             Retorna -1 se um dos inputs for inválido.
    """
    if chosen_numbers is None or actual_numbers is None:
        logger.error("Erro na avaliação: conjunto de números inválido (None).")
        return -1 # Indica erro
    if not isinstance(chosen_numbers, set) or not isinstance(actual_numbers, set):
         logger.error(f"Erro na avaliação: input não é um set. Escolhidos: {type(chosen_numbers)}, Sorteados: {type(actual_numbers)}")
         return -1

    hits = len(chosen_numbers.intersection(actual_numbers))
    # logger.debug(f"Avaliação: Escolhidos {chosen_numbers}, Sorteados {actual_numbers}, Acertos: {hits}")
    return hits


def summarize_results(results: Dict[int, int]) -> Dict[int, int]:
    """
    Recebe um dicionário de resultados {concurso: acertos} e retorna
    um resumo da contagem de cada faixa de acertos (11 a 15).

    Args:
        results (Dict[int, int]): Dicionário com {concurso: numero_de_acertos}.

    Returns:
        Dict[int, int]: Dicionário com {numero_de_acertos: contagem_de_vezes}.
    """
    hit_counts = Counter(results.values()) # Conta quantas vezes cada número de acertos ocorreu

    # Filtra para manter apenas as faixas de premiação relevantes (11 a 15) e acertos >= 0
    summary = {hits: count for hits, count in hit_counts.items() if 11 <= hits <= 15}

    # Garante que todas as faixas de 11 a 15 existam no resumo, mesmo que com 0 ocorrências
    for i in range(11, 16):
        if i not in summary:
            summary[i] = 0

    # Opcional: incluir contagem de < 11 acertos ou erros (-1)
    summary['<11'] = sum(count for hits, count in hit_counts.items() if 0 <= hits < 11)
    summary['errors'] = hit_counts.get(-1, 0) # Conta quantos erros (-1) ocorreram

    logger.info(f"Resumo do Backtest: {summary}")
    return summary

--------------------------------------------------------------------------------
# Arquivo: src/backtester/runner.py
--------------------------------------------------------------------------------
# src/backtester/runner.py

from typing import Callable, Optional, Set, Dict, Any, List
import pandas as pd
from collections import Counter
import time

# Importa do config (INCLUINDO BASE_COLS)
from src.config import logger, ALL_NUMBERS, BASE_COLS, NEW_BALL_COLUMNS

# Importa do database_manager (SEM BASE_COLS)
from src.database_manager import read_data_from_db, get_draw_numbers
from src.backtester.evaluator import evaluate_hits, summarize_results
# Importa o agregador para buscar o estado inicial
from src.analysis_aggregator import get_consolidated_analysis

# Fallbacks (caso config falhe)
if 'ALL_NUMBERS' not in globals(): ALL_NUMBERS = list(range(1, 26))
if 'NEW_BALL_COLUMNS' not in globals(): NEW_BALL_COLUMNS = [f'b{i}' for i in range(1,16)]
if 'BASE_COLS' not in globals(): BASE_COLS = ['concurso'] + NEW_BALL_COLUMNS


# Novo tipo para a função da estratégia
StrategyFuncType = Callable[[Dict[str, Any]], Optional[Set[int]]]

class BacktesterRunner:
    """ Executa backtesting usando atualização incremental (Fase 1: Freq/Delay). """
    def __init__(self,
                 strategy_func: StrategyFuncType,
                 strategy_name: str,
                 start_contest: int,
                 end_contest: int,
                 initial_analysis_needed: bool = True):
        self.strategy_func = strategy_func
        self.strategy_name = strategy_name
        self.start_contest = start_contest
        self.end_contest = end_contest
        self.initial_analysis_needed = initial_analysis_needed
        self.logger = logger
        self.current_contest: int = 0
        self.overall_freq: Optional[pd.Series] = None
        self.current_delay: Optional[pd.Series] = None
        self.last_seen_concurso: Optional[Dict[int, int]] = None
        self.analysis_state: Dict[str, Any] = {}
        self.results_log: Dict[int, int] = {}
        self.draw_data_period: Optional[pd.DataFrame] = None # Armazena dados lidos

    def _initialize_state(self) -> bool:
        """ Calcula o estado inicial das métricas até start_contest - 1. """
        self.current_contest = self.start_contest - 1
        self.logger.info(f"Inicializando estado do backtester até concurso {self.current_contest}...")

        # Se precisar de todas as análises (ex: top_score), chama o agregador UMA VEZ
        if self.initial_analysis_needed:
            self.logger.info("Buscando análises consolidadas iniciais...")
            initial_agg_results = get_consolidated_analysis(self.current_contest)
            if initial_agg_results is None: self.logger.error("Falha ao obter análises iniciais."); return False
            self.analysis_state.update(initial_agg_results)
            self.overall_freq = self.analysis_state.get('overall_freq')
            self.current_delay = self.analysis_state.get('current_delay')
            if self.current_delay is not None:
                 self.last_seen_concurso = { n: self.current_contest - int(d) if pd.notna(d) else 0 for n, d in self.current_delay.items() }
                 self.logger.info("last_seen_concurso inicializado a partir do current_delay agregado.")
            else: self.logger.error("current_delay não encontrado."); return False
            self.logger.info("Análises consolidadas iniciais obtidas.")
        # Senão, calcula apenas as básicas (Freq, Delay)
        else:
            # Usa BASE_COLS importado do config
            initial_data = read_data_from_db(columns=BASE_COLS, concurso_maximo=self.current_contest)
            if initial_data is None: self.logger.error("Falha ao ler dados iniciais."); return False
            # Frequência Geral
            if initial_data.empty: self.overall_freq = pd.Series(0, index=ALL_NUMBERS)
            else: melted = initial_data[NEW_BALL_COLUMNS].melt(value_name='n')['n'].dropna().astype(int); self.overall_freq = melted.value_counts().reindex(ALL_NUMBERS, fill_value=0)
            self.analysis_state['overall_freq'] = self.overall_freq; logger.info("Freq. geral inicial calculada.")
            # Atraso Atual e Last Seen
            self.last_seen_concurso = {n: 0 for n in ALL_NUMBERS}; self.current_delay = pd.Series(self.current_contest + 1, index=ALL_NUMBERS, dtype='Int64')
            if not initial_data.empty:
                for index, row in initial_data.iloc[::-1].iterrows():
                    conc = int(row['concurso']); drawn = {int(n) for n in row[NEW_BALL_COLUMNS].dropna().values}
                    for num in ALL_NUMBERS:
                         if self.last_seen_concurso.get(num, 0) == 0 and num in drawn: self.last_seen_concurso[num] = conc
                for num in ALL_NUMBERS:
                    last_seen = self.last_seen_concurso[num]
                    if last_seen > 0: self.current_delay[num] = self.current_contest - last_seen
            self.analysis_state['current_delay'] = self.current_delay; logger.info("Atraso atual inicial calculado.")
            self.analysis_state['numbers_in_last_draw'] = set(initial_data.iloc[-1][NEW_BALL_COLUMNS].dropna().astype(int)) if not initial_data.empty else set()

        # Pré-lê os dados do período de backtest
        self.logger.info(f"Pré-lendo dados sorteios {self.start_contest} a {self.end_contest}...")
        # Usa BASE_COLS importado do config
        self.draw_data_period = read_data_from_db(columns=BASE_COLS, concurso_minimo=self.start_contest, concurso_maximo=self.end_contest)
        if self.draw_data_period is None or self.draw_data_period.empty: self.logger.error("Não ler dados período backtest."); return False
        self.draw_data_period.set_index('concurso', inplace=True); logger.info("Dados período backtest lidos.")

        if self.overall_freq is None or self.current_delay is None or self.last_seen_concurso is None: self.logger.error("Estado inicial incompleto."); return False
        return True


    def _update_state(self, drawn_numbers_current_draw: Set[int]):
        """ Atualiza incrementalmente Freq Geral e Atraso Atual. """
        self.current_contest += 1; logger.debug(f"Atualizando estado p/ FIM do Concurso {self.current_contest}...")
        # 1. Frequência Geral
        if self.overall_freq is not None:
             self.overall_freq.loc[list(drawn_numbers_current_draw)] += 1 # Mais eficiente
             self.analysis_state['overall_freq'] = self.overall_freq
        # 2. Atraso Atual e Last Seen
        if self.current_delay is not None and self.last_seen_concurso is not None:
            # Incrementa atraso para todos que NÃO saíram
            not_drawn = set(ALL_NUMBERS) - drawn_numbers_current_draw
            self.current_delay.loc[list(not_drawn)] += 1
            # Zera atraso e atualiza last_seen para os que saíram
            self.current_delay.loc[list(drawn_numbers_current_draw)] = 0
            for num in drawn_numbers_current_draw: self.last_seen_concurso[num] = self.current_contest
            self.analysis_state['current_delay'] = self.current_delay
        # 3. Outras métricas (AINDA NÃO INCREMENTAIS - Recalcula p/ top_score)
        if self.strategy_name == 'top_score':
             # logger.warning("Backtester incremental Fase 1: Recalculando métricas p/ 'top_score'.") # Log opcional
             temp_results = get_consolidated_analysis(self.current_contest)
             if temp_results:
                 self.analysis_state.update(temp_results)
                 if self.overall_freq is not None: self.analysis_state['overall_freq'] = self.overall_freq # Garante overwrite
                 if self.current_delay is not None: self.analysis_state['current_delay'] = self.current_delay # Garante overwrite
        # 4. Guarda números deste sorteio para próxima iteração
        self.analysis_state['numbers_in_last_draw'] = drawn_numbers_current_draw


    def run(self) -> Optional[Dict[int, int]]:
        """ Executa o loop de backtest. """
        start_time = time.time()
        if not self._initialize_state(): return None

        total_contests = self.end_contest - self.start_contest + 1; contests_processed = 0
        self.logger.info(f"Iniciando loop backtest ({self.start_contest} a {self.end_contest})...")

        for contest_to_play in range(self.start_contest, self.end_contest + 1):
            # Estado reflete fim do concurso self.current_contest (= contest_to_play - 1)
            current_analysis_for_strategy = self.analysis_state.copy()

            # Chama Estratégia
            self.logger.debug(f"Backtest [Conc. {contest_to_play}]: Aplicando '{self.strategy_name}'...")
            chosen_numbers = self.strategy_func(current_analysis_for_strategy) # Passa dict

            # Processa resultado
            actual_numbers_set = None
            if contest_to_play in self.draw_data_period.index:
                 # Usa NEW_BALL_COLUMNS importado do config
                 actual_numbers_set = set(self.draw_data_period.loc[contest_to_play][NEW_BALL_COLUMNS].dropna().astype(int))
            else: self.logger.error(f"Dados reais não encontrados p/ {contest_to_play}!"); self.results_log[contest_to_play] = -1

            if chosen_numbers is None or len(chosen_numbers) != 15 :
                self.logger.error(f"Estratégia falhou conc. {contest_to_play}."); self.results_log[contest_to_play] = -1
            elif actual_numbers_set is not None:
                hits = evaluate_hits(chosen_numbers, actual_numbers_set)
                self.results_log[contest_to_play] = hits; self.logger.debug(f"Conc. {contest_to_play}: {hits} acertos.")
            # else: erro já logado

            # Atualiza estado com dados REAIS do concurso que acabou de acontecer
            if actual_numbers_set is not None: self._update_state(actual_numbers_set)
            else: self.logger.critical(f"Fim backtest - dados faltantes conc. {contest_to_play}."); break

            contests_processed += 1
            if contests_processed % 100 == 0: self.logger.info(f"Progresso: {contests_processed}/{total_contests} processados.")

        # Fim do Loop
        duration = time.time() - start_time
        self.logger.info(f"Backtest concluído p/ '{self.strategy_name}'. Processados: {contests_processed}/{total_contests}.")
        self.logger.info(f"Tempo total: {duration:.2f} segundos.")
        return summarize_results(self.results_log)

--------------------------------------------------------------------------------
# Arquivo: src/config.py
--------------------------------------------------------------------------------
# Lotofacil_Analysis/src/config.py
import os
import logging
from dotenv import load_dotenv
from typing import List, Dict, Any

load_dotenv()
logger = logging.getLogger(__name__)

# --- Definições de Constantes no Nível do Módulo ---
BASE_DIR: str = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR: str = os.path.join(BASE_DIR, 'Data')
LOG_DIR: str = os.path.join(BASE_DIR, 'Logs')
PLOT_DIR: str = os.path.join(BASE_DIR, 'Plots')

PLOT_DIR_CONFIG: str = os.getenv('PLOT_DIR', PLOT_DIR)
RAW_DATA_FILE_NAME: str = os.getenv('RAW_DATA_FILE_NAME', 'historico.csv')
CLEANED_DATA_FILE_NAME: str = os.getenv('CLEANED_DATA_FILE_NAME', 'cleaned_draws.pkl')

_columns_to_keep_str: str = os.getenv('COLUMNS_TO_KEEP', 'Concurso,Data Sorteio,Bola1,Bola2,Bola3,Bola4,Bola5,Bola6,Bola7,Bola8,Bola9,Bola10,Bola11,Bola12,Bola13,Bola14,Bola15')
COLUMNS_TO_KEEP: List[str] = [col.strip() for col in _columns_to_keep_str.split(',')]

_new_column_names_str: str = os.getenv('NEW_COLUMN_NAMES', 'contest_id,Data Sorteio,ball_1,ball_2,ball_3,ball_4,ball_5,ball_6,ball_7,ball_8,ball_9,ball_10,ball_11,ball_12,ball_13,ball_14,ball_15')
NEW_COLUMN_NAMES: List[str] = [col.strip() for col in _new_column_names_str.split(',')]

_ball_number_columns_str: str = os.getenv('BALL_NUMBER_COLUMNS', 'ball_1,ball_2,ball_3,ball_4,ball_5,ball_6,ball_7,ball_8,ball_9,ball_10,ball_11,ball_12,ball_13,ball_14,ball_15')
BALL_NUMBER_COLUMNS: List[str] = [col.strip() for col in _ball_number_columns_str.split(',')]

DB_NAME: str = os.getenv('DB_NAME', 'lotofacil.db')
DB_PATH: str = os.path.join(DATA_DIR, DB_NAME)

HISTORICO_CSV_FILENAME: str = RAW_DATA_FILE_NAME
HISTORICO_CSV_PATH: str = os.path.join(DATA_DIR, HISTORICO_CSV_FILENAME)
CLEANED_DATA_PATH: str = os.path.join(DATA_DIR, CLEANED_DATA_FILE_NAME)

ALL_NUMBERS: List[int] = list(range(1, 26))
NUMBERS_PER_DRAW: int = 15

# --- Nomes de Colunas Padrão (Chaves Primárias e Identificadores) ---
DRAWN_NUMBERS_COLUMN_NAME: str = os.getenv('DRAWN_NUMBERS_COLUMN_NAME', 'drawn_numbers')
CONTEST_ID_COLUMN_NAME: str = os.getenv('CONTEST_ID_COLUMN_NAME', 'contest_id')
DATE_COLUMN_NAME: str = os.getenv('DATE_COLUMN_NAME', 'date')
DEZENA_COLUMN_NAME: str = "dezena"

# --- Nomes de Colunas para Tabelas de Análise Específicas (Já Adicionados) ---
CURRENT_DELAY_COLUMN_NAME: str = "current_delay"
MAX_DELAY_OBSERVED_COLUMN_NAME: str = "max_delay_observed"
AVG_DELAY_COLUMN_NAME: str = "avg_delay"
FREQUENCY_COLUMN_NAME: str = "frequency"
RELATIVE_FREQUENCY_COLUMN_NAME: str = "relative_frequency"
RECURRENCE_CDF_COLUMN_NAME: str = "recurrence_cdf"
RANK_SLOPE_COLUMN_NAME: str = "rank_slope"
TREND_STATUS_COLUMN_NAME: str = "trend_status"
CHUNK_TYPE_COLUMN_NAME: str = "chunk_type"
CHUNK_SIZE_COLUMN_NAME: str = "chunk_size"
CICLO_NUM_COLUMN_NAME: str = "ciclo_num"
IS_MISSING_IN_CURRENT_CYCLE_COLUMN_NAME: str = "is_missing_in_current_cycle"
CYCLE_CLOSING_SCORE_COLUMN_NAME: str = "score" # Coluna na DB, aggregator renomeia para cycle_closing_propensity_score
ITEMSET_STR_COLUMN_NAME: str = "itemset_str"
K_COLUMN_NAME: str = "k"
SUPPORT_COLUMN_NAME: str = "support"
CONFIDENCE_COLUMN_NAME: str = "confidence"
LIFT_COLUMN_NAME: str = "lift"
ITEMSET_SCORE_COLUMN_NAME: str = "itemset_score"
ITEMSET_CURRENT_DELAY_COLUMN_NAME: str = "itemset_current_delay"
ITEMSET_AVG_DELAY_COLUMN_NAME: str = "itemset_avg_delay"
ITEMSET_MAX_DELAY_COLUMN_NAME: str = "itemset_max_delay"

# --- Nomes de Tabelas Padronizados ---
MAIN_DRAWS_TABLE_NAME: str = "draws"
FLAT_DRAWS_TABLE_NAME: str = "draw_results_flat"

# Tabelas de Análise Primárias (consumidas pelo AnalysisAggregator)
ANALYSIS_DELAYS_TABLE_NAME: str = "analysis_delays"
ANALYSIS_FREQUENCY_OVERALL_TABLE_NAME: str = "analysis_frequency_overall"
ANALYSIS_RECURRENCE_CDF_TABLE_NAME: str = "analysis_recurrence_cdf"
ANALYSIS_ITEMSET_METRICS_TABLE_NAME: str = "analysis_itemset_metrics"
ANALYSIS_CYCLE_STATUS_DEZENAS_TABLE_NAME: str = "analysis_cycle_status_dezenas"
ANALYSIS_CYCLE_CLOSING_PROPENSITY_TABLE_NAME: str = "analysis_cycle_closing_propensity"
ANALYSIS_RANK_TREND_METRICS_TABLE_NAME: str = "analysis_rank_trend_metrics"

# Outras Tabelas de Análise e Base
PROPRIEDADES_NUMERICAS_POR_CONCURSO_TABLE_NAME: str = "propriedades_numericas_por_concurso"
REPETICAO_CONCURSO_ANTERIOR_TABLE_NAME: str = "analise_repeticao_concurso_anterior"
CHUNK_METRICS_TABLE_NAME: str = "chunk_metrics"
DRAW_POSITION_FREQUENCY_TABLE_NAME: str = "draw_position_frequency"
GERAL_MA_FREQUENCY_TABLE_NAME: str = "geral_ma_frequency"
GERAL_MA_DELAY_TABLE_NAME: str = "geral_ma_delay"
ASSOCIATION_RULES_TABLE_NAME: str = "association_rules"
GRID_LINE_DISTRIBUTION_TABLE_NAME: str = "grid_line_distribution"
GRID_COLUMN_DISTRIBUTION_TABLE_NAME: str = "grid_column_distribution"
STATISTICAL_TESTS_RESULTS_TABLE_NAME: str = "statistical_tests_results"
MONTHLY_NUMBER_FREQUENCY_TABLE_NAME: str = "monthly_number_frequency"
MONTHLY_DRAW_PROPERTIES_TABLE_NAME: str = "monthly_draw_properties_summary"
SEQUENCE_METRICS_TABLE_NAME: str = "sequence_metrics"
FREQUENT_ITEMSETS_TABLE_NAME: str = "frequent_itemsets" # Tabela base para itemset_metrics

# Tabelas de Ciclo (Detalhes, Sumário, Progressão Bruta, Métricas por Ciclo)
ANALYSIS_CYCLES_DETAIL_TABLE_NAME: str = "analysis_cycles_detail"
ANALYSIS_CYCLES_SUMMARY_TABLE_NAME: str = "analysis_cycles_summary"
ANALYSIS_CYCLE_PROGRESSION_RAW_TABLE_NAME: str = "analysis_cycle_progression_raw"
CYCLES_DETAIL_TABLE_NAME_INPUT_FOR_AGG: str = ANALYSIS_CYCLES_DETAIL_TABLE_NAME # Para BlockAggregator
CYCLE_METRIC_FREQUENCY_TABLE_NAME: str = "ciclo_metric_frequency"
CYCLE_METRIC_ATRASO_MEDIO_TABLE_NAME: str = "ciclo_metric_atraso_medio"
CYCLE_METRIC_ATRASO_MAXIMO_TABLE_NAME: str = "ciclo_metric_atraso_maximo"
CYCLE_METRIC_ATRASO_FINAL_TABLE_NAME: str = "ciclo_metric_atraso_final"
CYCLE_RANK_FREQUENCY_TABLE_NAME: str = "ciclo_rank_frequency"
CYCLE_GROUP_METRICS_TABLE_NAME: str = "ciclo_group_metrics"

# Prefixos e Nomes de Tabelas Agregadas
EVOL_METRIC_FREQUENCY_BLOCK_PREFIX: str = "evol_metric_frequency_bloco"
EVOL_RANK_FREQUENCY_BLOCK_PREFIX: str = "evol_rank_frequency_bloco"
EVOL_METRIC_ATRASO_MEDIO_BLOCK_PREFIX: str = "evol_metric_atraso_medio_bloco"
EVOL_METRIC_ATRASO_MAXIMO_BLOCK_PREFIX: str = "evol_metric_atraso_maximo_bloco"
EVOL_METRIC_ATRASO_FINAL_BLOCK_PREFIX: str = "evol_metric_atraso_final_bloco"
EVOL_METRIC_OCCURRENCE_STD_DEV_BLOCK_PREFIX: str = "evol_metric_occurrence_std_dev_bloco"
EVOL_METRIC_DELAY_STD_DEV_BLOCK_PREFIX: str = "evol_metric_delay_std_dev_bloco"
EVOL_BLOCK_GROUP_METRICS_PREFIX: str = "evol_block_group_metrics"

BLOCK_ANALISES_CONSOLIDADAS_PREFIX: str = "bloco_analises_consolidadas"
CYCLE_ANALISES_CONSOLIDADAS_TABLE_NAME: str = "cycle_analises_consolidadas"

_default_chunk_type_for_rank_trend_env: str = os.getenv('DEFAULT_CHUNK_TYPE_FOR_PLOTTING', 'linear')
_default_chunk_size_for_rank_trend_env: str = os.getenv('DEFAULT_CHUNK_SIZE_FOR_PLOTTING', '50')
BLOCK_AGGREGATED_DATA_FOR_RANK_TREND_TABLE_NAME: str = f"{BLOCK_ANALISES_CONSOLIDADAS_PREFIX}_{_default_chunk_type_for_rank_trend_env}_{_default_chunk_size_for_rank_trend_env}"


# --- Outras Configurações de Análise ---
# (Restante das suas configurações como CHUNK_TYPES_CONFIG, APRIORI, LOG_LEVEL, etc. permanecem aqui)
# ... (COLE O RESTANTE DO SEU ARQUIVO CONFIG.PY ORIGINAL A PARTIR DAQUI) ...

CHUNK_TYPES_CONFIG: Dict[str, List[int]] = {
    "linear": [int(s.strip()) for s in os.getenv('CHUNK_TYPES_LINEAR', '10,25,50,75,100,150,200').split(',')],
    "fibonacci": [int(s.strip()) for s in os.getenv('CHUNK_TYPES_FIBONACCI', '5,8,13,21,34,55,89,144').split(',')],
    "primes": [int(s.strip()) for s in os.getenv('CHUNK_TYPES_PRIMES', '2,3,5,7,11,13,17,19,23,29,31,37,41,43,47,53,59,61,67,71,73,79,83,89,97').split(',')],
    "doidos": [int(s.strip()) for s in os.getenv('CHUNK_TYPES_DOIDOS', '33,66,99,122,157,190').split(',')]
}
APRIORI_MIN_SUPPORT: float = float(os.getenv('APRIORI_MIN_SUPPORT', 0.02))
FREQUENT_ITEMSETS_MIN_LEN: int = int(os.getenv('FREQUENT_ITEMSETS_MIN_LEN', 3))
FREQUENT_ITEMSETS_MAX_LEN: int = int(os.getenv('FREQUENT_ITEMSETS_MAX_LEN', 8))
LOG_LEVEL: str = os.getenv('LOG_LEVEL', 'INFO').upper()
LOG_FILE: str = os.path.join(LOG_DIR, 'lotofacil_analysis.log')
DEFAULT_CHUNK_TYPE_FOR_PLOTTING: str = os.getenv('DEFAULT_CHUNK_TYPE_FOR_PLOTTING', 'linear')
DEFAULT_CHUNK_SIZE_FOR_PLOTTING: int = int(os.getenv('DEFAULT_CHUNK_SIZE_FOR_PLOTTING', '50'))
_default_dezenas_plot_str: str = os.getenv('DEFAULT_DEZENAS_FOR_CHUNK_EVOLUTION_PLOT', '1,7,13,19,25')
DEFAULT_DEZENAS_FOR_CHUNK_EVOLUTION_PLOT: List[int] = [int(d.strip()) for d in _default_dezenas_plot_str.split(',')]
SEQUENCE_ANALYSIS_CONFIG = {
    "consecutive": {"min_len": 3, "max_len": 5, "active": True},
    "arithmetic_steps": {"steps_to_check": [2, 3], "min_len": 3, "max_len": 4, "active": True}
}
_geral_ma_freq_windows_str: str = os.getenv('GERAL_MA_FREQ_WINDOWS', '5,10,20,30')
GERAL_MA_FREQUENCY_WINDOWS: List[int] = [int(w.strip()) for w in _geral_ma_freq_windows_str.split(',')]
_geral_ma_delay_windows_str: str = os.getenv('GERAL_MA_DELAY_WINDOWS', '5,10,20,30')
GERAL_MA_DELAY_WINDOWS: List[int] = [int(w.strip()) for w in _geral_ma_delay_windows_str.split(',')]
ASSOCIATION_RULES_MIN_CONFIDENCE: float = float(os.getenv('ASSOCIATION_RULES_MIN_CONFIDENCE', '0.5'))
ASSOCIATION_RULES_MIN_LIFT: float = float(os.getenv('ASSOCIATION_RULES_MIN_LIFT', '1.0'))
LOTOFACIL_GRID_LINES: Dict[str, List[int]] = {
    "L1": [1, 2, 3, 4, 5], "L2": [6, 7, 8, 9, 10], "L3": [11, 12, 13, 14, 15],
    "L4": [16, 17, 18, 19, 20], "L5": [21, 22, 23, 24, 25],
}
LOTOFACIL_GRID_COLUMNS: Dict[str, List[int]] = {
    "C1": [1, 6, 11, 16, 21], "C2": [2, 7, 12, 17, 22], "C3": [3, 8, 13, 18, 23],
    "C4": [4, 9, 14, 19, 24], "C5": [5, 10, 15, 20, 25],
}
SUM_NORMALITY_TEST_BINS: int = int(os.getenv('SUM_NORMALITY_TEST_BINS', '10'))
POISSON_DISTRIBUTION_TEST_CONFIG: Dict[str, Dict[str, Any]] = {
    "Count_Primos_Per_Draw": {"column_name": "primos", "max_observed_count_for_chi2": 10},
    "Count_Pares_Per_Draw": {"column_name": "pares", "max_observed_count_for_chi2": 10},
    "Count_Impares_Per_Draw": {"column_name": "impares", "max_observed_count_for_chi2": 10}
}
AGGREGATOR_DEFAULT_RECENT_WINDOW: int = int(os.getenv('AGGREGATOR_DEFAULT_RECENT_WINDOW', '10'))
MIN_CONTESTS_FOR_HISTORICAL_DELAY: int = int(os.getenv('MIN_CONTESTS_FOR_HISTORICAL_DELAY', '10'))
MIN_CONTESTS_FOR_HISTORICAL_RECURRENCE: int = int(os.getenv('MIN_CONTESTS_FOR_HISTORICAL_RECURRENCE', '10'))
MIN_CONTESTS_FOR_ITEMSET_METRICS: int = int(os.getenv('MIN_CONTESTS_FOR_ITEMSET_METRICS', '10'))
_itemset_k_part_score_str = os.getenv('ITEMSET_K_VALUES_FOR_PARTICIPATION_SCORE', '2,3')
ITEMSET_K_VALUES_FOR_PARTICIPATION_SCORE: List[int] = [int(k.strip()) for k in _itemset_k_part_score_str.split(',')]
_itemset_default_k_agg_str = os.getenv('ITEMSET_DEFAULT_K_VALUES_AGGREGATOR', '2,3')
ITEMSET_DEFAULT_K_VALUES_AGGREGATOR: List[int] = [int(k.strip()) for k in _itemset_default_k_agg_str.split(',')]
RANK_TREND_WINDOW_BLOCKS: int = int(os.getenv('RANK_TREND_WINDOW_BLOCKS', '5'))
RANK_TREND_SLOPE_IMPROVING_THRESHOLD: float = float(os.getenv('RANK_TREND_SLOPE_IMPROVING_THRESHOLD', '-0.1'))
RANK_TREND_SLOPE_WORSENING_THRESHOLD: float = float(os.getenv('RANK_TREND_SLOPE_WORSENING_THRESHOLD', '0.1'))
RANK_VALUE_COLUMN_FOR_TREND: str = os.getenv('RANK_VALUE_COLUMN_FOR_TREND', 'rank_no_bloco')
RANK_ANALYSIS_TYPE_FILTER_FOR_TREND: str = os.getenv('RANK_ANALYSIS_TYPE_FILTER_FOR_TREND', 'rank_freq_bloco')


class Config:
    BASE_DIR: str = BASE_DIR
    DATA_DIR: str = DATA_DIR
    LOG_DIR: str = LOG_DIR
    PLOT_DIR: str = PLOT_DIR
    PLOT_DIR_CONFIG: str = PLOT_DIR_CONFIG
    DB_PATH: str = DB_PATH
    RAW_DATA_FILE_NAME: str = RAW_DATA_FILE_NAME
    CLEANED_DATA_FILE_NAME: str = CLEANED_DATA_FILE_NAME
    HISTORICO_CSV_PATH: str = HISTORICO_CSV_PATH
    CLEANED_DATA_PATH: str = CLEANED_DATA_PATH

    ALL_NUMBERS: List[int] = ALL_NUMBERS
    NUMBERS_PER_DRAW: int = NUMBERS_PER_DRAW

    COLUMNS_TO_KEEP: List[str] = COLUMNS_TO_KEEP
    NEW_COLUMN_NAMES: List[str] = NEW_COLUMN_NAMES
    BALL_NUMBER_COLUMNS: List[str] = BALL_NUMBER_COLUMNS
    DRAWN_NUMBERS_COLUMN_NAME: str = DRAWN_NUMBERS_COLUMN_NAME
    CONTEST_ID_COLUMN_NAME: str = CONTEST_ID_COLUMN_NAME
    DATE_COLUMN_NAME: str = DATE_COLUMN_NAME
    DEZENA_COLUMN_NAME: str = DEZENA_COLUMN_NAME

    CURRENT_DELAY_COLUMN_NAME: str = CURRENT_DELAY_COLUMN_NAME
    MAX_DELAY_OBSERVED_COLUMN_NAME: str = MAX_DELAY_OBSERVED_COLUMN_NAME
    AVG_DELAY_COLUMN_NAME: str = AVG_DELAY_COLUMN_NAME
    FREQUENCY_COLUMN_NAME: str = FREQUENCY_COLUMN_NAME
    RELATIVE_FREQUENCY_COLUMN_NAME: str = RELATIVE_FREQUENCY_COLUMN_NAME
    RECURRENCE_CDF_COLUMN_NAME: str = RECURRENCE_CDF_COLUMN_NAME
    RANK_SLOPE_COLUMN_NAME: str = RANK_SLOPE_COLUMN_NAME
    TREND_STATUS_COLUMN_NAME: str = TREND_STATUS_COLUMN_NAME
    CHUNK_TYPE_COLUMN_NAME: str = CHUNK_TYPE_COLUMN_NAME
    CHUNK_SIZE_COLUMN_NAME: str = CHUNK_SIZE_COLUMN_NAME
    CICLO_NUM_COLUMN_NAME: str = CICLO_NUM_COLUMN_NAME
    IS_MISSING_IN_CURRENT_CYCLE_COLUMN_NAME: str = IS_MISSING_IN_CURRENT_CYCLE_COLUMN_NAME
    CYCLE_CLOSING_SCORE_COLUMN_NAME: str = CYCLE_CLOSING_SCORE_COLUMN_NAME
    ITEMSET_STR_COLUMN_NAME: str = ITEMSET_STR_COLUMN_NAME
    K_COLUMN_NAME: str = K_COLUMN_NAME
    SUPPORT_COLUMN_NAME: str = SUPPORT_COLUMN_NAME
    CONFIDENCE_COLUMN_NAME: str = CONFIDENCE_COLUMN_NAME
    LIFT_COLUMN_NAME: str = LIFT_COLUMN_NAME
    ITEMSET_SCORE_COLUMN_NAME: str = ITEMSET_SCORE_COLUMN_NAME
    ITEMSET_CURRENT_DELAY_COLUMN_NAME: str = ITEMSET_CURRENT_DELAY_COLUMN_NAME
    ITEMSET_AVG_DELAY_COLUMN_NAME: str = ITEMSET_AVG_DELAY_COLUMN_NAME
    ITEMSET_MAX_DELAY_COLUMN_NAME: str = ITEMSET_MAX_DELAY_COLUMN_NAME

    MAIN_DRAWS_TABLE_NAME: str = MAIN_DRAWS_TABLE_NAME
    FLAT_DRAWS_TABLE_NAME: str = FLAT_DRAWS_TABLE_NAME
    ANALYSIS_DELAYS_TABLE_NAME: str = ANALYSIS_DELAYS_TABLE_NAME
    ANALYSIS_FREQUENCY_OVERALL_TABLE_NAME: str = ANALYSIS_FREQUENCY_OVERALL_TABLE_NAME
    ANALYSIS_RECURRENCE_CDF_TABLE_NAME: str = ANALYSIS_RECURRENCE_CDF_TABLE_NAME
    ANALYSIS_ITEMSET_METRICS_TABLE_NAME: str = ANALYSIS_ITEMSET_METRICS_TABLE_NAME
    ANALYSIS_CYCLE_STATUS_DEZENAS_TABLE_NAME: str = ANALYSIS_CYCLE_STATUS_DEZENAS_TABLE_NAME
    ANALYSIS_CYCLE_CLOSING_PROPENSITY_TABLE_NAME: str = ANALYSIS_CYCLE_CLOSING_PROPENSITY_TABLE_NAME
    ANALYSIS_RANK_TREND_METRICS_TABLE_NAME: str = ANALYSIS_RANK_TREND_METRICS_TABLE_NAME
    ANALYSIS_CYCLES_DETAIL_TABLE_NAME: str = ANALYSIS_CYCLES_DETAIL_TABLE_NAME
    ANALYSIS_CYCLES_SUMMARY_TABLE_NAME: str = ANALYSIS_CYCLES_SUMMARY_TABLE_NAME
    ANALYSIS_CYCLE_PROGRESSION_RAW_TABLE_NAME: str = ANALYSIS_CYCLE_PROGRESSION_RAW_TABLE_NAME
    
    PROPRIEDADES_NUMERICAS_POR_CONCURSO_TABLE_NAME: str = PROPRIEDADES_NUMERICAS_POR_CONCURSO_TABLE_NAME
    REPETICAO_CONCURSO_ANTERIOR_TABLE_NAME: str = REPETICAO_CONCURSO_ANTERIOR_TABLE_NAME
    CHUNK_METRICS_TABLE_NAME: str = CHUNK_METRICS_TABLE_NAME
    DRAW_POSITION_FREQUENCY_TABLE_NAME: str = DRAW_POSITION_FREQUENCY_TABLE_NAME
    GERAL_MA_FREQUENCY_TABLE_NAME: str = GERAL_MA_FREQUENCY_TABLE_NAME
    GERAL_MA_DELAY_TABLE_NAME: str = GERAL_MA_DELAY_TABLE_NAME
    ASSOCIATION_RULES_TABLE_NAME: str = ASSOCIATION_RULES_TABLE_NAME
    GRID_LINE_DISTRIBUTION_TABLE_NAME: str = GRID_LINE_DISTRIBUTION_TABLE_NAME
    GRID_COLUMN_DISTRIBUTION_TABLE_NAME: str = GRID_COLUMN_DISTRIBUTION_TABLE_NAME
    STATISTICAL_TESTS_RESULTS_TABLE_NAME: str = STATISTICAL_TESTS_RESULTS_TABLE_NAME
    MONTHLY_NUMBER_FREQUENCY_TABLE_NAME: str = MONTHLY_NUMBER_FREQUENCY_TABLE_NAME
    MONTHLY_DRAW_PROPERTIES_TABLE_NAME: str = MONTHLY_DRAW_PROPERTIES_TABLE_NAME
    SEQUENCE_METRICS_TABLE_NAME: str = SEQUENCE_METRICS_TABLE_NAME
    FREQUENT_ITEMSETS_TABLE_NAME: str = FREQUENT_ITEMSETS_TABLE_NAME

    EVOL_METRIC_FREQUENCY_BLOCK_PREFIX: str = EVOL_METRIC_FREQUENCY_BLOCK_PREFIX
    EVOL_RANK_FREQUENCY_BLOCK_PREFIX: str = EVOL_RANK_FREQUENCY_BLOCK_PREFIX
    EVOL_METRIC_ATRASO_MEDIO_BLOCK_PREFIX: str = EVOL_METRIC_ATRASO_MEDIO_BLOCK_PREFIX
    EVOL_METRIC_ATRASO_MAXIMO_BLOCK_PREFIX: str = EVOL_METRIC_ATRASO_MAXIMO_BLOCK_PREFIX
    EVOL_METRIC_ATRASO_FINAL_BLOCK_PREFIX: str = EVOL_METRIC_ATRASO_FINAL_BLOCK_PREFIX
    EVOL_METRIC_OCCURRENCE_STD_DEV_BLOCK_PREFIX: str = EVOL_METRIC_OCCURRENCE_STD_DEV_BLOCK_PREFIX
    EVOL_METRIC_DELAY_STD_DEV_BLOCK_PREFIX: str = EVOL_METRIC_DELAY_STD_DEV_BLOCK_PREFIX
    EVOL_BLOCK_GROUP_METRICS_PREFIX: str = EVOL_BLOCK_GROUP_METRICS_PREFIX

    CYCLES_DETAIL_TABLE_NAME_INPUT_FOR_AGG: str = CYCLES_DETAIL_TABLE_NAME_INPUT_FOR_AGG
    CYCLE_METRIC_FREQUENCY_TABLE_NAME: str = CYCLE_METRIC_FREQUENCY_TABLE_NAME
    CYCLE_METRIC_ATRASO_MEDIO_TABLE_NAME: str = CYCLE_METRIC_ATRASO_MEDIO_TABLE_NAME
    CYCLE_METRIC_ATRASO_MAXIMO_TABLE_NAME: str = CYCLE_METRIC_ATRASO_MAXIMO_TABLE_NAME
    CYCLE_METRIC_ATRASO_FINAL_TABLE_NAME: str = CYCLE_METRIC_ATRASO_FINAL_TABLE_NAME
    CYCLE_RANK_FREQUENCY_TABLE_NAME: str = CYCLE_RANK_FREQUENCY_TABLE_NAME
    CYCLE_GROUP_METRICS_TABLE_NAME: str = CYCLE_GROUP_METRICS_TABLE_NAME

    BLOCK_ANALISES_CONSOLIDADAS_PREFIX: str = BLOCK_ANALISES_CONSOLIDADAS_PREFIX
    BLOCK_AGGREGATED_DATA_FOR_RANK_TREND_TABLE_NAME: str = BLOCK_AGGREGATED_DATA_FOR_RANK_TREND_TABLE_NAME
    CYCLE_ANALISES_CONSOLIDADAS_TABLE_NAME: str = CYCLE_ANALISES_CONSOLIDADAS_TABLE_NAME

    CHUNK_TYPES_CONFIG: Dict[str, List[int]] = CHUNK_TYPES_CONFIG
    CHUNK_TYPES: Dict[str, List[int]] = CHUNK_TYPES_CONFIG

    APRIORI_MIN_SUPPORT: float = APRIORI_MIN_SUPPORT
    FREQUENT_ITEMSETS_MIN_LEN: int = FREQUENT_ITEMSETS_MIN_LEN
    FREQUENT_ITEMSETS_MAX_LEN: int = FREQUENT_ITEMSETS_MAX_LEN

    LOG_LEVEL: str = LOG_LEVEL
    LOG_FILE: str = LOG_FILE

    DEFAULT_CHUNK_TYPE_FOR_PLOTTING: str = DEFAULT_CHUNK_TYPE_FOR_PLOTTING
    DEFAULT_CHUNK_SIZE_FOR_PLOTTING: int = DEFAULT_CHUNK_SIZE_FOR_PLOTTING
    DEFAULT_DEZENAS_FOR_CHUNK_EVOLUTION_PLOT: List[int] = DEFAULT_DEZENAS_FOR_CHUNK_EVOLUTION_PLOT

    SEQUENCE_ANALYSIS_CONFIG: Dict[str,Dict[str,Any]] = SEQUENCE_ANALYSIS_CONFIG
    GERAL_MA_FREQUENCY_WINDOWS: List[int] = GERAL_MA_FREQUENCY_WINDOWS
    GERAL_MA_DELAY_WINDOWS: List[int] = GERAL_MA_DELAY_WINDOWS
    ASSOCIATION_RULES_MIN_CONFIDENCE: float = ASSOCIATION_RULES_MIN_CONFIDENCE
    ASSOCIATION_RULES_MIN_LIFT: float = ASSOCIATION_RULES_MIN_LIFT
    LOTOFACIL_GRID_LINES: Dict[str, List[int]] = LOTOFACIL_GRID_LINES
    LOTOFACIL_GRID_COLUMNS: Dict[str, List[int]] = LOTOFACIL_GRID_COLUMNS
    SUM_NORMALITY_TEST_BINS: int = SUM_NORMALITY_TEST_BINS
    POISSON_DISTRIBUTION_TEST_CONFIG: Dict[str, Dict[str, Any]] = POISSON_DISTRIBUTION_TEST_CONFIG

    AGGREGATOR_DEFAULT_RECENT_WINDOW: int = AGGREGATOR_DEFAULT_RECENT_WINDOW
    MIN_CONTESTS_FOR_HISTORICAL_DELAY: int = MIN_CONTESTS_FOR_HISTORICAL_DELAY
    MIN_CONTESTS_FOR_HISTORICAL_RECURRENCE: int = MIN_CONTESTS_FOR_HISTORICAL_RECURRENCE
    MIN_CONTESTS_FOR_ITEMSET_METRICS: int = MIN_CONTESTS_FOR_ITEMSET_METRICS
    ITEMSET_K_VALUES_FOR_PARTICIPATION_SCORE: List[int] = ITEMSET_K_VALUES_FOR_PARTICIPATION_SCORE
    ITEMSET_DEFAULT_K_VALUES_AGGREGATOR: List[int] = ITEMSET_DEFAULT_K_VALUES_AGGREGATOR

    RANK_TREND_WINDOW_BLOCKS: int = RANK_TREND_WINDOW_BLOCKS
    RANK_TREND_SLOPE_IMPROVING_THRESHOLD: float = RANK_TREND_SLOPE_IMPROVING_THRESHOLD
    RANK_TREND_SLOPE_WORSENING_THRESHOLD: float = RANK_TREND_SLOPE_WORSENING_THRESHOLD
    RANK_VALUE_COLUMN_FOR_TREND: str = RANK_VALUE_COLUMN_FOR_TREND
    RANK_ANALYSIS_TYPE_FILTER_FOR_TREND: str = RANK_ANALYSIS_TYPE_FILTER_FOR_TREND

    def __init__(self):
        os.makedirs(self.LOG_DIR, exist_ok=True)
        os.makedirs(self.PLOT_DIR, exist_ok=True)
        os.makedirs(self.DATA_DIR, exist_ok=True)
        logger.info("Objeto Config instanciado. Configurações carregadas.")
        # Log de debug para verificar constantes chave
        key_debug_attrs = [
            'CONTEST_ID_COLUMN_NAME', 'DEZENA_COLUMN_NAME', 'ANALYSIS_DELAYS_TABLE_NAME',
            'CURRENT_DELAY_COLUMN_NAME', 'FREQUENCY_COLUMN_NAME', 'RECURRENCE_CDF_COLUMN_NAME',
            'RANK_SLOPE_COLUMN_NAME', 'CICLO_NUM_COLUMN_NAME', 'CYCLE_CLOSING_SCORE_COLUMN_NAME',
            'ITEMSET_STR_COLUMN_NAME', 'ANALYSIS_ITEMSET_METRICS_TABLE_NAME',
            'PROPRIEDADES_NUMERICAS_POR_CONCURSO_TABLE_NAME', 
            'REPETICAO_CONCURSO_ANTERIOR_TABLE_NAME', 'CHUNK_METRICS_TABLE_NAME'
        ]
        for attr_name in key_debug_attrs:
            if hasattr(self, attr_name):
                logger.debug(f"{attr_name}: {getattr(self, attr_name)}")

try:
    config_obj = Config()
except Exception as e:
    print(f"ERRO CRÍTICO: Falha ao instanciar Config global: {e}")
    logger.critical(f"ERRO CRÍTICO: Falha ao instanciar Config global: {e}", exc_info=True)
    config_obj = None # type: ignore

if __name__ == '__main__':
    if config_obj:
        print("--- Testando Constantes de Configuração ---")
        for attr_name in dir(config_obj):
            if not attr_name.startswith('__') and not callable(getattr(config_obj, attr_name)):
                print(f"{attr_name}: {getattr(config_obj, attr_name)}")
    else:
        print("Instância config_obj não pôde ser criada.")

--------------------------------------------------------------------------------
# Arquivo: src/data_loader.py
--------------------------------------------------------------------------------
# src/data_loader.py
import pandas as pd
from pathlib import Path
import logging

# Importar as constantes de configuração relevantes
from src.config import (
    RAW_DATA_FILE_NAME,
    CLEANED_DATA_FILE_NAME,
    COLUMNS_TO_KEEP,
    NEW_COLUMN_NAMES,
    BALL_NUMBER_COLUMNS,
    CONTEST_ID_COLUMN_NAME, # <<< ADICIONADO IMPORT
    DATE_COLUMN_NAME # Para consistência com final_expected_cols
)

logger = logging.getLogger(__name__)

def load_and_clean_data(raw_file_path: str, cleaned_file_path_to_save: str) -> pd.DataFrame:
    """
    Carrega os dados brutos do arquivo CSV, realiza a limpeza e os transforma.
    Salva os dados limpos em formato pickle para carregamentos futuros mais rápidos.
    """
    try:
        logger.info(f"Iniciando carregamento e limpeza de dados de: {raw_file_path}")
        try:
            df = pd.read_csv(raw_file_path, sep=';', encoding='utf-8', header=0)
            logger.info(f"Dados carregados com sucesso de CSV (UTF-8): {raw_file_path}")
        except UnicodeDecodeError:
            logger.warning(f"Falha ao decodificar {raw_file_path} com UTF-8. Tentando com ISO-8859-1.")
            df = pd.read_csv(raw_file_path, sep=';', encoding='iso-8859-1', header=0)
            logger.info(f"Dados carregados com sucesso de CSV (ISO-8859-1): {raw_file_path}")

        logger.debug(f"Colunas originais do CSV: {df.columns.tolist()}")
        
        actual_columns_to_keep_from_config = [col for col in COLUMNS_TO_KEEP if col in df.columns]
        
        if len(actual_columns_to_keep_from_config) != len(COLUMNS_TO_KEEP):
            missing_cols = set(COLUMNS_TO_KEEP) - set(actual_columns_to_keep_from_config)
            logger.warning(f"Colunas de COLUMNS_TO_KEEP não encontradas no arquivo: {missing_cols}. Usando as que existem.")
            if not actual_columns_to_keep_from_config:
                logger.error(f"Nenhuma coluna de COLUMNS_TO_KEEP ({COLUMNS_TO_KEEP}) encontrada no arquivo. Verifique a configuração e o arquivo de dados.")
                return pd.DataFrame()
        
        df_processed = df[actual_columns_to_keep_from_config].copy()

        current_col_names = df_processed.columns.tolist()
        # Garante que NEW_COLUMN_NAMES tenha o tamanho certo para o zip
        effective_new_column_names = NEW_COLUMN_NAMES[:len(current_col_names)]
        rename_map = dict(zip(current_col_names, effective_new_column_names))
        df_processed.rename(columns=rename_map, inplace=True)
        logger.debug(f"Colunas renomeadas para: {df_processed.columns.tolist()}")

        # A lógica original do seu data_loader verifica por 'Data Sorteio' APÓS a renomeação.
        # Isso implica que 'Data Sorteio' não deve ser renomeada agressivamente por NEW_COLUMN_NAMES
        # se esta lógica for para funcionar como está.
        # Se NEW_COLUMN_NAMES[1] (correspondente a COLUMNS_TO_KEEP[1] == 'Data Sorteio')
        # for, por exemplo, 'draw_date_str', então a verificação abaixo deveria ser
        # if 'draw_date_str' in df_processed.columns:
        # Para o config.py que forneci, NEW_COLUMN_NAMES[1] é 'Data Sorteio'.
        
        date_column_after_rename = None
        if COLUMNS_TO_KEEP[1] == 'Data Sorteio': # Assumindo que a segunda coluna em COLUMNS_TO_KEEP é a data
            date_column_after_rename = NEW_COLUMN_NAMES[1] # Este é o nome da coluna de data após a renomeação

        if date_column_after_rename and date_column_after_rename in df_processed.columns:
            # DATE_COLUMN_NAME é o nome final da coluna de data ('date' por default)
            df_processed[DATE_COLUMN_NAME] = pd.to_datetime(df_processed[date_column_after_rename], format='%d/%m/%Y', errors='coerce')
            if date_column_after_rename != DATE_COLUMN_NAME: # Só dropa se o nome for diferente do nome final
                df_processed.drop(columns=[date_column_after_rename], inplace=True)
            df_processed.dropna(subset=[DATE_COLUMN_NAME], inplace=True)
        else:
            logger.warning(f"Coluna de data ('{date_column_after_rename}') não encontrada após renomeação para conversão. Verifique COLUMNS_TO_KEEP e NEW_COLUMN_NAMES.")

        for ball_col_name in BALL_NUMBER_COLUMNS:
            if ball_col_name in df_processed.columns:
                df_processed[ball_col_name] = pd.to_numeric(df_processed[ball_col_name], errors='coerce')
            else:
                logger.warning(f"Coluna de bola esperada '{ball_col_name}' não encontrada após renomeação.")
        
        existing_ball_cols_for_dropna = [col for col in BALL_NUMBER_COLUMNS if col in df_processed.columns]
        if existing_ball_cols_for_dropna:
            df_processed.dropna(subset=existing_ball_cols_for_dropna, inplace=True)
            for ball_col_name in existing_ball_cols_for_dropna:
                 df_processed[ball_col_name] = df_processed[ball_col_name].astype(int)
        else:
            logger.warning("Nenhuma coluna de bola encontrada para verificar NaNs ou converter para inteiro.")

        # --- CORREÇÃO AQUI para essential_cols e final_expected_cols ---
        # Usar os nomes de colunas FINAIS padronizados do config.py
        final_expected_cols = [CONTEST_ID_COLUMN_NAME, DATE_COLUMN_NAME] + BALL_NUMBER_COLUMNS
        
        cols_to_select_final = [col for col in final_expected_cols if col in df_processed.columns]
        
        essential_cols = [CONTEST_ID_COLUMN_NAME] + BALL_NUMBER_COLUMNS # Colunas essenciais após toda a renomeação
        # --- FIM DA CORREÇÃO ---
        
        missing_essential_cols = [col for col in essential_cols if col not in df_processed.columns]
        if missing_essential_cols:
            logger.error(f"Colunas essenciais estão faltando após o processamento: {missing_essential_cols}. Verifique a configuração e os dados.")
            logger.debug(f"Colunas disponíveis em df_processed: {df_processed.columns.tolist()}")
            return pd.DataFrame()
            
        df_final = df_processed[cols_to_select_final].copy() # Usar .copy() para evitar SettingWithCopyWarning

        # Adicionar a coluna 'drawn_numbers' (lista de dezenas) que muitas análises podem esperar
        # Esta coluna não estava sendo criada no seu data_loader.py, mas é um padrão útil.
        # As análises que forneci (como combination_analysis) esperam esta coluna.
        # Se você não a quiser, as análises precisarão ser ajustadas para ler de 'ball_1'...'ball_15'.
        try:
            df_final['drawn_numbers'] = df_final[BALL_NUMBER_COLUMNS].apply(lambda row: sorted([int(num) for num in row if pd.notna(num)]), axis=1)
            logger.info("Coluna 'drawn_numbers' (lista de dezenas) criada.")
        except Exception as e_drawn:
            logger.warning(f"Não foi possível criar a coluna 'drawn_numbers': {e_drawn}")


        df_final.to_pickle(cleaned_file_path_to_save)
        logger.info(f"Dados limpos e transformados ({len(df_final)} linhas) salvos em: {cleaned_file_path_to_save}")
        
        return df_final

    except FileNotFoundError:
        logger.error(f"Arquivo de dados brutos não encontrado em: {raw_file_path}. Verifique o caminho.")
        return pd.DataFrame()
    except Exception as e:
        logger.error(f"Erro durante o carregamento e limpeza dos dados de '{raw_file_path}': {e}", exc_info=True)
        return pd.DataFrame()

def load_cleaned_data(data_dir_path: str) -> pd.DataFrame:
    """
    Carrega os dados limpos de um arquivo pickle.
    """
    cleaned_file_path = Path(data_dir_path) / CLEANED_DATA_FILE_NAME # CLEANED_DATA_FILE_NAME é importado do config
    try:
        logger.info(f"Carregando dados limpos de: {cleaned_file_path}")
        df = pd.read_pickle(cleaned_file_path)
        logger.info(f"Dados limpos carregados com sucesso de: {cleaned_file_path}. DataFrame com {len(df)} linhas.")
        return df
    except FileNotFoundError:
        logger.warning(f"Arquivo de dados limpos não encontrado em: {cleaned_file_path}. Execute o processo de limpeza primeiro.")
        return pd.DataFrame()
    except Exception as e:
        logger.error(f"Erro ao carregar dados limpos de '{cleaned_file_path}': {e}", exc_info=True)
        return pd.DataFrame()

--------------------------------------------------------------------------------
# Arquivo: src/database_manager.py
--------------------------------------------------------------------------------
# src/database_manager.py
import sqlite3
import pandas as pd
import logging
import os
from typing import List, Any, Tuple, Optional

# Importar Config para type hinting, mas a instância é geralmente passada ou importada como config_obj
# from .config import Config 

logger = logging.getLogger(__name__)

class DatabaseManager:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn: Optional[sqlite3.Connection] = None
        self.cursor: Optional[sqlite3.Cursor] = None
        try:
            db_dir = os.path.dirname(self.db_path)
            if db_dir and not os.path.exists(db_dir): # Cria o diretório se não existir
                os.makedirs(db_dir)
                logger.info(f"Diretório do banco de dados criado: {db_dir}")
            self.connect()
            logger.info(f"Database Manager inicializado e conectado a: {db_path}")
        except sqlite3.Error as e:
            logger.error(f"Erro ao inicializar DatabaseManager para {db_path}: {e}", exc_info=True)
            raise

    def connect(self) -> None:
        """Estabelece a conexão com o banco de dados SQLite."""
        try:
            self.conn = sqlite3.connect(self.db_path, timeout=10) # Timeout para evitar locks longos
            self.conn.execute("PRAGMA foreign_keys = ON;")
            self.conn.execute("PRAGMA journal_mode = WAL;") # Melhor para concorrência
            self.cursor = self.conn.cursor()
            logger.debug(f"Conexão com o banco de dados {self.db_path} estabelecida.")
        except sqlite3.Error as e:
            logger.error(f"Erro ao conectar ao banco de dados {self.db_path}: {e}", exc_info=True)
            raise

    def close(self) -> None:
        """Fecha a conexão com o banco de dados."""
        if self.cursor:
            try: self.cursor.close()
            except sqlite3.Error as e: logger.error(f"Erro ao fechar cursor: {e}", exc_info=True)
            self.cursor = None
        if self.conn:
            try: self.conn.close()
            except sqlite3.Error as e: logger.error(f"Erro ao fechar conexão: {e}", exc_info=True)
            logger.debug(f"Conexão com o banco de dados {self.db_path} fechada.")
        self.conn = None

    def _ensure_connection(self):
        """Garante que uma conexão e cursor estejam ativos, tentando reconectar se necessário."""
        if not self.conn or not self.cursor:
            logger.warning("Conexão/cursor não ativos. Tentando reconectar...")
            self.connect() # connect() levanta exceção em caso de falha
            if not self.conn or not self.cursor: # Checagem dupla
                 logger.error("Falha crítica ao restabelecer conexão/cursor.")
                 raise sqlite3.Error("Falha ao restabelecer conexão com o banco de dados.")

    def _execute_ddl_query(self, query: str, params: Tuple = None) -> None:
        """Método interno para executar queries DDL (CREATE, ALTER, DROP)."""
        self._ensure_connection()
        try:
            logger.debug(f"Executando DDL: {query[:150]}...") # Log truncado
            self.cursor.execute(query, params or ())
            self.conn.commit()
            logger.debug("DDL comitada.")
        except sqlite3.Error as e:
            logger.error(f"Erro DDL: {query[:150]}... - {e}", exc_info=True)
            if self.conn: 
                try: self.conn.rollback()
                except Exception as rb_ex: logger.error(f"Erro no rollback após falha de DDL: {rb_ex}")
            raise

    def execute_query(self, query: str, params: Tuple = None) -> pd.DataFrame:
        """Executa uma query SELECT e retorna os resultados como um DataFrame Pandas."""
        self._ensure_connection()
        try:
            logger.debug(f"Executando SELECT: {query} com params: {params}")
            df = pd.read_sql_query(query, self.conn, params=params or ())
            logger.debug(f"SELECT executada. {len(df) if df is not None else 'Nenhuma'} linha(s).")
            return df if df is not None else pd.DataFrame()
        except Exception as e:
            logger.error(f"Erro SELECT: {query} - {e}", exc_info=True)
            return pd.DataFrame()

    def save_dataframe(self, df: pd.DataFrame, table_name: str, if_exists: str = 'replace') -> None:
        """Salva um DataFrame Pandas em uma tabela SQLite."""
        self._ensure_connection()
        if df is None:
            logger.warning(f"DataFrame para '{table_name}' é None. Nada salvo.")
            return
        try:
            logger.info(f"Salvando DataFrame em '{table_name}' (if_exists='{if_exists}', Linhas: {len(df)})")
            df.to_sql(table_name, self.conn, if_exists=if_exists, index=False, chunksize=1000)
            logger.info(f"DataFrame salvo em '{table_name}'.")
        except Exception as e:
            logger.error(f"Erro ao salvar DataFrame em '{table_name}': {e}", exc_info=True)
            raise

    def load_dataframe(self, table_name: str, query: Optional[str] = None, params: Optional[Tuple] = None) -> pd.DataFrame:
        """Carrega dados de uma tabela (ou query customizada) para um DataFrame Pandas."""
        final_query = query
        if not final_query:
            if not self.table_exists(table_name):
                logger.warning(f"Tabela '{table_name}' não existe. Retornando DataFrame vazio.")
                return pd.DataFrame()
            final_query = f"SELECT * FROM {table_name}"
        
        df = self.execute_query(final_query, params=params)
        if df is None: return pd.DataFrame() # execute_query retorna DataFrame vazio em erro
            
        log_source = f"tabela '{table_name}'" if not query else "query customizada"
        logger.info(f"DataFrame de {log_source} carregado com {len(df)} linhas.")
        return df

    def table_exists(self, table_name: str) -> bool:
        """Verifica se uma tabela existe no banco de dados."""
        self._ensure_connection()
        try:
            self.cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name=?;", (table_name,))
            exists = self.cursor.fetchone() is not None
            logger.debug(f"Tabela '{table_name}' {'existe.' if exists else 'não existe.'}")
            return exists
        except sqlite3.Error as e:
            logger.error(f"Erro ao verificar se tabela '{table_name}' existe: {e}", exc_info=True)
            return False

    def get_table_name_from_config(self, attr_name: str, default_name: str) -> str:
        """Auxiliar para obter nome de tabela do config_obj ou usar default."""
        from .config import config_obj # Importa config_obj aqui para acesso
        return getattr(config_obj, attr_name, default_name)

    # --- Métodos de Criação de Tabelas ---
    # (Usando constantes do config_obj onde disponível)

    def _create_table_draws(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('MAIN_DRAWS_TABLE_NAME', 'draws')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.CONTEST_ID_COLUMN_NAME} INTEGER PRIMARY KEY, {config_obj.DATE_COLUMN_NAME} TEXT,
            {", ".join([f'{col} INTEGER' for col in config_obj.BALL_NUMBER_COLUMNS])},
            {config_obj.DRAWN_NUMBERS_COLUMN_NAME} TEXT);"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_draw_results_flat(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('FLAT_DRAWS_TABLE_NAME', 'draw_results_flat')
        main_draws_table = self.get_table_name_from_config('MAIN_DRAWS_TABLE_NAME', 'draws')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.CONTEST_ID_COLUMN_NAME} INTEGER NOT NULL, {config_obj.DEZENA_COLUMN_NAME} INTEGER NOT NULL,
            PRIMARY KEY ({config_obj.CONTEST_ID_COLUMN_NAME}, {config_obj.DEZENA_COLUMN_NAME}),
            FOREIGN KEY ({config_obj.CONTEST_ID_COLUMN_NAME}) REFERENCES {main_draws_table}({config_obj.CONTEST_ID_COLUMN_NAME}));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")
            
    def _create_table_analysis_delays(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('ANALYSIS_DELAYS_TABLE_NAME', 'analysis_delays')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.CONTEST_ID_COLUMN_NAME} INTEGER NOT NULL, {config_obj.DEZENA_COLUMN_NAME} INTEGER NOT NULL,
            {config_obj.CURRENT_DELAY_COLUMN_NAME} INTEGER, {config_obj.MAX_DELAY_OBSERVED_COLUMN_NAME} INTEGER, {config_obj.AVG_DELAY_COLUMN_NAME} REAL,
            PRIMARY KEY ({config_obj.CONTEST_ID_COLUMN_NAME}, {config_obj.DEZENA_COLUMN_NAME}));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_analysis_frequency_overall(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('ANALYSIS_FREQUENCY_OVERALL_TABLE_NAME', 'analysis_frequency_overall')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.CONTEST_ID_COLUMN_NAME} INTEGER NOT NULL, {config_obj.DEZENA_COLUMN_NAME} INTEGER NOT NULL,
            {config_obj.FREQUENCY_COLUMN_NAME} INTEGER, {config_obj.RELATIVE_FREQUENCY_COLUMN_NAME} REAL,
            PRIMARY KEY ({config_obj.CONTEST_ID_COLUMN_NAME}, {config_obj.DEZENA_COLUMN_NAME}));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_analysis_recurrence_cdf(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('ANALYSIS_RECURRENCE_CDF_TABLE_NAME', 'analysis_recurrence_cdf')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.CONTEST_ID_COLUMN_NAME} INTEGER NOT NULL, {config_obj.DEZENA_COLUMN_NAME} INTEGER NOT NULL,
            {config_obj.RECURRENCE_CDF_COLUMN_NAME} REAL,
            PRIMARY KEY ({config_obj.CONTEST_ID_COLUMN_NAME}, {config_obj.DEZENA_COLUMN_NAME}));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_analysis_rank_trend_metrics(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('ANALYSIS_RANK_TREND_METRICS_TABLE_NAME', 'analysis_rank_trend_metrics')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.CONTEST_ID_COLUMN_NAME} INTEGER NOT NULL, {config_obj.DEZENA_COLUMN_NAME} INTEGER NOT NULL,
            {config_obj.RANK_SLOPE_COLUMN_NAME} REAL, {config_obj.TREND_STATUS_COLUMN_NAME} TEXT,
            {config_obj.CHUNK_TYPE_COLUMN_NAME} TEXT, {config_obj.CHUNK_SIZE_COLUMN_NAME} INTEGER,
            PRIMARY KEY ({config_obj.CONTEST_ID_COLUMN_NAME}, {config_obj.DEZENA_COLUMN_NAME}, {config_obj.CHUNK_TYPE_COLUMN_NAME}, {config_obj.CHUNK_SIZE_COLUMN_NAME}));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_analysis_cycle_status_dezenas(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('ANALYSIS_CYCLE_STATUS_DEZENAS_TABLE_NAME', 'analysis_cycle_status_dezenas')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.CONTEST_ID_COLUMN_NAME} INTEGER NOT NULL, {config_obj.DEZENA_COLUMN_NAME} INTEGER NOT NULL,
            {config_obj.CICLO_NUM_COLUMN_NAME} INTEGER, {config_obj.IS_MISSING_IN_CURRENT_CYCLE_COLUMN_NAME} INTEGER,
            PRIMARY KEY ({config_obj.CONTEST_ID_COLUMN_NAME}, {config_obj.DEZENA_COLUMN_NAME}));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_analysis_cycle_closing_propensity(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('ANALYSIS_CYCLE_CLOSING_PROPENSITY_TABLE_NAME', 'analysis_cycle_closing_propensity')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.DEZENA_COLUMN_NAME} INTEGER PRIMARY KEY, {config_obj.CYCLE_CLOSING_SCORE_COLUMN_NAME} REAL);"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_frequent_itemsets(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('FREQUENT_ITEMSETS_TABLE_NAME', 'frequent_itemsets')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.CONTEST_ID_COLUMN_NAME} INTEGER NOT NULL, {config_obj.ITEMSET_STR_COLUMN_NAME} TEXT NOT NULL,
            {config_obj.SUPPORT_COLUMN_NAME} REAL NOT NULL, {config_obj.K_COLUMN_NAME} INTEGER NOT NULL,
            frequency_count INTEGER, PRIMARY KEY ({config_obj.CONTEST_ID_COLUMN_NAME}, {config_obj.ITEMSET_STR_COLUMN_NAME}));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_analysis_itemset_metrics(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('ANALYSIS_ITEMSET_METRICS_TABLE_NAME', 'analysis_itemset_metrics')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.CONTEST_ID_COLUMN_NAME} INTEGER NOT NULL, {config_obj.ITEMSET_STR_COLUMN_NAME} TEXT NOT NULL,
            {config_obj.K_COLUMN_NAME} INTEGER, {config_obj.SUPPORT_COLUMN_NAME} REAL,
            {config_obj.CONFIDENCE_COLUMN_NAME} REAL, {config_obj.LIFT_COLUMN_NAME} REAL,
            {config_obj.ITEMSET_CURRENT_DELAY_COLUMN_NAME} INTEGER, {config_obj.ITEMSET_AVG_DELAY_COLUMN_NAME} REAL,
            {config_obj.ITEMSET_MAX_DELAY_COLUMN_NAME} INTEGER, {config_obj.ITEMSET_SCORE_COLUMN_NAME} REAL,
            last_occurrence_contest_id INTEGER, occurrences_draw_ids TEXT,
            PRIMARY KEY ({config_obj.CONTEST_ID_COLUMN_NAME}, {config_obj.ITEMSET_STR_COLUMN_NAME}));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_analysis_cycles_detail(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('ANALYSIS_CYCLES_DETAIL_TABLE_NAME', 'analysis_cycles_detail')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.CICLO_NUM_COLUMN_NAME} INTEGER, concurso_inicio INTEGER, concurso_fim INTEGER, 
            duracao_concursos INTEGER, numeros_faltantes TEXT, qtd_faltantes INTEGER,
            PRIMARY KEY({config_obj.CICLO_NUM_COLUMN_NAME}));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_analysis_cycles_summary(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('ANALYSIS_CYCLES_SUMMARY_TABLE_NAME', 'analysis_cycles_summary')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            summary_id INTEGER PRIMARY KEY DEFAULT 1, total_ciclos_fechados INTEGER, duracao_media_ciclo REAL, 
            duracao_min_ciclo INTEGER, duracao_max_ciclo INTEGER, duracao_mediana_ciclo REAL);"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_analysis_cycle_progression_raw(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('ANALYSIS_CYCLE_PROGRESSION_RAW_TABLE_NAME', 'analysis_cycle_progression_raw')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.CONTEST_ID_COLUMN_NAME} INTEGER, {config_obj.DATE_COLUMN_NAME} TEXT, ciclo_num_associado INTEGER, 
            dezenas_sorteadas_neste_concurso TEXT, numeros_que_faltavam_antes_deste_concurso TEXT, 
            qtd_faltavam_antes_deste_concurso INTEGER, dezenas_apuradas_neste_concurso TEXT, 
            qtd_apuradas_neste_concurso INTEGER, numeros_faltantes_apos_este_concurso TEXT, 
            qtd_faltantes_apos_este_concurso INTEGER, ciclo_fechou_neste_concurso INTEGER,
            PRIMARY KEY ({config_obj.CONTEST_ID_COLUMN_NAME}, ciclo_num_associado));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_propriedades_numericas_por_concurso(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('PROPRIEDADES_NUMERICAS_POR_CONCURSO_TABLE_NAME', 'propriedades_numericas_por_concurso')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.CONTEST_ID_COLUMN_NAME} INTEGER PRIMARY KEY, 
            soma_dezenas INTEGER, pares INTEGER, impares INTEGER, primos INTEGER);"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_analise_repeticao_concurso_anterior(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('REPETICAO_CONCURSO_ANTERIOR_TABLE_NAME', 'analise_repeticao_concurso_anterior')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.CONTEST_ID_COLUMN_NAME} INTEGER PRIMARY KEY, 
            {config_obj.DATE_COLUMN_NAME} TEXT, QtdDezenasRepetidas INTEGER, DezenasRepetidas TEXT);"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_chunk_metrics(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('CHUNK_METRICS_TABLE_NAME', 'chunk_metrics')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.CHUNK_TYPE_COLUMN_NAME} TEXT, {config_obj.CHUNK_SIZE_COLUMN_NAME} INTEGER,
            chunk_start_draw_id INTEGER, chunk_end_draw_id INTEGER, {config_obj.DEZENA_COLUMN_NAME} INTEGER,
            frequency_in_chunk_abs INTEGER, frequency_in_chunk_rel REAL,
            current_delay_in_chunk INTEGER, max_delay_in_chunk INTEGER, avg_delay_in_chunk REAL,
            delay_std_dev REAL, occurrence_std_dev REAL,
            PRIMARY KEY ({config_obj.CHUNK_TYPE_COLUMN_NAME}, {config_obj.CHUNK_SIZE_COLUMN_NAME}, chunk_start_draw_id, {config_obj.DEZENA_COLUMN_NAME}));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_draw_position_frequency(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('DRAW_POSITION_FREQUENCY_TABLE_NAME', 'draw_position_frequency')
        cols_posicao = ", ".join([f"Posicao_{i} INTEGER DEFAULT 0" for i in range(1, 16)])
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} ({config_obj.DEZENA_COLUMN_NAME} INTEGER PRIMARY KEY, {cols_posicao});"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_geral_ma_frequency(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('GERAL_MA_FREQUENCY_TABLE_NAME', 'geral_ma_frequency')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.CONTEST_ID_COLUMN_NAME} INTEGER NOT NULL, {config_obj.DEZENA_COLUMN_NAME} INTEGER NOT NULL, Janela INTEGER NOT NULL,
            MA_Frequencia REAL, PRIMARY KEY ({config_obj.CONTEST_ID_COLUMN_NAME}, {config_obj.DEZENA_COLUMN_NAME}, Janela));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_geral_ma_delay(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('GERAL_MA_DELAY_TABLE_NAME', 'geral_ma_delay')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.CONTEST_ID_COLUMN_NAME} INTEGER NOT NULL, {config_obj.DEZENA_COLUMN_NAME} INTEGER NOT NULL, Janela INTEGER NOT NULL,
            MA_Atraso REAL, PRIMARY KEY ({config_obj.CONTEST_ID_COLUMN_NAME}, {config_obj.DEZENA_COLUMN_NAME}, Janela));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_association_rules(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('ASSOCIATION_RULES_TABLE_NAME', 'association_rules')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            antecedents_str TEXT NOT NULL, consequents_str TEXT NOT NULL,
            antecedent_support REAL, consequent_support REAL, support REAL,
            confidence REAL, lift REAL, leverage REAL, conviction REAL,
            PRIMARY KEY (antecedents_str, consequents_str));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_grid_line_distribution(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('GRID_LINE_DISTRIBUTION_TABLE_NAME', 'grid_line_distribution')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            Linha TEXT NOT NULL, Qtd_Dezenas_Sorteadas INTEGER NOT NULL,
            Frequencia_Absoluta INTEGER NOT NULL, Frequencia_Relativa REAL NOT NULL,
            PRIMARY KEY (Linha, Qtd_Dezenas_Sorteadas));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_grid_column_distribution(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('GRID_COLUMN_DISTRIBUTION_TABLE_NAME', 'grid_column_distribution')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            Coluna TEXT NOT NULL, Qtd_Dezenas_Sorteadas INTEGER NOT NULL,
            Frequencia_Absoluta INTEGER NOT NULL, Frequencia_Relativa REAL NOT NULL,
            PRIMARY KEY (Coluna, Qtd_Dezenas_Sorteadas));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")
    
    def _create_table_statistical_tests_results(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('STATISTICAL_TESTS_RESULTS_TABLE_NAME', 'statistical_tests_results')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            Test_ID INTEGER PRIMARY KEY AUTOINCREMENT, Test_Name TEXT NOT NULL,
            Timestamp DATETIME DEFAULT CURRENT_TIMESTAMP, Chi2_Statistic REAL,
            P_Value REAL, Degrees_Freedom INTEGER, Alpha_Level REAL DEFAULT 0.05,
            Conclusion TEXT, Parameters TEXT, Notes TEXT);"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_monthly_number_frequency(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('MONTHLY_NUMBER_FREQUENCY_TABLE_NAME', 'monthly_number_frequency')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            {config_obj.DEZENA_COLUMN_NAME} INTEGER NOT NULL, Mes INTEGER NOT NULL,
            Frequencia_Absoluta_Total_Mes INTEGER, Total_Sorteios_Considerados_Mes INTEGER,
            Frequencia_Relativa_Mes REAL, PRIMARY KEY ({config_obj.DEZENA_COLUMN_NAME}, Mes));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_monthly_draw_properties_summary(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('MONTHLY_DRAW_PROPERTIES_TABLE_NAME', 'monthly_draw_properties_summary')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            Mes INTEGER PRIMARY KEY, Total_Sorteios_Mes INTEGER, Soma_Media_Mensal REAL,
            Media_Pares_Mensal REAL, Media_Impares_Mensal REAL, Media_Primos_Mensal REAL);"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")
            
    def _create_table_sequence_metrics(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('SEQUENCE_METRICS_TABLE_NAME', 'sequence_metrics')
        query = f"""CREATE TABLE IF NOT EXISTS {table_name} (
            sequence_description TEXT, sequence_type TEXT, length INTEGER, step INTEGER,
            specific_sequence TEXT, frequency_count INTEGER, support REAL,
            PRIMARY KEY (sequence_type, length, step, specific_sequence));"""
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_ciclo_metric_frequency(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('CYCLE_METRIC_FREQUENCY_TABLE_NAME', 'ciclo_metric_frequency')
        query = f"CREATE TABLE IF NOT EXISTS {table_name} ({config_obj.CICLO_NUM_COLUMN_NAME} INTEGER, {config_obj.DEZENA_COLUMN_NAME} INTEGER, frequencia_no_ciclo INTEGER, PRIMARY KEY ({config_obj.CICLO_NUM_COLUMN_NAME}, {config_obj.DEZENA_COLUMN_NAME}));"
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")
    
    def _create_table_ciclo_metric_atraso_medio(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('CYCLE_METRIC_ATRASO_MEDIO_TABLE_NAME', 'ciclo_metric_atraso_medio')
        query = f"CREATE TABLE IF NOT EXISTS {table_name} ({config_obj.CICLO_NUM_COLUMN_NAME} INTEGER, {config_obj.DEZENA_COLUMN_NAME} INTEGER, atraso_medio_no_ciclo REAL, PRIMARY KEY ({config_obj.CICLO_NUM_COLUMN_NAME}, {config_obj.DEZENA_COLUMN_NAME}));"
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_ciclo_metric_atraso_maximo(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('CYCLE_METRIC_ATRASO_MAXIMO_TABLE_NAME', 'ciclo_metric_atraso_maximo')
        query = f"CREATE TABLE IF NOT EXISTS {table_name} ({config_obj.CICLO_NUM_COLUMN_NAME} INTEGER, {config_obj.DEZENA_COLUMN_NAME} INTEGER, atraso_maximo_no_ciclo INTEGER, PRIMARY KEY ({config_obj.CICLO_NUM_COLUMN_NAME}, {config_obj.DEZENA_COLUMN_NAME}));"
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_ciclo_metric_atraso_final(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('CYCLE_METRIC_ATRASO_FINAL_TABLE_NAME', 'ciclo_metric_atraso_final')
        query = f"CREATE TABLE IF NOT EXISTS {table_name} ({config_obj.CICLO_NUM_COLUMN_NAME} INTEGER, {config_obj.DEZENA_COLUMN_NAME} INTEGER, atraso_final_no_ciclo INTEGER, PRIMARY KEY ({config_obj.CICLO_NUM_COLUMN_NAME}, {config_obj.DEZENA_COLUMN_NAME}));"
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_ciclo_rank_frequency(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('CYCLE_RANK_FREQUENCY_TABLE_NAME', 'ciclo_rank_frequency')
        query = f"CREATE TABLE IF NOT EXISTS {table_name} ({config_obj.CICLO_NUM_COLUMN_NAME} INTEGER, {config_obj.DEZENA_COLUMN_NAME} INTEGER, frequencia_no_ciclo INTEGER, rank_freq_no_ciclo INTEGER, PRIMARY KEY ({config_obj.CICLO_NUM_COLUMN_NAME}, {config_obj.DEZENA_COLUMN_NAME}));"
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_table_ciclo_group_metrics(self) -> None:
        from .config import config_obj
        table_name = self.get_table_name_from_config('CYCLE_GROUP_METRICS_TABLE_NAME', 'ciclo_group_metrics')
        query = f"CREATE TABLE IF NOT EXISTS {table_name} ({config_obj.CICLO_NUM_COLUMN_NAME} INTEGER PRIMARY KEY, avg_pares_no_ciclo REAL, avg_impares_no_ciclo REAL, avg_primos_no_ciclo REAL);"
        if not self.table_exists(table_name): self._execute_ddl_query(query); logger.debug(f"Tabela '{table_name}' verificada/criada.")

    def _create_all_tables(self) -> None:
        """Verifica e cria todas as tabelas conhecidas se não existirem."""
        logger.info("Verificando e criando todas as tabelas do banco de dados se não existirem...")
        
        creation_methods = [
            self._create_table_draws, self._create_table_draw_results_flat,
            self._create_table_analysis_delays, self._create_table_analysis_frequency_overall,
            self._create_table_analysis_recurrence_cdf, self._create_table_analysis_rank_trend_metrics,
            self._create_table_analysis_cycle_status_dezenas, self._create_table_analysis_cycle_closing_propensity,
            self._create_table_frequent_itemsets, self._create_table_analysis_itemset_metrics,
            self._create_table_analysis_cycles_detail, self._create_table_analysis_cycles_summary,
            self._create_table_analysis_cycle_progression_raw,
            self._create_table_propriedades_numericas_por_concurso,
            self._create_table_analise_repeticao_concurso_anterior,
            self._create_table_chunk_metrics,
            self._create_table_draw_position_frequency,
            self._create_table_geral_ma_frequency, self._create_table_geral_ma_delay,
            self._create_table_association_rules, self._create_table_grid_line_distribution,
            self._create_table_grid_column_distribution, self._create_table_statistical_tests_results,
            self._create_table_monthly_number_frequency, self._create_table_monthly_draw_properties_summary,
            self._create_table_sequence_metrics,
            self._create_table_ciclo_metric_frequency, self._create_table_ciclo_metric_atraso_medio,
            self._create_table_ciclo_metric_atraso_maximo, self._create_table_ciclo_metric_atraso_final,
            self._create_table_ciclo_rank_frequency, self._create_table_ciclo_group_metrics,
        ]
        
        for method_func in creation_methods:
            try:
                method_func()
            except Exception as e_create:
                method_name = method_func.__name__ if hasattr(method_func, '__name__') else 'desconhecido'
                logger.error(f"Erro ao tentar executar método de criação de tabela '{method_name}': {e_create}", exc_info=True)
        logger.info("Verificação e criação de tabelas (definidas na lista) concluída.")

    def __enter__(self):
        if not self.conn: self.connect()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

# Bloco if __name__ == '__main__' para teste direto
if __name__ == '__main__':
    if not logging.getLogger().hasHandlers():
        logging.basicConfig(
            level=logging.DEBUG, 
            format='%(asctime)s - %(name)s - %(levelname)s - [%(module)s.%(funcName)s:%(lineno)d] - %(message)s',
            handlers=[logging.StreamHandler()]
        )
    
    db_test_path_in_memory = ":memory:" 
    logger.info(f"Usando banco de dados de teste em: {db_test_path_in_memory}")
    
    db_m = None
    try:
        # Importar config_obj aqui para o contexto de teste
        from src.config import config_obj
        if not config_obj:
            raise ImportError("config_obj não pôde ser importado ou é None para o teste do DatabaseManager")

        db_m = DatabaseManager(db_path=db_test_path_in_memory)
        db_m._create_all_tables() # Chama o método que executa todos os _create_table_*
        
        logger.info(f"Tabelas no banco de dados após _create_all_tables: {db_m.get_table_names()}")

        # Teste simples de save/load
        test_df_data = {'col1': [1, 2, 3], 'col2': ['x', 'y', 'z']}
        test_df = pd.DataFrame(test_df_data)
        db_m.save_dataframe(test_df, 'test_table_for_manager')
        loaded_df = db_m.load_dataframe('test_table_for_manager')
        if loaded_df is not None and not loaded_df.empty and loaded_df.equals(test_df):
            logger.info(f"Teste de save/load para 'test_table_for_manager' BEM-SUCEDIDO.")
        else:
            logger.error(f"Falha no teste de save/load para 'test_table_for_manager'. Carregado:\n{loaded_df}")

    except Exception as e_test:
        logger.error(f"Erro no script de teste do DatabaseManager: {e_test}", exc_info=True)
    finally:
        if db_m:
            db_m.close()

--------------------------------------------------------------------------------
# Arquivo: src/main.py
--------------------------------------------------------------------------------
# Lotofacil_Analysis/src/main.py
import argparse
import logging
import sys
import os
import pandas as pd
from typing import List, Dict, Any, Callable, Optional

# Importar Config e DatabaseManager PRIMEIRO
from src.config import config_obj, Config 
from src.database_manager import DatabaseManager

from src.data_loader import load_and_clean_data, load_cleaned_data # Funções do seu data_loader.py
from src.orchestrator import Orchestrator

# --- Importações das Funções de Etapa do Pipeline ---
# (Suas importações de execute_*.py permanecem aqui)
from src.pipeline_steps.execute_frequency import run_frequency_analysis
from src.pipeline_steps.execute_delay import run_delay_analysis
# ... (todas as suas outras importações de pipeline_steps) ...
from src.pipeline_steps.execute_max_delay import run_max_delay_analysis_step
from src.pipeline_steps.execute_positional_analysis import run_positional_analysis_step
from src.pipeline_steps.execute_recurrence_analysis import run_recurrence_analysis_step
from src.pipeline_steps.execute_grid_analysis import run_grid_analysis_step
from src.pipeline_steps.execute_statistical_tests import run_statistical_tests_step
from src.pipeline_steps.execute_seasonality_analysis import run_seasonality_analysis_step
from src.pipeline_steps.execute_frequent_itemsets import run_frequent_itemsets_analysis_step
from src.pipeline_steps.execute_pairs import run_pair_analysis_step
from src.pipeline_steps.execute_association_rules import run_association_rules_step
from src.pipeline_steps.execute_frequent_itemset_metrics import run_frequent_itemset_metrics_step
from src.pipeline_steps.execute_properties import run_number_properties_analysis
from src.pipeline_steps.execute_sequence_analysis import run_sequence_analysis_step
from src.pipeline_steps.execute_cycles import run_cycle_identification_step
from src.pipeline_steps.execute_cycle_stats import run_cycle_stats_step
from src.pipeline_steps.execute_cycle_progression import run_cycle_progression_analysis_step
from src.pipeline_steps.execute_cycle_closing_propensity import run_cycle_closing_propensity_analysis
from src.pipeline_steps.execute_detailed_cycle_metrics import run_detailed_cycle_metrics_step
from src.pipeline_steps.execute_repetition_analysis import run_repetition_analysis_step
from src.pipeline_steps.execute_temporal_trend_analysis import run_temporal_trend_analysis_step
from src.pipeline_steps.execute_chunk_evolution_analysis import run_chunk_evolution_analysis_step
from src.pipeline_steps.execute_block_aggregation import run_block_aggregation_step
from src.pipeline_steps.execute_rank_trend_analysis import run_rank_trend_analysis_step


# Configuração de Logging (como na sua versão mais recente)
if config_obj:
    logging.basicConfig(
        level=config_obj.LOG_LEVEL,
        format='%(asctime)s - %(name)s - %(levelname)s - [%(module)s.%(funcName)s:%(lineno)d] - %(message)s',
        handlers=[
            logging.FileHandler(config_obj.LOG_FILE, mode='w', encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ],
        force=True
    )
else:
    logging.basicConfig(
        level="INFO", 
        format='%(asctime)s - %(name)s - %(levelname)s - [%(module)s.%(funcName)s:%(lineno)d] - %(message)s',
        handlers=[logging.StreamHandler(sys.stdout)]
    )
    logging.critical("Falha ao carregar config_obj. Usando logging de fallback.")

logger = logging.getLogger(__name__)

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 200)
pd.set_option('display.max_colwidth', 80)


def main(cmd_args: argparse.Namespace):
    if not config_obj:
        logger.critical("Objeto de configuração não está disponível. Encerrando a aplicação.")
        return

    logger.info(f"Aplicação Lotofacil Analysis iniciada com argumentos: {cmd_args}")
    
    all_data_df: Optional[pd.DataFrame] = None

    try:
        # Caminhos para os arquivos de dados, usando config_obj
        raw_file_full_path = config_obj.HISTORICO_CSV_PATH
        cleaned_pickle_full_path = config_obj.CLEANED_DATA_PATH

        if not cmd_args.force_reload:
            logger.info(f"Tentando carregar dados limpos de: {cleaned_pickle_full_path}")
            # A função load_cleaned_data no seu data_loader.py espera data_dir_path
            all_data_df = load_cleaned_data(config_obj.DATA_DIR) 
        
        if all_data_df is None or all_data_df.empty or cmd_args.force_reload:
            action_msg = "--force-reload especificado." if cmd_args.force_reload else "Dados limpos não encontrados ou vazios."
            logger.info(f"{action_msg} Processando dados brutos de: {raw_file_full_path}")
            
            # ***** CORREÇÃO DA CHAMADA load_and_clean_data *****
            # A função espera: load_and_clean_data(raw_file_path: str, cleaned_file_path_to_save: str)
            all_data_df = load_and_clean_data(
                raw_file_path=raw_file_full_path, 
                cleaned_file_path_to_save=cleaned_pickle_full_path
            )
            # O terceiro argumento 'config=config_obj' foi removido pois não é esperado pela função.

        if all_data_df is None or all_data_df.empty:
            logger.error("Nenhum dado carregado. Verifique os arquivos de dados e as configurações. Encerrando.")
            return
        
        logger.info(f"{len(all_data_df)} sorteios carregados para análise.")
        
        # --- Definição do Pipeline de Análise ---
        # (A definição do PIPELINE_CONFIG que você tem aqui permanece a mesma por enquanto)
        # Certifique-se que as assinaturas das funções run_*_step correspondam aos "args"
        default_step_args = ["all_data_df", "db_manager", "config", "shared_context"]
        db_config_shared_args = ["db_manager", "config", "shared_context"]

        main_analysis_pipeline_config: List[Dict[str, Any]] = [
            {"name": "frequency_analysis", "func": run_frequency_analysis, "args": default_step_args},
            {"name": "delay_analysis", "func": run_delay_analysis, "args": default_step_args},
            # {"name": "max_delay_analysis", "func": run_max_delay_analysis_step, "args": default_step_args},
            {"name": "positional_analysis", "func": run_positional_analysis_step, "args": default_step_args},
            {"name": "recurrence_analysis", "func": run_recurrence_analysis_step, "args": default_step_args},
            {"name": "grid_analysis", "func": run_grid_analysis_step, "args": default_step_args},
            {"name": "statistical_tests", "func": run_statistical_tests_step, "args": default_step_args},
            {"name": "seasonality_analysis", "func": run_seasonality_analysis_step, "args": default_step_args},
            
            {"name": "frequent_itemsets_analysis", "func": run_frequent_itemsets_analysis_step, 
             "args": default_step_args, 
             "output_key": "combination_analyzer_instance"},
            
            {"name": "pair_analysis", "func": run_pair_analysis_step, 
             "args": default_step_args + ["combination_analyzer_instance"]},
            
            {"name": "association_rules", "func": run_association_rules_step, 
             "args": db_config_shared_args + ["combination_analyzer_instance"]},

            {"name": "frequent_itemset_metrics_analysis", "func": run_frequent_itemset_metrics_step, "args": default_step_args},
            {"name": "number_properties", "func": run_number_properties_analysis, "args": default_step_args},
            {"name": "sequence_analysis", "func": run_sequence_analysis_step, "args": default_step_args},
            
            {"name": "cycle_identification", "func": run_cycle_identification_step, 
             "args": default_step_args, "output_key": "cycles_detail_df"}, 
            
            {"name": "cycle_stats", "func": run_cycle_stats_step, 
             "args": ["all_data_df", "db_manager", "config", "shared_context"]},
            
            {"name": "cycle_progression", "func": run_cycle_progression_analysis_step, "args": default_step_args},
            
            {"name": "cycle_closing_propensity", "func": run_cycle_closing_propensity_analysis, 
             "args": db_config_shared_args + ["cycles_detail_df"]}, 

            {"name": "detailed_cycle_metrics", "func": run_detailed_cycle_metrics_step, 
             "args": default_step_args + ["cycles_detail_df"]},

            {"name": "repetition_analysis", "func": run_repetition_analysis_step, "args": default_step_args},
            {"name": "temporal_trend_analysis", "func": run_temporal_trend_analysis_step, "args": default_step_args},
            {"name": "chunk_evolution_analysis", "func": run_chunk_evolution_analysis_step, "args": default_step_args},
            {"name": "block_aggregation", "func": run_block_aggregation_step, "args": db_config_shared_args}, 
            {"name": "rank_trend_analysis", "func": run_rank_trend_analysis_step, "args": db_config_shared_args + ["all_data_df"]},
        ]
        
        pipeline_to_run_actual: List[Dict[str, Any]] = []
        if cmd_args.run_steps:
            if "all_analysis" in cmd_args.run_steps:
                pipeline_to_run_actual = main_analysis_pipeline_config
            else:
                for step_name in cmd_args.run_steps:
                    found_step = next((s for s in main_analysis_pipeline_config if s["name"] == step_name), None)
                    if found_step:
                        if found_step not in pipeline_to_run_actual:
                           pipeline_to_run_actual.append(found_step)
                    else:
                        logger.warning(f"Etapa solicitada '{step_name}' não encontrada na configuração principal. Será ignorada.")
        elif cmd_args.force_reload: 
            logger.info("Opção --force-reload ativada. Executando todas as etapas de análise ('all_analysis').")
            pipeline_to_run_actual = main_analysis_pipeline_config
        
        if not pipeline_to_run_actual and not cmd_args.run_strategy_flow :
             logger.info("Nenhuma etapa de pipeline solicitada para execução. Use --run-steps all_analysis ou --force-reload.")
        elif pipeline_to_run_actual:
            with DatabaseManager(db_path=config_obj.DB_PATH) as db_m:
                logger.info(f"Executando pipeline com etapas: {[s['name'] for s in pipeline_to_run_actual]}")
                orchestrator = Orchestrator(pipeline=pipeline_to_run_actual, db_manager=db_m)

                orchestrator.set_shared_context('all_data_df', all_data_df)
                orchestrator.set_shared_context('config', config_obj)
                orchestrator.set_shared_context('shared_context', orchestrator.shared_context) 

                logger.info("Verificando e criando estrutura do banco de dados...")
                db_m._create_all_tables() 

                logger.info(f"Iniciando o Orchestrator. Contexto inicial: {list(orchestrator.shared_context.keys())}")
                orchestrator.run()
        
    except FileNotFoundError as fnf_error:
        logger.critical(f"Erro de arquivo não encontrado: {fnf_error}.", exc_info=True)
    except KeyError as key_error:
        logger.critical(f"Erro de chave não encontrada (KeyError): {key_error}", exc_info=True)
    except Exception as e:
        logger.critical(f"Erro fatal na execução principal: {e}", exc_info=True)
    finally:
        logger.info("Aplicação principal finalizada.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Aplicativo de Análise da Lotofácil.")
    parser.add_argument("--force-reload", action="store_true", help="Força o recarregamento dos dados do arquivo CSV bruto.")
    parser.add_argument("--run-steps", nargs='*', help="Execute etapas específicas (ou 'all_analysis'). Ex: --run-steps frequency_analysis delay_analysis")
    parser.add_argument("--run-strategy-flow", action="store_true", help="Executa o fluxo de agregação e teste de estratégias.")
    
    parsed_args = parser.parse_args()
    
    if parsed_args.run_steps is not None and not parsed_args.run_steps: 
        logger.warning("--run-steps foi especificado sem nenhuma etapa. Nenhuma etapa de análise será executada. Forneça nomes de etapas ou 'all_analysis'.")

    main(cmd_args=parsed_args)

--------------------------------------------------------------------------------
# Arquivo: src/orchestrator.py
--------------------------------------------------------------------------------
# src/orchestrator.py
import logging
from typing import List, Dict, Callable, Any, Optional
import time
import pandas as pd # Adicionado para type check em resultado

# DatabaseManager não precisa ser importado aqui se a instância é injetada
# from src.database_manager import DatabaseManager 

logger = logging.getLogger(__name__)

class Orchestrator:
    def __init__(self, pipeline: List[Dict[str, Any]], db_manager: Any): # db_manager pode ser Any ou DatabaseManager
        self.pipeline = pipeline
        # Contexto compartilhado inicializado com dependências essenciais
        self.shared_context: Dict[str, Any] = {"db_manager": db_manager}
        # O config_obj e all_data_df serão adicionados via set_shared_context pelo main.py
        logger.info(f"Orchestrator inicializado com {len(pipeline)} etapas no pipeline.")

    def set_shared_context(self, key: str, value: Any):
        """Adiciona ou atualiza um item no contexto compartilhado."""
        self.shared_context[key] = value
        # Para DataFrames grandes, logar apenas o shape ou tipo
        if isinstance(value, pd.DataFrame):
            logger.debug(f"Contexto compartilhado: Chave='{key}', Tipo=DataFrame, Shape={value.shape if value is not None else 'None'}")
        else:
            logger.debug(f"Contexto compartilhado: Chave='{key}', Tipo='{type(value).__name__}'")

    def get_shared_context_value(self, key: str, default: Any = None) -> Any:
        """Recupera um valor do contexto compartilhado, com um default opcional."""
        return self.shared_context.get(key, default)

    def _prepare_step_arguments(self, step_config: Dict[str, Any]) -> Dict[str, Any]:
        """Prepara os argumentos para uma etapa, buscando do shared_context."""
        prepared_args: Dict[str, Any] = {}
        step_name = step_config.get("name", "Desconhecida")
        
        arg_keys_from_config = step_config.get("args", [])
        for arg_key in arg_keys_from_config:
            if arg_key not in self.shared_context:
                error_msg = (
                    f"Erro na configuração da etapa '{step_name}': "
                    f"Argumento/chave de contexto '{arg_key}' não encontrado no contexto compartilhado. "
                    f"Chaves disponíveis: {list(self.shared_context.keys())}"
                )
                logger.error(error_msg)
                raise KeyError(error_msg) # Levanta erro para parar se um arg essencial estiver faltando
            prepared_args[arg_key] = self.shared_context[arg_key]
            logger.debug(f"Para etapa '{step_name}', argumento '{arg_key}' obtido do contexto.")

        # Adiciona kwargs explícitos da configuração da etapa
        explicit_kwargs = step_config.get("kwargs", {})
        if explicit_kwargs:
            prepared_args.update(explicit_kwargs)
            logger.debug(f"Para etapa '{step_name}', kwargs explícitos adicionados: {explicit_kwargs}")
            
        return prepared_args

    def run_step(self, step_config: Dict[str, Any]) -> bool:
        """Executa uma única etapa do pipeline."""
        step_name = step_config.get("name", "Etapa Desconhecida")
        step_func: Optional[Callable] = step_config.get("func")
        step_succeeded = False # Assume falha até prova em contrário

        if not step_func or not callable(step_func):
            logger.error(f"Configuração inválida para '{step_name}': 'func' ausente ou não é uma função. Pulando.")
            return False

        logger.info(f"--- Iniciando etapa: {step_name} ---")
        start_time = time.time()
        
        try:
            step_kwargs = self._prepare_step_arguments(step_config)
            # Logar apenas chaves para não poluir com valores grandes de DataFrames
            logger.debug(f"Argumentos preparados para '{step_name}': {list(step_kwargs.keys())}")
            
            result = step_func(**step_kwargs) # Desempacota os argumentos
            
            output_key = step_config.get("output_key")
            if output_key:
                self.set_shared_context(output_key, result) # set_shared_context já lida com logging
            
            if isinstance(result, bool):
                step_succeeded = result
                if not result:
                     logger.warning(f"Etapa '{step_name}' concluída, mas retornou False (indicando falha ou condição não atendida).")
                else:
                    logger.info(f"Etapa '{step_name}' concluída com sucesso (retornou True).")
            else: # Se não retorna bool, assume sucesso se não houver exceção
                step_succeeded = True
                logger.info(f"Etapa '{step_name}' concluída (sem retorno booleano explícito, sucesso assumido).")

        except KeyError as e: # Erro já logado em _prepare_step_arguments
            logger.error(f"Falha ao executar '{step_name}': Argumento essencial não encontrado no contexto ({e}). Etapa pulada.")
            # step_succeeded já é False
        except Exception as e:
            logger.error(f"Erro inesperado ao executar a etapa '{step_name}': {e}", exc_info=True)
            # step_succeeded já é False
        finally:
            end_time = time.time()
            logger.info(f"--- Etapa '{step_name}' finalizada. Duração: {end_time - start_time:.2f} segundos. Sucesso: {step_succeeded} ---")
        return step_succeeded

    def run(self) -> None:
        """Executa todas as etapas definidas no pipeline."""
        logger.info(f"Iniciando execução do pipeline com {len(self.pipeline)} etapas.")
        total_start_time = time.time()

        if not self.pipeline:
            logger.warning("Pipeline está vazio. Nenhuma etapa para executar.")
            return

        for i, step_config in enumerate(self.pipeline):
            step_number = i + 1
            step_name = step_config.get("name", f"Etapa Anônima {step_number}")
            logger.info(f"Processando Etapa {step_number}/{len(self.pipeline)}: {step_name}")
            
            success = self.run_step(step_config)
            
            # Opção: parar o pipeline se uma etapa crítica falhar
            # if not success and step_config.get('critical', False): # Adicionar 'critical': True na config da etapa
            #     logger.error(f"Etapa crítica '{step_name}' falhou. Interrompendo o pipeline.")
            #     break 
            
            logger.info("-" * 50) 

        total_end_time = time.time()
        logger.info(f"Execução completa do pipeline finalizada. Duração total: {total_end_time - total_start_time:.2f} segundos.")

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/__init__.py
--------------------------------------------------------------------------------
# src/pipeline_steps/__init__.py
from .execute_frequency import run_frequency_analysis
from .execute_delay import run_delay_analysis
from .execute_max_delay import run_max_delay_analysis_step
from .execute_pairs import run_pair_analysis_step # Ou o nome correto do seu step de pares
from .execute_frequent_itemsets import run_frequent_itemsets_analysis_step
from .execute_cycles import run_cycle_identification_step
from .execute_cycle_stats import run_cycle_stats_step
from .execute_cycle_progression import run_cycle_progression_analysis_step
from .execute_detailed_cycle_metrics import run_detailed_cycle_metrics_step
from .execute_properties import run_number_properties_analysis
from .execute_repetition_analysis import run_repetition_analysis_step
from .execute_chunk_evolution_analysis import run_chunk_evolution_analysis_step
from .execute_block_aggregation import run_block_aggregation_step
from .execute_rank_trend_analysis import run_rank_trend_analysis_step # Adicionado
from .execute_metrics_viz import run_metrics_visualization_step
from .execute_chunk_evolution_visualization import run_chunk_evolution_visualization_step

__all__ = [
    "run_frequency_analysis",
    "run_delay_analysis",
    "run_max_delay_analysis_step",
    "run_pair_analysis_step",
    "run_frequent_itemsets_analysis_step",
    "run_cycle_identification_step",
    "run_cycle_stats_step",
    "run_cycle_progression_analysis_step",
    "run_detailed_cycle_metrics_step",
    "run_number_properties_analysis",
    "run_repetition_analysis_step",
    "run_chunk_evolution_analysis_step",
    "run_block_aggregation_step",
    "run_rank_trend_analysis_step", # Adicionado
    "run_metrics_visualization_step",
    "run_chunk_evolution_visualization_step"
]

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_association_rules.py
--------------------------------------------------------------------------------
# Lotofacil_Analysis/src/pipeline_steps/execute_association_rules.py
import logging
import pandas as pd
from typing import Any, Dict

# Para type hints mais específicos:
from src.analysis.combination_analysis import CombinationAnalyzer
# from src.database_manager import DatabaseManager
# from src.config import Config

logger = logging.getLogger(__name__)

def run_association_rules_step(
    db_manager: Any, # DatabaseManager
    config: Any, # Config
    shared_context: Dict[str, Any],
    combination_analyzer_instance: CombinationAnalyzer, # CORRIGIDO
    **kwargs
) -> bool:
    step_name = "Geração de Regras de Associação"
    logger.info(f"==== Iniciando Etapa: {step_name} ====")

    required_attrs = [
        'ASSOCIATION_RULES_MIN_CONFIDENCE', 'ASSOCIATION_RULES_MIN_LIFT',
        'ASSOCIATION_RULES_TABLE_NAME'
    ]
    for attr in required_attrs:
        if not hasattr(config, attr):
            logger.error(f"{step_name}: Atributo de config '{attr}' não encontrado.")
            return False

    if combination_analyzer_instance is None:
        logger.error(f"{step_name}: Instância 'combination_analyzer_instance' é None.")
        return False

    if not hasattr(combination_analyzer_instance, 'generate_association_rules'):
        logger.error(f"{step_name}: 'combination_analyzer_instance' não possui 'generate_association_rules'.")
        return False

    if not hasattr(db_manager, 'save_dataframe'):
        logger.error(f"{step_name}: Objeto 'db_manager' não possui o método 'save_dataframe'.")
        return False

    mlxtend_frequent_itemsets_df = shared_context.get('mlxtend_frequent_itemsets_df_for_rules')
    
    empty_rules_cols = ['antecedents_str', 'consequents_str', 'antecedent support', 
                        'consequent support', 'support', 'confidence', 'lift', 
                        'leverage', 'conviction']

    if not isinstance(mlxtend_frequent_itemsets_df, pd.DataFrame) or mlxtend_frequent_itemsets_df.empty:
        logger.warning(f"{step_name}: 'mlxtend_frequent_itemsets_df_for_rules' não encontrado/vazio. Nenhuma regra gerada.")
        db_manager.save_dataframe(pd.DataFrame(columns=empty_rules_cols), config.ASSOCIATION_RULES_TABLE_NAME, if_exists='replace')
        shared_context['association_rules_df'] = pd.DataFrame(columns=empty_rules_cols)
        return True

    min_confidence = config.ASSOCIATION_RULES_MIN_CONFIDENCE
    min_lift = config.ASSOCIATION_RULES_MIN_LIFT
    association_rules_table_name = config.ASSOCIATION_RULES_TABLE_NAME

    try:
        logger.info(f"Gerando regras com min_confidence={min_confidence}, min_lift={min_lift}...")

        rules_df = combination_analyzer_instance.generate_association_rules(
            frequent_itemsets_mlxtend_df=mlxtend_frequent_itemsets_df,
            metric="confidence",
            min_threshold=min_confidence,
            min_lift=min_lift
        )

        if not isinstance(rules_df, pd.DataFrame):
            logger.error(f"{step_name}: Geração de regras não retornou um DataFrame.")
            db_manager.save_dataframe(pd.DataFrame(columns=empty_rules_cols), association_rules_table_name, if_exists='replace')
            shared_context['association_rules_df'] = pd.DataFrame(columns=empty_rules_cols)
            return False

        if rules_df.empty:
            logger.info(f"{step_name}: Nenhuma regra gerada. Tabela '{association_rules_table_name}' vazia.")
            db_manager.save_dataframe(pd.DataFrame(columns=empty_rules_cols), association_rules_table_name, if_exists='replace')
        else:
            final_rules_df = rules_df[[col for col in empty_rules_cols if col in rules_df.columns]].copy()
            db_manager.save_dataframe(final_rules_df,
                                      association_rules_table_name,
                                      if_exists='replace')
            logger.info(f"{len(final_rules_df)} regras de associação salvas em: {association_rules_table_name}")
        
        shared_context['association_rules_df'] = rules_df if not rules_df.empty else pd.DataFrame(columns=empty_rules_cols)
        logger.info(f"Resultado de {step_name} adicionado ao shared_context.")

    except Exception as e:
        logger.error(f"Erro durante a execução da {step_name}: {e}", exc_info=True)
        db_manager.save_dataframe(pd.DataFrame(columns=empty_rules_cols), association_rules_table_name, if_exists='replace')
        shared_context['association_rules_df'] = pd.DataFrame(columns=empty_rules_cols)
        return False

    logger.info(f"==== Etapa: {step_name} CONCLUÍDA ====")
    return True

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_block_aggregation.py
--------------------------------------------------------------------------------
# Lotofacil_Analysis/src/pipeline_steps/execute_block_aggregation.py
import logging
from typing import Any, Dict

from src.analysis.block_aggregator import (
    aggregate_block_data_to_wide_format,
    aggregate_cycle_data_to_wide_format
)
# Para type hints, se desejar ser mais específico com os parâmetros:
# from src.config import Config 
# from src.database_manager import DatabaseManager

logger = logging.getLogger(__name__)

def run_block_aggregation_step(
    db_manager: Any, # DatabaseManager
    config: Any, # CORRIGIDO de config_param para config
    shared_context: Dict[str, Any],
    **kwargs
) -> bool:
    block_aggregation_successful = True
    cycle_aggregation_successful = True
    step_name = "Block and Cycle Data Aggregation"
    logger.info(f"==== Iniciando Etapa: {step_name} ====")

    # Validação do objeto config injetado
    required_attrs = [
        'CHUNK_TYPES_CONFIG', 'BLOCK_ANALISES_CONSOLIDADAS_PREFIX',
        'CYCLE_ANALISES_CONSOLIDADAS_TABLE_NAME',
        'EVOL_METRIC_FREQUENCY_BLOCK_PREFIX', # Usado por aggregate_block_data_to_wide_format
        'EVOL_RANK_FREQUENCY_BLOCK_PREFIX',   # Exemplo de outro prefixo
        'EVOL_BLOCK_GROUP_METRICS_PREFIX',    # Usado por aggregate_block_data_to_wide_format
        'ANALYSIS_CYCLES_DETAIL_TABLE_NAME',    # Usado por aggregate_cycle_data_to_wide_format
        'CYCLE_METRIC_FREQUENCY_TABLE_NAME',    # Usado por aggregate_cycle_data_to_wide_format
        'CYCLE_GROUP_METRICS_TABLE_NAME',       # Usado por aggregate_cycle_data_to_wide_format
        'ALL_NUMBERS'
    ]
    # Adicionar todas as constantes de prefixo de tabela usadas em per_dezena_metric_configs
    # em block_aggregator.py à lista required_attrs aqui também, para uma verificação completa.
    # Ex: 'EVOL_METRIC_ATRASO_MEDIO_BLOCK_PREFIX', etc.
    # Por simplicidade, esta lista está um pouco mais curta, mas idealmente seria exaustiva.

    missing_attrs = [attr for attr in required_attrs if not hasattr(config, attr)]
    if missing_attrs:
        logger.error(f"{step_name}: Objeto 'config' injetado não possui os atributos esperados: {missing_attrs}. Abortando.")
        return False

    try:
        logger.info("Sub-etapa: Agregação de Dados de Bloco para Formato Largo.")
        aggregate_block_data_to_wide_format(db_manager, config) # Passa o config injetado
        logger.info("Sub-etapa: Agregação de Dados de Bloco para Formato Largo concluída.")
    except Exception as e:
        logger.error(f"Erro crítico durante a sub-etapa de agregação de dados de bloco: {e}", exc_info=True)
        block_aggregation_successful = False

    try:
        logger.info("Sub-etapa: Agregação de Dados de Ciclo para Formato Largo.")
        aggregate_cycle_data_to_wide_format(db_manager, config) # Passa o config injetado
        logger.info("Sub-etapa: Agregação de Dados de Ciclo para Formato Largo concluída.")
    except Exception as e:
        logger.error(f"Erro crítico durante a sub-etapa de agregação de dados de ciclo: {e}", exc_info=True)
        cycle_aggregation_successful = False

    final_success = block_aggregation_successful and cycle_aggregation_successful
    if final_success:
        logger.info(f"Etapa {step_name} concluída com sucesso.")
    else:
        logger.warning(f"Etapa {step_name} concluída com uma ou mais falhas (bloco: {block_aggregation_successful}, ciclo: {cycle_aggregation_successful}).")

    return final_success

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_chunk_evolution_analysis.py
--------------------------------------------------------------------------------
# Lotofacil_Analysis/src/pipeline_steps/execute_chunk_evolution_analysis.py
import logging
import pandas as pd
from typing import Any, Dict

from src.analysis.chunk_analysis import calculate_chunk_metrics_and_persist
# Para type hints, se desejar ser mais específico com os parâmetros:
# from src.config import Config 
# from src.database_manager import DatabaseManager

logger = logging.getLogger(__name__)

def run_chunk_evolution_analysis_step(
    all_data_df: pd.DataFrame,
    db_manager: Any, # DatabaseManager
    config: Any, # CORRIGIDO de config_param para config
    shared_context: Dict[str, Any],
    **kwargs
) -> bool:
    step_name = "Chunk Evolution Metrics Calculation"
    logger.info(f"==== Iniciando Etapa: {step_name} ====")

    try:
        if all_data_df.empty:
            logger.warning(f"DataFrame de dados (all_data_df) está vazio para a etapa {step_name}. Pulando.")
            return True

        # Validação do objeto config injetado
        required_attrs = [
            'CHUNK_TYPES_CONFIG', 'ALL_NUMBERS',
            'EVOL_METRIC_FREQUENCY_BLOCK_PREFIX', # Assegura que os prefixos de tabela estejam no config
            'CONTEST_ID_COLUMN_NAME', 'BALL_NUMBER_COLUMNS',
            'EVOL_BLOCK_GROUP_METRICS_PREFIX'
        ]
        missing_attrs = [attr for attr in required_attrs if not hasattr(config, attr)]
        if missing_attrs:
            logger.error(f"{step_name}: Objeto 'config' injetado não possui os atributos esperados: {missing_attrs}. Abortando.")
            return False

        # Passa o objeto 'config' injetado e validado
        calculate_chunk_metrics_and_persist(
            all_data_df=all_data_df,
            db_manager=db_manager,
            config=config # Usar o config injetado
        )
        logger.info(f"Etapa do pipeline: {step_name} concluída com sucesso.")
        return True
    except AttributeError as ae:
        logger.error(f"Erro de atributo durante a {step_name}. Verifique o objeto de configuração: {ae}", exc_info=True)
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False
    except Exception as e:
        logger.error(f"Erro na etapa {step_name}: {e}", exc_info=True)
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_chunk_evolution_visualization.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_chunk_evolution_visualization.py
import logging
from pathlib import Path
from typing import Optional, Any, Dict, List # Adicionado List para DEFAULT_DEZENAS

# from src.database_manager import DatabaseManager # Para type hint
# from src.config import Config # Para type hint
from src.visualization.plotter import plot_chunk_metric_evolution
# As constantes de config são usadas diretamente pela função de plotagem ou aqui
# Se plot_chunk_metric_evolution as importa diretamente, ótimo.
# Senão, este step precisa pegá-las do config_obj e passá-las.
# Pelo código que você forneceu, este step importa as CONSTANTES do config.py
from src.config import (
    DEFAULT_CHUNK_TYPE_FOR_PLOTTING,
    DEFAULT_CHUNK_SIZE_FOR_PLOTTING,
    DEFAULT_DEZENAS_FOR_CHUNK_EVOLUTION_PLOT,
    # PLOT_DIR_CONFIG # Usaremos config.PLOT_DIR passado via shared_context
)

logger = logging.getLogger(__name__)

# RENOMEADO PARA CORRESPONDER AO __init__.py e main.py
def run_chunk_evolution_visualization_step(
    db_manager: Any, # Tipo real: DatabaseManager
    config: Any, # Tipo real: Config
    shared_context: Dict[str, Any], # Para consistência
    **kwargs
) -> bool:
    """
    Executa a visualização da evolução de métricas de chunks para configurações padrão.
    Os gráficos são salvos no diretório especificado por config.PLOT_DIR.

    Args:
        db_manager: Instância do DatabaseManager.
        config: Objeto de configuração (contém PLOT_DIR e defaults de plotagem).
        shared_context: Dicionário de contexto compartilhado (não usado diretamente aqui).
        **kwargs: Argumentos adicionais do pipeline.

    Returns:
        True se a visualização foi gerada com sucesso, False caso contrário.
    """
    step_name = "Chunk Evolution Visualization"
    logger.info(f"Iniciando etapa do pipeline: {step_name}.")
    
    try:
        output_dir_to_use = Path(config.PLOT_DIR) # Usando config.PLOT_DIR da instância config_obj
        output_dir_to_use.mkdir(parents=True, exist_ok=True)
        logger.info(f"Gráficos de evolução de chunk serão salvos em: {output_dir_to_use}")

        # Obter parâmetros de plotagem do config_obj, que por sua vez os obtém
        # das constantes de nível de módulo (que podem vir de .env)
        chunk_type_to_plot: str = config.DEFAULT_CHUNK_TYPE_FOR_PLOTTING
        chunk_size_to_plot: int = config.DEFAULT_CHUNK_SIZE_FOR_PLOTTING
        dezenas_to_plot_list: List[int] = config.DEFAULT_DEZENAS_FOR_CHUNK_EVOLUTION_PLOT
        
        # Exemplo: plotar a evolução da frequência absoluta
        # Você pode querer iterar por várias métricas ou torná-las configuráveis
        metric_to_plot = "Frequencia Absoluta" 

        logger.info(f"Gerando gráfico para: Tipo='{chunk_type_to_plot}', "
                    f"Tamanho={chunk_size_to_plot}, Métrica='{metric_to_plot}', "
                    f"Dezenas={dezenas_to_plot_list}")
        
        plot_chunk_metric_evolution(
            db_manager=db_manager,
            chunk_type=chunk_type_to_plot,
            chunk_size=chunk_size_to_plot,
            metric_to_plot=metric_to_plot, # Ou itere/configure outras métricas
            dezenas_to_plot=dezenas_to_plot_list,
            output_dir=str(output_dir_to_use)
        )
        
        # Você poderia adicionar mais chamadas a plot_chunk_metric_evolution para outras métricas aqui
        # Ex: metric_to_plot = "Atraso Medio no Bloco"
        # plot_chunk_metric_evolution(...)
        
        logger.info(f"Etapa do pipeline: {step_name} concluída com sucesso.")
        return True
    except AttributeError as e:
        logger.error(f"Erro na etapa {step_name}: Atributo de configuração ausente. Verifique seu config.py. Detalhes: {e}", exc_info=True)
        return False
    except Exception as e:
        logger.error(f"Erro ao executar a etapa {step_name}: {e}", exc_info=True)
        return False

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_combinations.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_combinations.py
import pandas as pd
import logging # Adicionado
from src.analysis.combination_analysis import calculate_pair_frequencies # Supondo esta função para pares
from src.database_manager import DatabaseManager
# from src.config import logger # Removido

logger = logging.getLogger(__name__) # Corrigido

def run_pair_combination_analysis(all_data_df: pd.DataFrame, db_manager: DatabaseManager, **kwargs) -> bool:
    """
    Executa a análise de combinações de pares.
    """
    try:
        logger.info("Iniciando análise de combinações de pares.")
        
        pair_freq_df = calculate_pair_frequencies(all_data_df)
        if pair_freq_df is not None and not pair_freq_df.empty:
            db_manager.save_dataframe_to_db(pair_freq_df, 'frequencia_pares')
            logger.info("Frequência de pares salva no banco de dados.")
        else:
            logger.warning("Não foi possível calcular ou DataFrame de frequência de pares vazio.")
            
        # Se combination_analysis.py também lida com trios, etc., adicione chamadas aqui
        # Ex:
        # trio_freq_df = calculate_trio_frequencies(all_data_df)
        # db_manager.save_dataframe_to_db(trio_freq_df, 'frequencia_trios')

        logger.info("Análise de combinações de pares concluída.")
        return True
    except Exception as e:
        logger.error(f"Erro na análise de combinações de pares: {e}", exc_info=True)
        return False

# Se este arquivo se chamar execute_pairs.py, o import em __init__.py e a chamada em main.py
# devem refletir isso. O main.py atual usa "pair-combination-analysis" e chama
# ps.run_pair_combination_analysis, o que sugere que esta função deve estar em __init__.py.
# O arquivo execute_pairs.py que você forneceu pode ser este, ou uma parte dele.

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_cycle_closing_propensity.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_cycle_closing_propensity.py
import pandas as pd
import logging
from typing import Any, Dict, Optional
from sklearn.preprocessing import MinMaxScaler

# Importa as funções corrigidas do módulo de análise
from src.analysis.cycle_closing_analysis import calculate_closing_number_stats, get_cycles_df_corrected
from src.config import Config # Para type hinting
from src.database_manager import DatabaseManager # Para type hinting

logger = logging.getLogger(__name__)

def run_cycle_closing_propensity_analysis(
    db_manager: DatabaseManager,
    config: Config,
    shared_context: Dict[str, Any],
    # cycles_detail_df é esperado do contexto, produzido por uma etapa anterior
    cycles_detail_df: Optional[pd.DataFrame] = None, 
    **kwargs 
) -> bool:
    step_name = "Cycle Closing Propensity Analysis"
    logger.info(f"==== Iniciando Etapa: {step_name} ====")

    required_attrs = [
        'ANALYSIS_CYCLES_DETAIL_TABLE_NAME', 'ANALYSIS_CYCLE_CLOSING_PROPENSITY_TABLE_NAME',
        'ALL_NUMBERS', 'DEZENA_COLUMN_NAME', 'CYCLE_CLOSING_SCORE_COLUMN_NAME',
        'CICLO_NUM_COLUMN_NAME' # Usado por get_cycles_df_corrected
    ]
    for attr in required_attrs:
        if not hasattr(config, attr):
            logger.error(f"{step_name}: Atributo de configuração '{attr}' não encontrado. Abortando.")
            return False

    try:
        current_cycles_df: Optional[pd.DataFrame] = None
        if cycles_detail_df is not None and not cycles_detail_df.empty:
            logger.info(f"{step_name}: Usando 'cycles_detail_df' fornecido pelo contexto ({len(cycles_detail_df)} linhas).")
            current_cycles_df = cycles_detail_df
        else:
            logger.info(f"{step_name}: 'cycles_detail_df' não fornecido ou vazio no contexto. Carregando da DB.")
            # get_cycles_df_corrected agora usa db_manager e config
            current_cycles_df = get_cycles_df_corrected(db_manager, config) 
        
        if current_cycles_df is None or current_cycles_df.empty:
            logger.warning(f"Nenhum dado de detalhe de ciclos disponível. Não é possível calcular propensão.")
            # Cria e salva uma tabela default para o AnalysisAggregator não quebrar
            propensity_df_default = pd.DataFrame({
                config.DEZENA_COLUMN_NAME: config.ALL_NUMBERS, 
                config.CYCLE_CLOSING_SCORE_COLUMN_NAME: 0.0
            })
            table_name_default = config.ANALYSIS_CYCLE_CLOSING_PROPENSITY_TABLE_NAME
            db_manager.save_dataframe(propensity_df_default, table_name_default, if_exists='replace')
            logger.info(f"Tabela '{table_name_default}' salva com defaults (sem dados de ciclo).")
            return True # Considera sucesso, pois não havia dados para processar

        logger.debug("Chamando calculate_closing_number_stats...")
        # Passa db_manager e config para a função de análise
        closing_stats_df = calculate_closing_number_stats(db_manager, config, current_cycles_df)

        # Nome da coluna de score como será salva no banco (antes do alias do Aggregator)
        score_col_in_db = config.CYCLE_CLOSING_SCORE_COLUMN_NAME # Deve ser "score"

        if closing_stats_df is None or closing_stats_df.empty:
            logger.info("Nenhuma estatística de fechamento de ciclo calculada. Scores serão 0.")
            propensity_df_final = pd.DataFrame({config.DEZENA_COLUMN_NAME: config.ALL_NUMBERS, score_col_in_db: 0.0})
        else:
            propensity_df = closing_stats_df.reset_index() # 'dezena' vira coluna
            
            if config.DEZENA_COLUMN_NAME not in propensity_df.columns:
                logger.error(f"Coluna '{config.DEZENA_COLUMN_NAME}' não encontrada após reset_index. Verifique a saída de calculate_closing_number_stats.")
                return False
            
            if 'closing_freq' not in propensity_df.columns:
                logger.warning("Coluna 'closing_freq' não encontrada nas estatísticas. Scores serão 0.")
                propensity_df[score_col_in_db] = 0.0
            else:
                propensity_df_to_score = propensity_df[[config.DEZENA_COLUMN_NAME, 'closing_freq']].copy()
                propensity_df_to_score['closing_freq'].fillna(0, inplace=True) # Trata NaNs antes de escalar
                
                scaler = MinMaxScaler()
                if propensity_df_to_score.empty or propensity_df_to_score['closing_freq'].sum() == 0: # Se todos os valores são zero ou df vazio
                    propensity_df_to_score[score_col_in_db] = 0.0
                elif propensity_df_to_score['closing_freq'].nunique() > 1:
                    propensity_df_to_score[score_col_in_db] = scaler.fit_transform(propensity_df_to_score[['closing_freq']])
                else: # Todos os valores são iguais e não zero
                    propensity_df_to_score[score_col_in_db] = 0.5 
                
            # Garante que todas as dezenas estejam presentes e seleciona colunas finais
            all_dezenas_df = pd.DataFrame({config.DEZENA_COLUMN_NAME: config.ALL_NUMBERS})
            propensity_df_final = pd.merge(all_dezenas_df, propensity_df_to_score[[config.DEZENA_COLUMN_NAME, score_col_in_db]], 
                                           on=config.DEZENA_COLUMN_NAME, how='left').fillna({score_col_in_db: 0.0})

        table_name_to_save = config.ANALYSIS_CYCLE_CLOSING_PROPENSITY_TABLE_NAME
        db_manager.save_dataframe(propensity_df_final[[config.DEZENA_COLUMN_NAME, score_col_in_db]], table_name_to_save, if_exists='replace')
        logger.info(f"Scores de propensão de fechamento ({len(propensity_df_final)} dezenas) salvos em '{table_name_to_save}'.")

        logger.info(f"==== Etapa: {step_name} CONCLUÍDA ====")
        return True

    except AttributeError as ae: # Para erros de config não encontrado
        logger.error(f"Erro de atributo em {step_name} (verifique constantes em config): {ae}", exc_info=True)
        return False
    except Exception as e:
        logger.error(f"Erro durante a execução da {step_name}: {e}", exc_info=True)
        return False

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_cycle_progression.py
--------------------------------------------------------------------------------
import logging
import pandas as pd
from typing import Any, Dict, List, Set

from src.analysis.cycle_progression_analysis import calculate_cycle_progression
# Removido: from src.config import config_obj # Usaremos o config injetado

logger = logging.getLogger(__name__)

def run_cycle_progression_analysis_step(
    all_data_df: pd.DataFrame,
    db_manager: Any, # Idealmente DatabaseManager
    config: Any, # Nome do parâmetro corrigido de config_param para config
    shared_context: Dict[str, Any],
    **kwargs
) -> bool:
    step_name = "Cycle Progression and Status Analysis"
    logger.info(f"==== Iniciando Etapa: {step_name} ====")

    if all_data_df.empty:
        logger.warning(f"{step_name}: DataFrame de dados (all_data_df) está vazio. Etapa pulada.")
        return True

    # Verificar se atributos essenciais de config existem
    required_config_attrs = [
        'CONTEST_ID_COLUMN_NAME', 'ALL_NUMBERS',
        'ANALYSIS_CYCLE_PROGRESSION_RAW_TABLE_NAME',
        'ANALYSIS_CYCLE_STATUS_DEZENAS_TABLE_NAME'
    ]
    for attr in required_config_attrs:
        if not hasattr(config, attr): # Usar o 'config' injetado
            logger.error(f"{step_name}: Atributo de config '{attr}' não encontrado. Abortando.")
            return False

    try:
        # A função calculate_cycle_progression precisará ser ajustada se ela usa config_obj internamente,
        # ou podemos passar o objeto config para ela.
        # Assumindo que calculate_cycle_progression pode aceitar config:
        # df_progression_raw = calculate_cycle_progression(all_data_df, config)
        # Se calculate_cycle_progression depende de config_obj globalmente, essa é uma refatoração maior.
        # Por ora, vamos manter a chamada como está, e o problema interno de calculate_cycle_progression
        # com config_obj (se houver) precisaria ser tratado em src/analysis/cycle_progression_analysis.py.
        # O código original fornecido aqui já chamava calculate_cycle_progression(all_data_df, config_obj)
        # Onde config_obj era importado. Se config_obj é o mesmo que o 'config' injetado,
        # então está ok, mas o ideal é injetar dependências.
        # Para esta correção, vamos focar na assinatura do step e assumir que config_obj é o desejado por calculate_cycle_progression.
        # No entanto, a boa prática é usar o 'config' injetado.
        # Se calculate_cycle_progression SÓ usa config_obj, então o parâmetro 'config' neste step
        # não estaria sendo usado para essa chamada específica, mas poderia ser para outras coisas.
        # Vou manter a chamada a calculate_cycle_progression como estava, mas a recomendação é refatorá-la.
        df_progression_raw = calculate_cycle_progression(all_data_df, config) # << MUDADO PARA USAR config injetado

        status_dezenas_records: List[Dict[str, Any]] = []

        if df_progression_raw is None or df_progression_raw.empty:
            logger.warning(f"{step_name}: Nenhum dado de progressão de ciclo foi gerado. "
                           "A tabela de status de dezenas será populada com defaults.")
            all_contest_ids_in_input = sorted(all_data_df[config.CONTEST_ID_COLUMN_NAME].unique()) # Usar config injetado
            for c_id in all_contest_ids_in_input:
                for dezena_val in config.ALL_NUMBERS: # Usar config injetado
                    status_dezenas_records.append({
                        config.CONTEST_ID_COLUMN_NAME: c_id, # Usar config injetado
                        'dezena': dezena_val,
                        'is_missing_in_current_cycle': 0
                    })
            if not all_contest_ids_in_input:
                 logger.info(f"{step_name}: Nenhum dado de progressão e nenhum concurso. Saindo.")
                 return True
        else:
            raw_progression_table_name = config.ANALYSIS_CYCLE_PROGRESSION_RAW_TABLE_NAME # Usar config injetado
            db_manager.save_dataframe(df_progression_raw, raw_progression_table_name, if_exists='replace')
            logger.info(f"Dados brutos de progressão de ciclo salvos na '{raw_progression_table_name}' ({len(df_progression_raw)}).")

            logger.info(f"{step_name}: Transformando dados de progressão para status de dezenas...")
            col_id_from_raw = "Concurso"
            col_faltantes_from_raw = 'numeros_faltantes_apos_este_concurso'

            if col_id_from_raw not in df_progression_raw.columns or \
               col_faltantes_from_raw not in df_progression_raw.columns:
                logger.error(f"Colunas esperadas não encontradas em df_progression_raw. Não gerar status.")
                return False

            for _, row in df_progression_raw.iterrows():
                concurso_id_val = int(row[col_id_from_raw])
                faltantes_str = row[col_faltantes_from_raw]
                numeros_faltantes_set: Set[int] = set()
                if pd.notna(faltantes_str) and isinstance(faltantes_str, str) and faltantes_str.strip():
                    try:
                        numeros_faltantes_set = set(map(int, faltantes_str.split(',')))
                    except ValueError:
                        logger.warning(f"Erro ao parsear faltantes ('{faltantes_str}') para concurso {concurso_id_val}.")

                for dezena_val in config.ALL_NUMBERS: # Usar config injetado
                    is_missing = 1 if dezena_val in numeros_faltantes_set else 0
                    status_dezenas_records.append({
                        config.CONTEST_ID_COLUMN_NAME: concurso_id_val, # Usar config injetado
                        'dezena': dezena_val,
                        'is_missing_in_current_cycle': is_missing
                    })

        if not status_dezenas_records:
            logger.warning(f"{step_name}: Nenhum registro de status de dezenas foi gerado.")
        else:
            df_status_dezenas = pd.DataFrame(status_dezenas_records)
            df_status_dezenas.drop_duplicates(subset=[config.CONTEST_ID_COLUMN_NAME, 'dezena'], keep='last', inplace=True) # Usar config

            status_table_name = config.ANALYSIS_CYCLE_STATUS_DEZENAS_TABLE_NAME # Usar config injetado
            db_manager.save_dataframe(df_status_dezenas, status_table_name, if_exists='replace')
            logger.info(f"Dados de status de dezenas salvos na '{status_table_name}' ({len(df_status_dezenas)}).")

        logger.info(f"==== Etapa: {step_name} CONCLUÍDA ====")
        return True

    except AttributeError as ae: # Este AttributeError agora pode ser mais específico se o config não tiver um atributo
        logger.error(f"Erro de atributo em {step_name}: {ae}", exc_info=True)
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False
    except Exception as e:
        logger.error(f"Erro na etapa {step_name}: {e}", exc_info=True)
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_cycle_stats.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_cycle_stats.py
import pandas as pd
import logging
from typing import Dict, Any
from src.analysis.cycle_analysis import identify_and_process_cycles # Esta função já retorna o sumário
# from src.database_manager import DatabaseManager
# from src.config import Config

logger = logging.getLogger(__name__)

def run_cycle_stats_step( # Nome da função como esperado pelo main.py
    all_data_df: pd.DataFrame, # Padronizado
    db_manager: Any, 
    config: Any, 
    shared_context: Dict[str, Any], # Adicionado
    **kwargs
) -> bool:
    step_name = "Cycle Summary Stats Calculation"
    logger.info(f"Iniciando etapa: {step_name}.")
    try:
        # identify_and_process_cycles já calcula o sumário.
        # O step run_cycle_identification_step também chama identify_and_process_cycles.
        # Para evitar recálculo, poderíamos pegar 'ciclos_detalhe' do shared_context
        # se o step de identificação o salvar lá com uma "output_key".
        # Por ora, vamos recalcular, mas isso é um ponto de otimização.
        
        # Alternativamente, se run_cycle_identification_step já salvou 'ciclos_detalhe'
        # e 'ciclos_sumario_estatisticas', este step poderia ser apenas para garantir
        # ou para outras estatísticas. O código atual do execute_cycle_stats.py
        # que você forneceu também chama identify_and_process_cycles.
        
        cycle_analysis_results = identify_and_process_cycles(all_data_df, config) # Passa config
        
        saved_summary = False
        if cycle_analysis_results:
            df_summary = cycle_analysis_results.get('ciclos_sumario_estatisticas')
            if df_summary is not None and not df_summary.empty:
                db_manager.save_dataframe(df_summary, 'ciclos_sumario_estatisticas', if_exists='replace')
                logger.info(f"Sumário estatístico dos ciclos salvo na tabela 'ciclos_sumario_estatisticas'.")
                saved_summary = True
            else:
                logger.info("Nenhum DataFrame de sumário estatístico de ciclo para salvar.")
                
            # Opcional: salvar 'ciclos_detalhe' também se não foi salvo pelo step de identificação
            df_details = cycle_analysis_results.get('ciclos_detalhe')
            if df_details is not None and not df_details.empty:
                 if not db_manager.table_exists('ciclos_detalhe'): # Salva se não existir
                    db_manager.save_dataframe(df_details, 'ciclos_detalhe', if_exists='replace')
                    logger.info("Detalhes dos ciclos salvos pela etapa de stats (pois não existia).")
        else:
            logger.warning(f"Análise de ciclo não retornou resultados para {step_name}.")

        logger.info(f"Etapa: {step_name} concluída.")
        return True
    except Exception as e:
        logger.error(f"Erro na etapa {step_name}: {e}", exc_info=True)
        return False

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_cycles.py
--------------------------------------------------------------------------------
import pandas as pd
import logging
from typing import Dict, Any, Optional # Adicionado Optional

from src.analysis.cycle_analysis import identify_and_process_cycles
# from src.database_manager import DatabaseManager # Para type hint
# from src.config import Config # Para type hint

logger = logging.getLogger(__name__)

def run_cycle_identification_step(
    all_data_df: pd.DataFrame,
    db_manager: Any, # DatabaseManager
    config: Any, # Config
    shared_context: Dict[str, Any],
    **kwargs
) -> Optional[pd.DataFrame]: # Tipo de retorno alterado para o DataFrame ou None
    step_name = "Cycle Identification and Basic Stats"
    logger.info(f"Iniciando etapa do pipeline: {step_name}")
    df_details_to_return: Optional[pd.DataFrame] = None # DataFrame a ser retornado

    try:
        cycle_analysis_results = identify_and_process_cycles(all_data_df, config)

        if not cycle_analysis_results:
            logger.warning(f"Nenhum resultado de ciclo retornado para {step_name}.")
            # Mesmo sem resultados, podemos querer retornar um DataFrame vazio se o output_key espera um.
            # Ou o Orchestrator pode lidar com None e não adicionar ao shared_context.
            # Vamos retornar None se não houver resultados válidos.
            logger.info(f"==== Etapa: {step_name} CONCLUÍDA (sem dados de ciclo) ====")
            return None

        saved_any = False
        df_details = cycle_analysis_results.get('ciclos_detalhe')
        if df_details is not None and not df_details.empty:
            try:
                db_manager.save_dataframe(df_details, 'ciclos_detalhe', if_exists='replace')
                logger.info(f"Resultados de 'ciclos_detalhe' salvos na tabela 'ciclos_detalhe'.")
                df_details_to_return = df_details # Armazena para retornar
                saved_any = True
            except Exception as e_save:
                logger.error(f"Erro ao salvar tabela 'ciclos_detalhe': {e_save}", exc_info=True)
                # Não retorna df_details se o salvamento falhar, ou decide com base na criticidade
        else:
            logger.info(f"DataFrame para 'ciclos_detalhe' vazio ou nulo.")
            # Se for vazio mas válido, pode ser retornado.
            if isinstance(df_details, pd.DataFrame): # Checa se é um DataFrame mesmo que vazio
                df_details_to_return = df_details


        df_summary = cycle_analysis_results.get('ciclos_sumario_estatisticas')
        if df_summary is not None and not df_summary.empty:
            try:
                db_manager.save_dataframe(df_summary, 'ciclos_sumario_estatisticas', if_exists='replace')
                logger.info(f"Resultados de 'ciclos_sumario_estatisticas' salvos na tabela 'ciclos_sumario_estatisticas'.")
                saved_any = True
            except Exception as e_save_sum:
                logger.error(f"Erro ao salvar tabela 'ciclos_sumario_estatisticas': {e_save_sum}", exc_info=True)
        else:
            logger.info(f"DataFrame para 'ciclos_sumario_estatisticas' vazio ou nulo.")

        if saved_any:
            logger.info(f"Etapa {step_name} concluída com dados salvos.")
        else:
            logger.info(f"Etapa {step_name} concluída, sem novos dados salvos (ou dados já existentes).")

        # Retorna o DataFrame que deve ser colocado no shared_context via output_key
        return df_details_to_return

    except Exception as e:
        logger.error(f"Erro na etapa {step_name}: {e}", exc_info=True)
        return None # Retorna None em caso de falha na etapa

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_delay.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_delay.py
import pandas as pd
import logging
from typing import Any, Dict, List, Optional

from src.config import Config 
from src.database_manager import DatabaseManager

from src.analysis.delay_analysis import (
    get_draw_matrix, 
    calculate_current_delay, 
    calculate_max_delay, 
    calculate_mean_delay
)

logger = logging.getLogger(__name__)

def run_delay_analysis(
    all_data_df: pd.DataFrame,
    db_manager: DatabaseManager,
    config: Config,
    shared_context: Dict[str, Any],
    force_full_recalculation: bool = False, # Novo parâmetro, pode ser controlado pelo --force-reload
    **kwargs 
) -> bool:
    step_name = "Delay Analysis (Historical Incremental)"
    logger.info(f"==== Iniciando Etapa: {step_name} ====")

    # Validações de config (como antes)
    required_attrs = [
        'CONTEST_ID_COLUMN_NAME', 'DEZENA_COLUMN_NAME', 'ALL_NUMBERS',
        'ANALYSIS_DELAYS_TABLE_NAME', 'CURRENT_DELAY_COLUMN_NAME',
        'MAX_DELAY_OBSERVED_COLUMN_NAME', 'AVG_DELAY_COLUMN_NAME',
        'MIN_CONTESTS_FOR_HISTORICAL_DELAY'
    ]
    for attr in required_attrs:
        if not hasattr(config, attr):
            logger.error(f"{step_name}: Atributo de config '{attr}' não encontrado. Abortando.")
            return False
    if all_data_df.empty:
        logger.warning(f"{step_name}: 'all_data_df' vazio. Etapa pulada.")
        return True

    contest_id_col = config.CONTEST_ID_COLUMN_NAME
    dezena_col = config.DEZENA_COLUMN_NAME
    current_delay_col_name = config.CURRENT_DELAY_COLUMN_NAME
    max_delay_col_name = config.MAX_DELAY_OBSERVED_COLUMN_NAME
    avg_delay_col_name = config.AVG_DELAY_COLUMN_NAME
    table_name = config.ANALYSIS_DELAYS_TABLE_NAME
    min_hist_contests = config.MIN_CONTESTS_FOR_HISTORICAL_DELAY

    start_processing_from_contest_id = 1 # Default para processar tudo
    if_exists_mode = 'replace'

    if not force_full_recalculation and db_manager.table_exists(table_name):
        last_processed_query = f"SELECT MAX({contest_id_col}) FROM {table_name}"
        last_processed_df = db_manager.execute_query(last_processed_query)
        if last_processed_df is not None and not last_processed_df.empty and pd.notna(last_processed_df.iloc[0,0]):
            last_saved_contest = int(last_processed_df.iloc[0,0])
            start_processing_from_contest_id = last_saved_contest + 1
            if_exists_mode = 'append'
            logger.info(f"{step_name}: Tabela '{table_name}' existente. Último concurso processado: {last_saved_contest}.")
        else:
            logger.info(f"{step_name}: Tabela '{table_name}' existe mas está vazia ou não tem {contest_id_col}. Recalculando tudo.")
    else:
        logger.info(f"{step_name}: {'--force-reload ativo' if force_full_recalculation else 'Tabela não existe'}. Recalculando tudo para '{table_name}'.")

    # Seleciona apenas os concursos que precisam ser processados como "pontos finais" do cálculo histórico
    # Se for append, processaremos apenas os novos. Se for replace, todos.
    all_contest_ids_in_data = sorted(all_data_df[contest_id_col].unique())
    
    # Pontos de concurso para os quais vamos calcular e salvar o estado histórico
    target_contest_ids_to_calculate = [
        cid for i, cid in enumerate(all_contest_ids_in_data) 
        if cid >= start_processing_from_contest_id and i >= min_hist_contests - 1
    ]
    
    if not target_contest_ids_to_calculate:
        logger.info(f"{step_name}: Nenhum novo concurso para processar (a partir de {start_processing_from_contest_id} e após {min_hist_contests-1} iniciais).")
        return True

    historical_delays_data: List[pd.DataFrame] = []
    
    total_points_to_process = len(target_contest_ids_to_calculate)
    log_interval = max(1, total_points_to_process // 20) if total_points_to_process > 100 else 1
    
    logger.info(f"{step_name}: Processamento de atrasos para {total_points_to_process} pontos (concursos de corte).")

    for i, current_max_contest_id in enumerate(target_contest_ids_to_calculate):
        if (i + 1) % log_interval == 0 or i == 0 or i == total_points_to_process - 1 :
             logger.info(f"{step_name}: Progresso - {i+1}/{total_points_to_process} (concurso de corte: {current_max_contest_id})")
        
        # df_upto_contest AINDA é todo o histórico até current_max_contest_id
        # Isso é necessário para calcular max_delay e avg_delay corretamente da forma atual.
        df_upto_contest = all_data_df[all_data_df[contest_id_col] <= current_max_contest_id].copy()
        if df_upto_contest.empty or len(df_upto_contest) < min_hist_contests : 
            logger.debug(f"Dados insuficientes até concurso {current_max_contest_id} (linhas: {len(df_upto_contest)}).")
            continue

        try:
            draw_matrix_upto_contest = get_draw_matrix(df_upto_contest, config)
            if draw_matrix_upto_contest.empty:
                logger.warning(f"Matriz de sorteios vazia para concurso {current_max_contest_id}.")
                # Lógica de default para este ponto de cálculo
                default_df = pd.DataFrame({
                    contest_id_col: current_max_contest_id,
                    dezena_col: config.ALL_NUMBERS,
                    current_delay_col_name: len(df_upto_contest),
                    max_delay_col_name: len(df_upto_contest),
                    avg_delay_col_name: pd.NA
                })
                cols_order_def = [contest_id_col, dezena_col, current_delay_col_name, max_delay_col_name, avg_delay_col_name]
                historical_delays_data.append(default_df[cols_order_def])
                continue

            last_contest_in_matrix = draw_matrix_upto_contest.index.max()
            first_contest_in_matrix = draw_matrix_upto_contest.index.min()
            
            current_delay_df_raw = calculate_current_delay(draw_matrix_upto_contest, config, last_contest_in_matrix)
            max_delay_df_raw = calculate_max_delay(draw_matrix_upto_contest, config, first_contest_in_matrix, last_contest_in_matrix)
            mean_delay_df_raw = calculate_mean_delay(draw_matrix_upto_contest, config)
            
            # ... (lógica de merge e formatação como na versão anterior)
            all_dezenas_df = pd.DataFrame({dezena_col: config.ALL_NUMBERS})
            all_dezenas_df[dezena_col] = all_dezenas_df[dezena_col].astype(int)

            current_delay_df = all_dezenas_df.copy()
            if current_delay_df_raw is not None and not current_delay_df_raw.empty:
                current_delay_df_raw = current_delay_df_raw.rename(columns={'Dezena': dezena_col, 'Atraso Atual': current_delay_col_name}, errors='ignore')
                if dezena_col in current_delay_df_raw and current_delay_col_name in current_delay_df_raw:
                    current_delay_df = pd.merge(current_delay_df, current_delay_df_raw[[dezena_col, current_delay_col_name]], on=dezena_col, how='left')
            current_delay_df[current_delay_col_name] = current_delay_df[current_delay_col_name].fillna(len(draw_matrix_upto_contest)).astype(int)

            max_delay_df = all_dezenas_df.copy()
            if max_delay_df_raw is not None and not max_delay_df_raw.empty:
                max_delay_df_raw = max_delay_df_raw.rename(columns={'Dezena': dezena_col, 'Atraso Maximo': max_delay_col_name}, errors='ignore')
                if dezena_col in max_delay_df_raw and max_delay_col_name in max_delay_df_raw:
                     max_delay_df = pd.merge(max_delay_df, max_delay_df_raw[[dezena_col, max_delay_col_name]], on=dezena_col, how='left')
            
            mean_delay_df = all_dezenas_df.copy()
            if mean_delay_df_raw is not None and not mean_delay_df_raw.empty:
                mean_delay_df_raw = mean_delay_df_raw.rename(columns={'Dezena': dezena_col, 'Atraso Medio': avg_delay_col_name}, errors='ignore')
                if dezena_col in mean_delay_df_raw and avg_delay_col_name in mean_delay_df_raw:
                    mean_delay_df = pd.merge(mean_delay_df, mean_delay_df_raw[[dezena_col, avg_delay_col_name]], on=dezena_col, how='left')
            mean_delay_df[avg_delay_col_name] = pd.to_numeric(mean_delay_df[avg_delay_col_name], errors='coerce')

            merged_df = pd.merge(current_delay_df, max_delay_df, on=dezena_col, how='outer')
            merged_df = pd.merge(merged_df, mean_delay_df, on=dezena_col, how='outer')
            merged_df[contest_id_col] = current_max_contest_id
            cols_order = [contest_id_col, dezena_col, current_delay_col_name, max_delay_col_name, avg_delay_col_name]
            
            for col_name_check in cols_order:
                if col_name_check not in merged_df.columns:
                    default_val = len(draw_matrix_upto_contest) if col_name_check != avg_delay_col_name else pd.NA
                    merged_df[col_name_check] = default_val
            merged_df = merged_df[cols_order]

            merged_df[current_delay_col_name] = merged_df[current_delay_col_name].fillna(len(draw_matrix_upto_contest)).astype(int)
            merged_df[max_delay_col_name] = merged_df[max_delay_col_name].fillna(merged_df[current_delay_col_name]).astype(int)
            
            historical_delays_data.append(merged_df)
        except Exception as e_inner:
            logger.error(f"Erro ao processar atrasos para concurso {current_max_contest_id}: {e_inner}", exc_info=True)

    if not historical_delays_data:
        logger.warning(f"{step_name}: Nenhum novo dado de atraso histórico foi gerado para o intervalo solicitado.")
        return True # Não é um erro se não havia nada novo para processar

    final_df_to_save = pd.concat(historical_delays_data, ignore_index=True)
    
    try:
        if if_exists_mode == 'replace' and db_manager.table_exists(table_name):
             logger.info(f"Modo 'replace': Removendo dados antigos da tabela '{table_name}' antes de salvar.")
             # Poderia deletar ou a tabela seria substituída por to_sql.
             # Se to_sql com if_exists='replace' não apagar primeiro, você pode precisar de um DELETE.
             # Mas df.to_sql com if_exists='replace' geralmente dropa e recria.
        
        db_manager.save_dataframe(final_df_to_save, table_name, if_exists=if_exists_mode)
        logger.info(f"Dados de atraso ({len(final_df_to_save)} linhas) salvos em '{table_name}' (modo: {if_exists_mode}).")
        logger.info(f"==== Etapa: {step_name} CONCLUÍDA ====")
        return True
    except Exception as e:
        logger.error(f"Erro na etapa {step_name} ao salvar dados: {e}", exc_info=True)
        return False

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_detailed_cycle_metrics.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_detailed_cycle_metrics.py
import logging
import pandas as pd
from typing import Any, Dict, Optional

from src.analysis.cycle_analysis import identify_and_process_cycles, calculate_detailed_metrics_per_closed_cycle

logger = logging.getLogger(__name__)

def run_detailed_cycle_metrics_step(
    all_data_df: pd.DataFrame, 
    db_manager: Any, 
    config: Any, 
    shared_context: Dict[str, Any],
    **kwargs
) -> bool:
    step_name = "Detailed Cycle Metrics Calculation"
    logger.info(f"Iniciando etapa do pipeline: {step_name}")

    try:
        # Tenta carregar 'ciclos_detalhe' do banco de dados
        df_ciclos_detalhe = db_manager.load_dataframe("ciclos_detalhe") 

        if df_ciclos_detalhe is None or df_ciclos_detalhe.empty:
            logger.warning(f"DataFrame 'ciclos_detalhe' não encontrado no banco ou vazio na etapa {step_name}. Tentando identificar ciclos agora.")
            # Fallback: Se não encontrar no DB, recalcula.
            cycle_identification_results = identify_and_process_cycles(all_data_df, config) # Passa config
            df_ciclos_detalhe = cycle_identification_results.get('ciclos_detalhe')
            
            if df_ciclos_detalhe is None or df_ciclos_detalhe.empty:
                logger.error(f"Não foi possível obter 'ciclos_detalhe' nem do DB nem por novo cálculo na etapa {step_name}. Abortando step.")
                return False

        # Passa config para calculate_detailed_metrics_per_closed_cycle
        dict_metric_dfs = calculate_detailed_metrics_per_closed_cycle(all_data_df, df_ciclos_detalhe, config)

        if not dict_metric_dfs:
            logger.warning(f"Nenhuma métrica detalhada de ciclo retornada pela análise para a etapa {step_name}.")
            return True 

        saved_any = False
        for table_key, df_to_save in dict_metric_dfs.items():
            if df_to_save is not None and not df_to_save.empty:
                try:
                    db_manager.save_dataframe(df_to_save, table_key, if_exists='replace')
                    logger.info(f"Métricas detalhadas de ciclo '{table_key}' salvas na tabela '{table_key}'.")
                    saved_any = True
                except Exception as e_save:
                    logger.error(f"Erro ao salvar a tabela de métricas detalhadas de ciclo '{table_key}': {e_save}", exc_info=True)
            else:
                logger.info(f"DataFrame para métricas detalhadas de ciclo '{table_key}' está vazio ou nulo.")
        
        if saved_any: 
            logger.info(f"Etapa {step_name} concluída com dados salvos.")
        else: 
            logger.info(f"Etapa {step_name} concluída, mas nenhum novo dado foi salvo.")
        return True
        
    except Exception as e:
        logger.error(f"Erro ao executar a etapa {step_name}: {e}", exc_info=True)
        return False

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_frequency.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_frequency.py
import pandas as pd
import logging
from typing import Any, Dict, List, Optional

from src.config import Config 
from src.database_manager import DatabaseManager

logger = logging.getLogger(__name__)

def run_frequency_analysis(
    all_data_df: pd.DataFrame,
    db_manager: DatabaseManager,
    config: Config,
    shared_context: Dict[str, Any],
    **kwargs 
) -> bool:
    step_name = "Frequency Analysis (Historical)"
    logger.info(f"==== Iniciando Etapa: {step_name} ====")

    required_attrs = [
        'CONTEST_ID_COLUMN_NAME', 'DEZENA_COLUMN_NAME', 'ALL_NUMBERS',
        'ANALYSIS_FREQUENCY_OVERALL_TABLE_NAME', 
        'FREQUENCY_COLUMN_NAME', 'RELATIVE_FREQUENCY_COLUMN_NAME'
    ]
    for attr in required_attrs:
        if not hasattr(config, attr):
            logger.error(f"{step_name}: Atributo de config '{attr}' não encontrado. Abortando.")
            return False

    if all_data_df.empty:
        logger.warning(f"{step_name}: 'all_data_df' vazio. Etapa pulada.")
        return True

    historical_frequency_data: List[pd.DataFrame] = []
    
    contest_id_col = config.CONTEST_ID_COLUMN_NAME
    dezena_col = config.DEZENA_COLUMN_NAME
    freq_col = config.FREQUENCY_COLUMN_NAME
    rel_freq_col = config.RELATIVE_FREQUENCY_COLUMN_NAME
    
    all_contest_ids = sorted(all_data_df[contest_id_col].unique())

    if not all_contest_ids:
        logger.warning(f"{step_name}: Nenhum concurso único. Etapa pulada.")
        return True
        
    from src.analysis.frequency_analysis import calculate_frequency, calculate_relative_frequency
    
    total_contests = len(all_contest_ids)
    log_interval = max(1, total_contests // 20) if total_contests > 100 else 1

    logger.info(f"{step_name}: Processando frequências para {total_contests} concursos.")

    for i, current_max_contest_id in enumerate(all_contest_ids):
        if (i + 1) % log_interval == 0 or i == 0 or i == total_contests - 1:
            logger.info(f"{step_name}: Progresso - {i+1}/{total_contests} (concurso de corte: {current_max_contest_id})")
        
        df_upto_contest = all_data_df[all_data_df[contest_id_col] <= current_max_contest_id].copy()

        if df_upto_contest.empty:
            logger.debug(f"Nenhum dado até concurso {current_max_contest_id}.")
            continue

        try:
            abs_freq_df_raw = calculate_frequency(df_upto_contest, config) 
            
            abs_freq_df = pd.DataFrame({dezena_col: config.ALL_NUMBERS})
            if abs_freq_df_raw is not None and not abs_freq_df_raw.empty:
                rename_map_abs = {}
                if 'Dezena' in abs_freq_df_raw.columns: rename_map_abs['Dezena'] = dezena_col
                if 'Frequencia Absoluta' in abs_freq_df_raw.columns: rename_map_abs['Frequencia Absoluta'] = freq_col
                abs_freq_df_renamed = abs_freq_df_raw.rename(columns=rename_map_abs, errors='ignore')
                
                if dezena_col in abs_freq_df_renamed and freq_col in abs_freq_df_renamed:
                    abs_freq_df = pd.merge(abs_freq_df, abs_freq_df_renamed[[dezena_col, freq_col]], on=dezena_col, how='left')
                else:
                    abs_freq_df[freq_col] = 0 
            else: # Se abs_freq_df_raw for None ou vazio
                 abs_freq_df[freq_col] = 0
            
            # CORREÇÃO FutureWarning:
            abs_freq_df[freq_col] = abs_freq_df[freq_col].fillna(0)
            abs_freq_df[dezena_col] = abs_freq_df[dezena_col].astype(int)

            num_draws_considered = len(df_upto_contest[contest_id_col].unique())
            
            rel_freq_df = pd.DataFrame({dezena_col: config.ALL_NUMBERS})
            if num_draws_considered > 0:
                temp_abs_for_rel = abs_freq_df.rename(columns={
                    dezena_col: 'Dezena', freq_col: 'Frequencia Absoluta'
                })
                rel_freq_df_raw = calculate_relative_frequency(temp_abs_for_rel, num_draws_considered, config)
                if rel_freq_df_raw is not None and not rel_freq_df_raw.empty:
                    rename_map_rel = {}
                    if 'Dezena' in rel_freq_df_raw.columns: rename_map_rel['Dezena'] = dezena_col
                    if 'Frequencia Relativa' in rel_freq_df_raw.columns: rename_map_rel['Frequencia Relativa'] = rel_freq_col
                    rel_freq_df_renamed = rel_freq_df_raw.rename(columns=rename_map_rel, errors='ignore')
                    if dezena_col in rel_freq_df_renamed and rel_freq_col in rel_freq_df_renamed:
                         rel_freq_df = pd.merge(rel_freq_df, rel_freq_df_renamed[[dezena_col, rel_freq_col]], on=dezena_col, how='left')
                    else:
                        rel_freq_df[rel_freq_col] = 0.0
                else: # Se rel_freq_df_raw for None ou vazio
                    rel_freq_df[rel_freq_col] = 0.0
            else: # Se num_draws_considered == 0
                rel_freq_df[rel_freq_col] = 0.0
            
            # CORREÇÃO FutureWarning:
            rel_freq_df[rel_freq_col] = rel_freq_df[rel_freq_col].fillna(0.0)
            rel_freq_df[dezena_col] = rel_freq_df[dezena_col].astype(int)

            merged_df = pd.merge(abs_freq_df, rel_freq_df, on=dezena_col, how='outer')
            merged_df[contest_id_col] = current_max_contest_id
            
            cols_order = [contest_id_col, dezena_col, freq_col, rel_freq_col]
            for col_name_check in cols_order: # Assegura colunas antes de reordenar
                if col_name_check not in merged_df.columns:
                    if col_name_check == freq_col: merged_df[col_name_check] = 0
                    elif col_name_check == rel_freq_col: merged_df[col_name_check] = 0.0
            merged_df = merged_df[cols_order]
            
            # CORREÇÃO FutureWarning (já deve estar preenchido, mas como garantia):
            merged_df[freq_col] = merged_df[freq_col].fillna(0)
            merged_df[rel_freq_col] = merged_df[rel_freq_col].fillna(0.0)

            historical_frequency_data.append(merged_df)
        except Exception as e_inner:
            logger.error(f"Erro ao processar frequências para concurso {current_max_contest_id}: {e_inner}", exc_info=True)

    if not historical_frequency_data:
        logger.warning(f"{step_name}: Nenhum dado de frequência histórica gerado.")
        return False

    final_historical_df = pd.concat(historical_frequency_data, ignore_index=True)
    
    try:
        table_name = config.ANALYSIS_FREQUENCY_OVERALL_TABLE_NAME 
        db_manager.save_dataframe(final_historical_df, table_name, if_exists='replace')
        logger.info(f"Dados de frequência ({len(final_historical_df)} linhas) salvos em '{table_name}'.")
        logger.info(f"==== Etapa: {step_name} CONCLUÍDA ====")
        return True
    except Exception as e:
        logger.error(f"Erro na etapa {step_name} ao salvar dados: {e}", exc_info=True)
        return False

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_frequent_itemset_metrics.py
--------------------------------------------------------------------------------
# Lotofacil_Analysis/src/pipeline_steps/execute_frequent_itemset_metrics.py
import pandas as pd
import logging
from typing import Any, Dict, Optional

# É uma boa prática ter os type hints corretos
from src.config import Config
from src.database_manager import DatabaseManager

logger = logging.getLogger(__name__)

def run_frequent_itemset_metrics_step(
    all_data_df: pd.DataFrame,
    db_manager: DatabaseManager, # Type hint atualizado
    config: Config,             # CORRIGIDO de config_param para config
    shared_context: Dict[str, Any],
    **kwargs
) -> bool:
    step_name = "Frequent Itemset Metrics Analysis"
    logger.info(f"==== Iniciando Etapa: {step_name} ====")

    required_attrs = [
        'FREQUENT_ITEMSETS_TABLE_NAME', # Tabela de onde ler os itemsets base
        'ANALYSIS_ITEMSET_METRICS_TABLE_NAME', # Tabela de saída para esta etapa
        'CONTEST_ID_COLUMN_NAME',
        'ITEMSET_STR_COLUMN_NAME',
        # Adicione outras colunas que sua tabela de métricas de itemset deve ter
    ]
    for attr in required_attrs:
        if not hasattr(config, attr):
            logger.error(f"{step_name}: Atributo de config '{attr}' não encontrado. Abortando.")
            return False
    try:
        logger.info(f"{step_name}: Lógica de cálculo de métricas de itemsets ainda não implementada.")
        
        # Exemplo de colunas para a tabela de placeholder
        placeholder_columns = [
            config.CONTEST_ID_COLUMN_NAME, # Se as métricas são por concurso
            config.ITEMSET_STR_COLUMN_NAME,
            # Adicione aqui as colunas que representarão suas métricas calculadas
            # Ex: 'itemset_avg_delay', 'itemset_max_delay', 'itemset_lift_vs_overall_avg'
        ]
        # Se CONTEST_ID_COLUMN_NAME não for usado, pode remover da lista acima.
        # Certifique-se que as colunas aqui correspondam à definição da tabela ANALYSIS_ITEMSET_METRICS_TABLE_NAME
        
        placeholder_metrics_df = pd.DataFrame(columns=placeholder_columns)
        
        db_manager.save_dataframe(
            placeholder_metrics_df,
            config.ANALYSIS_ITEMSET_METRICS_TABLE_NAME,
            if_exists='replace'
        )
        logger.info(f"{step_name}: Tabela de placeholder '{config.ANALYSIS_ITEMSET_METRICS_TABLE_NAME}' salva/assegurada.")

        logger.info(f"==== Etapa: {step_name} CONCLUÍDA (com placeholder) ====")
        return True

    except Exception as e:
        logger.error(f"Erro inesperado na etapa {step_name}: {e}", exc_info=True)
        return False

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_frequent_itemsets.py
--------------------------------------------------------------------------------
# Lotofacil_Analysis/src/pipeline_steps/execute_frequent_itemsets.py
import logging
import pandas as pd
from typing import Dict, Any, Optional

from src.analysis.combination_analysis import CombinationAnalyzer
# Para type hints mais específicos, se desejar:
# from src.config import Config
# from src.database_manager import DatabaseManager

logger = logging.getLogger(__name__)

def run_frequent_itemsets_analysis_step(
    all_data_df: pd.DataFrame,
    db_manager: Any, # Espera-se DatabaseManager
    config: Any, # Espera-se Config
    shared_context: Dict[str, Any],
    **kwargs
) -> Optional[CombinationAnalyzer]: # Retorna a instância do analyzer ou None
    step_name = "Frequent Itemsets Analysis"
    logger.info(f"==== Iniciando Etapa: {step_name} ====")
    analyzer_instance: Optional[CombinationAnalyzer] = None

    try:
        required_config_attrs = [
            'APRIORI_MIN_SUPPORT', 'FREQUENT_ITEMSETS_MIN_LEN',
            'FREQUENT_ITEMSETS_MAX_LEN', 'DRAWN_NUMBERS_COLUMN_NAME',
            'FREQUENT_ITEMSETS_TABLE_NAME', 'ALL_NUMBERS'
        ]
        for attr in required_config_attrs:
            if not hasattr(config, attr):
                logger.error(f"{step_name}: Atributo de config '{attr}' não encontrado.")
                logger.info(f"==== Etapa: {step_name} FALHOU ====")
                return None

        logger.info(f"{step_name}: Instanciando CombinationAnalyzer.")
        analyzer_instance = CombinationAnalyzer(all_numbers=config.ALL_NUMBERS)

        if not hasattr(analyzer_instance, 'analyze_frequent_itemsets'):
            logger.error(f"{step_name}: Instância 'CombinationAnalyzer' não tem método 'analyze_frequent_itemsets'.")
            logger.info(f"==== Etapa: {step_name} FALHOU ====")
            return None

        if not hasattr(db_manager, 'save_dataframe'):
            logger.error(f"{step_name}: Objeto 'db_manager' não tem método 'save_dataframe'.")
            logger.info(f"==== Etapa: {step_name} FALHOU ====")
            return None

        min_support = config.APRIORI_MIN_SUPPORT
        min_len = config.FREQUENT_ITEMSETS_MIN_LEN
        max_len = config.FREQUENT_ITEMSETS_MAX_LEN
        drawn_numbers_col = config.DRAWN_NUMBERS_COLUMN_NAME
        frequent_itemsets_table_name = config.FREQUENT_ITEMSETS_TABLE_NAME

        logger.info(f"Parâmetros da análise: min_support={min_support}, min_len={min_len}, max_len={max_len}")

        df_for_db, df_raw_for_rules = analyzer_instance.analyze_frequent_itemsets(
            all_draws_df=all_data_df,
            min_support=min_support,
            min_len=min_len,
            max_len=max_len,
            drawn_numbers_col=drawn_numbers_col
        )

        if not isinstance(df_for_db, pd.DataFrame):
            logger.error(f"{step_name}: analyze_frequent_itemsets não retornou DataFrame para 'df_for_db'.")
            logger.info(f"==== Etapa: {step_name} FALHOU ====")
            return None

        expected_db_cols = ['itemset_str', 'support', 'length', 'frequency_count']
        # Adicionar CONTEST_ID_COLUMN_NAME se sua tabela de itemsets frequentes o espera
        # Esta coluna é adicionada pela função get_frequent_itemsets em combination_analysis se necessário
        # Por isso, não adicionamos aqui, mas sim garantimos que ela exista se a tabela a espera.
        # No entanto, a lógica atual em combination_analysis.analyze_frequent_itemsets NÃO adiciona contest_id.
        # Se a tabela FREQUENT_ITEMSETS_TABLE_NAME precisa de contest_id, a lógica de adição deve estar
        # em combination_analysis.py ou aqui, se for um valor global (ex: último concurso).
        # Para itemsets históricos, a abordagem seria diferente.

        if df_for_db.empty:
            logger.info(f"{step_name}: Nenhum itemset frequente (DB) encontrado. Tabela '{frequent_itemsets_table_name}' vazia.")
            empty_db_df = pd.DataFrame(columns=expected_db_cols)
            db_manager.save_dataframe(empty_db_df, frequent_itemsets_table_name, if_exists='replace')
        else:
            cols_to_save_db = [col for col in expected_db_cols if col in df_for_db.columns]
            missing_expected_cols = [col for col in ['itemset_str', 'support', 'length'] if col not in cols_to_save_db]
            if missing_expected_cols:
                 logger.error(f"{step_name}: df_for_db não contém colunas essenciais: {missing_expected_cols}. Presentes: {df_for_db.columns}")
                 return None

            logger.info(f"{step_name}: Análise (DB) concluída. {len(df_for_db)} itemsets.")
            db_manager.save_dataframe(df_for_db[cols_to_save_db], frequent_itemsets_table_name, if_exists='replace')
            logger.info(f"{step_name}: Resultados (DB) salvos em '{frequent_itemsets_table_name}'.")

        shared_context['frequent_itemsets_df_for_db'] = df_for_db

        if not isinstance(df_raw_for_rules, pd.DataFrame):
            logger.error(f"{step_name}: analyze_frequent_itemsets não retornou DataFrame para 'df_raw_for_rules'.")
            logger.info(f"==== Etapa: {step_name} FALHOU (df_raw_for_rules) ====")
            return None

        if df_raw_for_rules.empty:
            logger.info(f"{step_name}: Nenhum itemset frequente (mlxtend bruto) encontrado.")
        else:
            logger.info(f"{step_name}: {len(df_raw_for_rules)} itemsets (mlxtend bruto) preparados.")

        shared_context['mlxtend_frequent_itemsets_df_for_rules'] = df_raw_for_rules
        logger.info(f"{step_name}: DataFrame (mlxtend bruto) adicionado ao shared_context.")

    except AttributeError as e:
        logger.error(f"Erro na etapa {step_name}: Atributo ausente. Detalhes: {e}", exc_info=True)
        return None
    except ValueError as e:
        logger.error(f"Erro na etapa {step_name}: Dados/parâmetros. Detalhes: {e}", exc_info=True)
        return None
    except Exception as e:
        logger.error(f"Erro inesperado na etapa {step_name}: {e}", exc_info=True)
        return None

    logger.info(f"==== Etapa: {step_name} CONCLUÍDA ====")
    return analyzer_instance

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_grid_analysis.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_grid_analysis.py
import logging
import pandas as pd
from typing import Any, Dict # Para os type hints dos argumentos

# Importa a função de análise principal
from src.analysis.grid_analysis import analyze_grid_distribution

logger = logging.getLogger(__name__)

def run_grid_analysis_step(
    all_data_df: pd.DataFrame,
    db_manager: Any,  # Espera-se uma instância de DatabaseManager
    config: Any,      # Espera-se uma instância de Config
    shared_context: Dict[str, Any], # Dicionário de contexto compartilhado
    **kwargs
) -> bool:
    """
    Executa a etapa de Análise de Distribuição por Linhas e Colunas.
    Calcula a frequência com que 0 a 5 dezenas são sorteadas em cada linha/coluna.
    Os argumentos são injetados pelo Orchestrator.
    """
    step_name = "Análise de Distribuição por Linhas e Colunas"
    logger.info(f"==== Iniciando Etapa: {step_name} ====")
    
    # Validações básicas dos argumentos e configurações injetados
    if not hasattr(config, 'DRAWN_NUMBERS_COLUMN_NAME') or \
       not hasattr(config, 'LOTOFACIL_GRID_LINES') or \
       not hasattr(config, 'LOTOFACIL_GRID_COLUMNS') or \
       not hasattr(config, 'GRID_LINE_DISTRIBUTION_TABLE_NAME') or \
       not hasattr(config, 'GRID_COLUMN_DISTRIBUTION_TABLE_NAME'):
        logger.error("Atributos de configuração necessários para a Análise de Linhas e Colunas não encontrados.")
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False

    if not isinstance(all_data_df, pd.DataFrame) or all_data_df.empty:
        logger.error("DataFrame de sorteios (all_data_df) injetado está inválido ou vazio.")
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False
    
    if not hasattr(db_manager, 'save_dataframe'):
        logger.error("Objeto db_manager injetado não possui o método 'save_dataframe'.")
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False

    line_table_name = config.GRID_LINE_DISTRIBUTION_TABLE_NAME
    column_table_name = config.GRID_COLUMN_DISTRIBUTION_TABLE_NAME

    try:
        logger.info("Calculando distribuição de frequência por linhas e colunas...")
        line_distribution_df, column_distribution_df = analyze_grid_distribution(all_data_df, config)

        # Validação dos DataFrames retornados
        if not isinstance(line_distribution_df, pd.DataFrame) or \
           not isinstance(column_distribution_df, pd.DataFrame):
            logger.error("A análise de grid não retornou os DataFrames esperados.")
            logger.info(f"==== Etapa: {step_name} FALHOU ====")
            return False
        
        # Salvar distribuição por linhas
        if line_distribution_df.empty:
            logger.info(f"DataFrame de distribuição por linhas está vazio. Nada será salvo na tabela '{line_table_name}'.")
        else:
            db_manager.save_dataframe(line_distribution_df, line_table_name, if_exists='replace')
            logger.info(f"Distribuição por linhas salva na tabela: {line_table_name}")
        
        shared_context['grid_line_distribution_df'] = line_distribution_df
        logger.info("Resultado da distribuição por linhas adicionado ao shared_context.")

        # Salvar distribuição por colunas
        if column_distribution_df.empty:
            logger.info(f"DataFrame de distribuição por colunas está vazio. Nada será salvo na tabela '{column_table_name}'.")
        else:
            db_manager.save_dataframe(column_distribution_df, column_table_name, if_exists='replace')
            logger.info(f"Distribuição por colunas salva na tabela: {column_table_name}")

        shared_context['grid_column_distribution_df'] = column_distribution_df
        logger.info("Resultado da distribuição por colunas adicionado ao shared_context.")

    except Exception as e:
        logger.error(f"Erro durante a execução da {step_name}: {e}", exc_info=True)
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False
            
    logger.info(f"==== Etapa: {step_name} CONCLUÍDA ====")
    return True

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_max_delay.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_max_delay.py
import pandas as pd
import logging
from typing import Any, Dict # Adicionado
from src.analysis.delay_analysis import calculate_max_delay
# from src.database_manager import DatabaseManager # Para type hint
# from src.config import Config # Para type hint

logger = logging.getLogger(__name__)

def run_max_delay_analysis_step(
    all_data_df: pd.DataFrame, 
    db_manager: Any, # DatabaseManager
    config: Any, # Config
    shared_context: Dict[str, Any], # Adicionado
    **kwargs
) -> bool:
    step_name = "Max Delay Analysis (Separate)"
    logger.info(f"Iniciando etapa: {step_name}.")
    try:
        max_delay_df = calculate_max_delay(all_data_df, config) # Passa config
        if max_delay_df is not None and not max_delay_df.empty:
            db_manager.save_dataframe(max_delay_df, 'atraso_maximo_separado', if_exists='replace')
            logger.info(f"Atraso máximo (etapa separada) salvo na tabela 'atraso_maximo_separado'.")
        else:
            logger.warning(f"Não foi possível calcular ou DataFrame de atraso máximo (etapa separada) vazio.")
            
        logger.info(f"Etapa: {step_name} concluída.")
        return True
    except Exception as e:
        logger.error(f"Erro na etapa {step_name}: {e}", exc_info=True)
        return False

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_metrics_viz.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_metrics_viz.py
import logging
from pathlib import Path
from typing import Optional, Any, Dict # Adicionado Any, Dict para type hints

# from src.database_manager import DatabaseManager # Para type hint
# from src.config import Config # Para type hint
# Importa as funções de plotagem específicas
from src.visualization.plotter import plot_frequency, plot_delay # Adicione outras se este step as usar
# Importa a configuração do diretório de plotagem
from src.config import PLOT_DIR_CONFIG # Para fallback se não vier do pipeline

logger = logging.getLogger(__name__)

# RENOMEADO PARA CORRESPONDER AO __init__.py e main.py
def run_metrics_visualization_step(
    db_manager: Any, # Tipo real: DatabaseManager
    config: Any, # Tipo real: Config
    shared_context: Dict[str, Any], # Para consistência com o Orchestrator
    # output_dir_from_pipeline: Optional[Path] = None, # Removido, pois o config.PLOT_DIR é o padrão
    **kwargs
) -> bool:
    """
    Executa a visualização das principais métricas (frequência, atraso).
    Os gráficos são salvos no diretório especificado por config.PLOT_DIR.
    """
    step_name = "Core Metrics Visualization"
    logger.info(f"Iniciando etapa do pipeline: {step_name}.")
    
    try:
        # O diretório de saída é agora obtido do objeto config
        # PLOT_DIR_CONFIG é uma constante de nível de módulo, mas config.PLOT_DIR é o atributo da instância
        output_dir_to_use = Path(config.PLOT_DIR) # Usando config.PLOT_DIR da instância config_obj
        output_dir_to_use.mkdir(parents=True, exist_ok=True)
        logger.info(f"Gráficos serão salvos em: {output_dir_to_use}")

        # Plotar Frequência Absoluta
        # Presume que db_manager.load_dataframe (ou um método similar) existe
        df_freq_abs = db_manager.load_dataframe('frequencia_absoluta') # Supondo nome da tabela
        if df_freq_abs is not None and not df_freq_abs.empty:
            plot_frequency(df_freq_abs, metric_type='Absoluta', output_dir=str(output_dir_to_use)) 
            logger.info("Gráfico de frequência absoluta gerado.")
        else:
            logger.warning("Dados de frequência absoluta não encontrados ou vazios. Gráfico não gerado.")

        # Plotar Atraso Atual
        df_delay_curr = db_manager.load_dataframe('atraso_atual') # Supondo nome da tabela
        if df_delay_curr is not None and not df_delay_curr.empty:
            plot_delay(df_delay_curr, delay_type='Atual', output_dir=str(output_dir_to_use))
            logger.info("Gráfico de atraso atual gerado.")
        else:
            logger.warning("Dados de atraso atual não encontrados ou vazios. Gráfico não gerado.")
            
        # Adicionar mais plots conforme necessário (ex: Atraso Máximo, Frequência Relativa)
        # Exemplo:
        # df_freq_rel = db_manager.load_dataframe('frequencia_relativa')
        # if df_freq_rel is not None and not df_freq_rel.empty:
        #     plot_frequency(df_freq_rel, metric_type='Relativa', output_dir=str(output_dir_to_use))
        #     logger.info("Gráfico de frequência relativa gerado.")
        # else:
        #     logger.warning("Dados de frequência relativa não encontrados. Gráfico não gerado.")

        logger.info(f"Etapa do pipeline: {step_name} concluída com sucesso.")
        return True
    except AttributeError as e:
        logger.error(f"Erro na etapa {step_name}: Atributo de configuração ausente (ex: config.PLOT_DIR). Detalhes: {e}", exc_info=True)
        return False
    except Exception as e:
        logger.error(f"Erro ao executar a etapa {step_name}: {e}", exc_info=True)
        return False

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_pairs.py
--------------------------------------------------------------------------------
# Lotofacil_Analysis/src/pipeline_steps/execute_pairs.py
import pandas as pd
import logging
from typing import Any, Dict

# Para type hints mais específicos:
from src.analysis.combination_analysis import CombinationAnalyzer
# from src.database_manager import DatabaseManager
# from src.config import Config

logger = logging.getLogger(__name__)

def run_pair_analysis_step(
    all_data_df: pd.DataFrame,
    db_manager: Any, # DatabaseManager
    config: Any, # Config
    shared_context: Dict[str, Any],
    combination_analyzer_instance: CombinationAnalyzer, # CORRIGIDO
    **kwargs
) -> bool:
    step_name = "Pair Analysis"
    logger.info(f"Iniciando etapa do pipeline: {step_name}")

    try:
        required_attrs = [
            'DRAWN_NUMBERS_COLUMN_NAME', 'CONTEST_ID_COLUMN_NAME',
            'ANALYSIS_PAIR_METRICS_TABLE_NAME'
        ]
        for attr in required_attrs:
            if not hasattr(config, attr):
                logger.error(f"{step_name}: Atributo de config '{attr}' não encontrado.")
                return False

        if combination_analyzer_instance is None:
            logger.error(f"{step_name}: Instância 'combination_analyzer_instance' é None.")
            return False # Falha crítica se o analyzer não estiver disponível
        
        if not hasattr(combination_analyzer_instance, 'analyze_pairs'):
             logger.error(f"{step_name}: 'combination_analyzer_instance' não possui método 'analyze_pairs'.")
             return False

        drawn_numbers_col = config.DRAWN_NUMBERS_COLUMN_NAME
        contest_id_col = config.CONTEST_ID_COLUMN_NAME
        pair_metrics_table_name = config.ANALYSIS_PAIR_METRICS_TABLE_NAME


        pairs_df = combination_analyzer_instance.analyze_pairs(
            all_draws_df=all_data_df,
            drawn_numbers_col=drawn_numbers_col,
            contest_id_col=contest_id_col
        )

        expected_cols = ['pair_str', 'frequency', 'last_contest', 'current_delay']
        if pairs_df is not None and not pairs_df.empty:
            cols_to_save = [col for col in expected_cols if col in pairs_df.columns]
            # Adiciona colunas faltantes com NA se não vierem da análise mas são esperadas na tabela
            for эко_col in expected_cols:
                if эко_col not in pairs_df.columns:
                    pairs_df[эко_col] = pd.NA # ou np.nan se float
            
            db_manager.save_dataframe(pairs_df[cols_to_save], pair_metrics_table_name, if_exists='replace')
            logger.info(f"Métricas de pares salvas na tabela '{pair_metrics_table_name}'.")
        else:
            logger.warning(f"{step_name}: Não foi possível calcular ou DataFrame de métricas de pares vazio.")
            db_manager.save_dataframe(pd.DataFrame(columns=expected_cols), pair_metrics_table_name, if_exists='replace')


        logger.info(f"Etapa do pipeline: {step_name} concluída.")
        return True
    except AttributeError as e:
        logger.error(f"Erro de atributo na etapa {step_name}: {e}", exc_info=True)
        return False
    except Exception as e:
        logger.error(f"Erro ao executar a etapa {step_name}: {e}", exc_info=True)
        return False

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_positional_analysis.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_positional_analysis.py
import logging
import pandas as pd
from typing import Any, Dict # Para os type hints dos argumentos
from src.analysis.positional_analysis import analyze_draw_position_frequency
# Nenhuma importação da classe SharedContext é necessária aqui

logger = logging.getLogger(__name__)

def run_positional_analysis_step(
    all_data_df: pd.DataFrame,
    db_manager: Any,  # Espera-se uma instância de DatabaseManager
    config: Any,      # Espera-se uma instância de Config
    shared_context: Dict[str, Any], # Dicionário de contexto compartilhado
    **kwargs
) -> bool:
    """
    Executa a etapa de análise de frequência posicional.
    Calcula e salva a frequência com que cada dezena aparece em cada
    posição de sorteio (1ª a 15ª bola).
    Os argumentos são injetados pelo Orchestrator.
    """
    step_name = "Análise de Frequência Posicional"
    logger.info(f"==== Iniciando Etapa: {step_name} ====")
    
    # Acessa config, db_manager, e all_data_df diretamente dos argumentos.
    # A constante DRAW_POSITION_FREQUENCY_TABLE_NAME é acessada via objeto config.
    # É importante que 'config' realmente seja uma instância da classe Config
    # e 'db_manager' uma instância de DatabaseManager para que os atributos
    # e métodos esperados (config.DRAW_POSITION_FREQUENCY_TABLE_NAME, db_manager.save_dataframe) existam.

    if not hasattr(config, 'DRAW_POSITION_FREQUENCY_TABLE_NAME'):
        logger.error("Atributo 'DRAW_POSITION_FREQUENCY_TABLE_NAME' não encontrado no objeto config injetado.")
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False

    draw_position_frequency_table_name = config.DRAW_POSITION_FREQUENCY_TABLE_NAME

    if not isinstance(all_data_df, pd.DataFrame) or all_data_df.empty:
        logger.error("DataFrame de sorteios (all_data_df) injetado está inválido ou vazio.")
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False
    
    if not hasattr(db_manager, 'save_dataframe'):
        logger.error("Objeto db_manager injetado não possui o método 'save_dataframe'.")
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False

    try:
        logger.info("Chamando analyze_draw_position_frequency...")
        # A função de análise recebe o DataFrame e o objeto config
        positional_freq_df = analyze_draw_position_frequency(all_data_df, config)
        
        if not isinstance(positional_freq_df, pd.DataFrame):
            logger.error("A análise de frequência posicional não retornou um DataFrame.")
            logger.info(f"==== Etapa: {step_name} FALHOU ====")
            return False

        if positional_freq_df.empty and not all_data_df.empty:
            logger.warning("A análise de frequência posicional resultou em um DataFrame vazio, embora os dados de entrada não estivessem vazios. Verifique os logs da função de análise.")
        
        if positional_freq_df.empty:
            logger.info(f"DataFrame de frequência posicional está vazio. Nada será salvo na tabela '{draw_position_frequency_table_name}'.")
        else:
            db_manager.save_dataframe(positional_freq_df, 
                                      draw_position_frequency_table_name, 
                                      if_exists='replace')
            logger.info(f"Frequência posicional salva na tabela: {draw_position_frequency_table_name}")
        
        # Adiciona dados de volta ao dicionário shared_context, seguindo o padrão
        shared_context['positional_frequency_df'] = positional_freq_df
        logger.info(f"Resultado da {step_name} adicionado ao dicionário shared_context como 'positional_frequency_df'.")

    except Exception as e:
        logger.error(f"Erro durante a execução da {step_name}: {e}", exc_info=True)
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False
            
    logger.info(f"==== Etapa: {step_name} CONCLUÍDA ====")
    return True

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_properties.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_properties.py
import pandas as pd
import logging
from typing import Any, Dict # Adicionado para type hints
from src.analysis.number_properties_analysis import analyze_number_properties
# from src.database_manager import DatabaseManager # Para type hint
# from src.config import Config # Para type hint

logger = logging.getLogger(__name__)

def run_number_properties_analysis(
    all_data_df: pd.DataFrame, 
    db_manager: Any, # DatabaseManager
    config: Any, # Config
    shared_context: Dict[str, Any], 
    **kwargs
) -> bool:
    step_name = "Number Properties Analysis" # Adicionado para clareza no log
    logger.info(f"Iniciando etapa: {step_name}.")
    try:
        if all_data_df.empty:
            logger.warning(f"DataFrame de dados (all_data_df) está vazio para a etapa {step_name}. Pulando.")
            return True 
            
        # CORRIGIDO: Passa config para a função de análise
        properties_by_contest_df = analyze_number_properties(all_data_df, config) 
        
        if properties_by_contest_df is not None and not properties_by_contest_df.empty:
            # O nome da tabela 'propriedades_numericas_por_concurso' é usado na função de análise
            # mas o save_dataframe usa o nome da tabela que você definir aqui ou no config.
            # Se analyze_number_properties já renomeia a coluna de concurso para "Concurso",
            # e a tabela no DB espera "Concurso", está OK.
            table_name = 'propriedades_numericas_por_concurso'
            db_manager.save_dataframe(properties_by_contest_df, table_name, if_exists='replace')
            logger.info(f"Análise de propriedades numéricas por concurso salva na tabela '{table_name}'.")
        else:
            logger.warning("Não foi possível calcular ou DataFrame de propriedades numéricas vazio.")
            
        logger.info(f"Etapa: {step_name} concluída.")
        return True
    except Exception as e:
        logger.error(f"Erro na etapa {step_name}: {e}", exc_info=True)
        return False

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_rank_trend_analysis.py
--------------------------------------------------------------------------------
# Lotofacil_Analysis/src/pipeline_steps/execute_rank_trend_analysis.py
import logging
import pandas as pd
from typing import Any, Dict, Optional
import numpy as np

# Para type hints mais específicos:
from src.config import Config
# from src.database_manager import DatabaseManager

from src.analysis.rank_trend_analysis import (
    calculate_and_persist_rank_per_chunk,
    calculate_historical_rank_trends
)

logger = logging.getLogger(__name__)

def run_rank_trend_analysis_step(
    db_manager: Any, # DatabaseManager
    config: Config,  # CORRIGIDO de config_param para config
    shared_context: Dict[str, Any],
    all_data_df: pd.DataFrame, # Assinatura alinhada com main.py
    **kwargs
) -> bool:
    step_name = "Rank Trend Metrics Analysis (Slope/Status)"
    logger.info(f"==== Iniciando Etapa: {step_name} ====")

    # CORREÇÃO: Usar o nome da constante como está no config.py
    block_agg_table_name_const = 'BLOCK_AGGREGATED_DATA_FOR_RANK_TREND_TABLE_NAME'

    required_attrs_for_rank_trend = [
        block_agg_table_name_const, # Nome da constante corrigido
        'RANK_ANALYSIS_TYPE_FILTER_FOR_TREND', 'RANK_VALUE_COLUMN_FOR_TREND',
        'RANK_TREND_WINDOW_BLOCKS', 'RANK_TREND_SLOPE_IMPROVING_THRESHOLD',
        'RANK_TREND_SLOPE_WORSENING_THRESHOLD', 'ANALYSIS_RANK_TREND_METRICS_TABLE_NAME',
        'ALL_NUMBERS', 'CONTEST_ID_COLUMN_NAME', 'CHUNK_TYPES_CONFIG',
        'EVOL_RANK_FREQUENCY_BLOCK_PREFIX' # Usado por calculate_and_persist_rank_per_chunk
    ]
    missing_attrs = [attr for attr in required_attrs_for_rank_trend if not hasattr(config, attr)]
    if missing_attrs:
        logger.error(f"{step_name}: Objeto 'config' não possui atributos: {missing_attrs}. Abortando.")
        return False

    try:
        logger.info("Sub-etapa: Gerando/Atualizando ranks de frequência por chunk.")
        calculate_and_persist_rank_per_chunk(db_manager, config) # Usar config injetado
        logger.info("Sub-etapa: Geração de ranks de frequência por chunk concluída.")

        logger.info("Sub-etapa: Calculando slope e status da tendência de rank.")
        aggregated_block_table_name = getattr(config, block_agg_table_name_const)
        
        rank_type_filter = config.RANK_ANALYSIS_TYPE_FILTER_FOR_TREND
        # A rank_value_column_name agora é usada para filtrar o tipo_analise,
        # mas a função calculate_historical_rank_trends itera sobre as dezenas.
        # rank_value_column_name não é o nome da coluna da dezena aqui, mas sim o tipo de rank.
        rank_value_col_identifier = config.RANK_VALUE_COLUMN_FOR_TREND # Ex: 'rank_no_bloco' (nome da métrica PIVOTADA)


        trend_window = config.RANK_TREND_WINDOW_BLOCKS
        improving_thresh = config.RANK_TREND_SLOPE_IMPROVING_THRESHOLD
        worsening_thresh = config.RANK_TREND_SLOPE_WORSENING_THRESHOLD

        if not db_manager.table_exists(aggregated_block_table_name):
            logger.error(f"Tabela base '{aggregated_block_table_name}' não encontrada. Verifique 'Block Aggregation'.")
            return False

        df_rank_trends = calculate_historical_rank_trends(
            db_manager=db_manager,
            config=config, 
            aggregated_block_table_name=aggregated_block_table_name,
            rank_analysis_type_filter=rank_type_filter, # Ex: "rank_freq_bloco"
            rank_value_column_name=rank_value_col_identifier, # Passa o identificador da coluna de rank
            trend_window_blocks=trend_window,
            slope_improving_threshold=improving_thresh,
            slope_worsening_threshold=worsening_thresh
        )

        output_table_name = config.ANALYSIS_RANK_TREND_METRICS_TABLE_NAME
        expected_cols = [config.CONTEST_ID_COLUMN_NAME, config.DEZENA_COLUMN_NAME, config.RANK_SLOPE_COLUMN_NAME, config.TREND_STATUS_COLUMN_NAME]

        if df_rank_trends is None or df_rank_trends.empty:
            logger.warning(f"Nenhum dado de tendência de rank gerado. Tabela '{output_table_name}' vazia.")
            empty_trends_df = pd.DataFrame(columns=expected_cols)
            db_manager.save_dataframe(empty_trends_df, output_table_name, if_exists='replace')
        else:
            # Assegurar colunas e ordem e tipos corretos
            final_df = pd.DataFrame(columns=expected_cols) # Cria com ordem correta
            for col in expected_cols:
                if col in df_rank_trends.columns:
                    final_df[col] = df_rank_trends[col]
                else: # Preenche com defaults apropriados se alguma coluna estiver faltando no resultado
                    if col == config.RANK_SLOPE_COLUMN_NAME: final_df[col] = np.nan
                    elif col == config.TREND_STATUS_COLUMN_NAME: final_df[col] = 'indefinido'
                    elif col == config.CONTEST_ID_COLUMN_NAME: final_df[col] = pd.NA # Ou um ID default
                    elif col == config.DEZENA_COLUMN_NAME: final_df[col] = pd.NA # Ou um ID default

            db_manager.save_dataframe(final_df, output_table_name, if_exists='replace')
            logger.info(f"Métricas de tendência de rank salvas na '{output_table_name}' ({len(final_df)}).")

        logger.info(f"==== Etapa: {step_name} CONCLUÍDA ====")
        return True

    except AttributeError as ae:
        logger.error(f"Erro de atributo em {step_name} (verifique config): {ae}", exc_info=True)
        return False
    except Exception as e:
        logger.error(f"Erro crítico na etapa {step_name}: {e}", exc_info=True)
        return False

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_recurrence_analysis.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_recurrence_analysis.py
import logging
import pandas as pd
from typing import Any, Dict, List, Optional

from src.config import Config
from src.database_manager import DatabaseManager

from src.analysis.recurrence_analysis import analyze_recurrence
from src.analysis.delay_analysis import get_draw_matrix 

logger = logging.getLogger(__name__)

def run_recurrence_analysis_step(
    all_data_df: pd.DataFrame,
    db_manager: DatabaseManager,
    config: Config,
    shared_context: Dict[str, Any],
    force_full_recalculation: bool = False, # Adicionado para controle incremental
    **kwargs
) -> bool:
    step_name = "Recurrence Analysis (Historical CDF Incremental)"
    logger.info(f"==== Iniciando Etapa: {step_name} ====")

    required_attrs = [
        'CONTEST_ID_COLUMN_NAME', 'DEZENA_COLUMN_NAME', 'ALL_NUMBERS',
        'ANALYSIS_RECURRENCE_CDF_TABLE_NAME', 'RECURRENCE_CDF_COLUMN_NAME',
        'ANALYSIS_DELAYS_TABLE_NAME', 'CURRENT_DELAY_COLUMN_NAME',
        'MIN_CONTESTS_FOR_HISTORICAL_RECURRENCE'
    ]
    for attr in required_attrs:
        if not hasattr(config, attr):
            logger.error(f"{step_name}: Atributo de config '{attr}' não encontrado. Abortando.")
            return False

    if not isinstance(all_data_df, pd.DataFrame) or all_data_df.empty:
        logger.warning(f"{step_name}: DataFrame de sorteios (all_data_df) inválido ou vazio. Etapa pulada.")
        return True 

    contest_id_col = config.CONTEST_ID_COLUMN_NAME
    dezena_col = config.DEZENA_COLUMN_NAME
    recurrence_cdf_col = config.RECURRENCE_CDF_COLUMN_NAME
    delays_table_name = config.ANALYSIS_DELAYS_TABLE_NAME
    table_name_to_save = config.ANALYSIS_RECURRENCE_CDF_TABLE_NAME
    min_hist_contests = config.MIN_CONTESTS_FOR_HISTORICAL_RECURRENCE

    start_processing_from_contest_id = 1
    if_exists_mode = 'replace'

    # Lógica Incremental
    if not force_full_recalculation and db_manager.table_exists(table_name_to_save):
        last_processed_query = f"SELECT MAX({contest_id_col}) FROM {table_name_to_save}"
        last_processed_df = db_manager.execute_query(last_processed_query)
        if last_processed_df is not None and not last_processed_df.empty and pd.notna(last_processed_df.iloc[0,0]):
            last_saved_contest = int(last_processed_df.iloc[0,0])
            max_contest_in_data = all_data_df[contest_id_col].max() # Último concurso nos dados de entrada
            if max_contest_in_data > last_saved_contest:
                start_processing_from_contest_id = last_saved_contest + 1
                if_exists_mode = 'append'
                logger.info(f"{step_name}: Tabela '{table_name_to_save}' existente. Último concurso processado: {last_saved_contest}. Iniciando incremental de {start_processing_from_contest_id}.")
            else:
                logger.info(f"{step_name}: Tabela '{table_name_to_save}' já está atualizada até o concurso {last_saved_contest}. Nenhum novo concurso para processar.")
                return True # Nada a fazer
        else:
            logger.info(f"{step_name}: Tabela '{table_name_to_save}' existe mas está vazia ou sem {contest_id_col}. Recalculando tudo.")
            # if_exists_mode permanece 'replace'
    else:
        logger.info(f"{step_name}: {'--force-reload ativo' if force_full_recalculation else 'Tabela de destino não existe'}. Recalculando tudo para '{table_name_to_save}'.")
        # if_exists_mode permanece 'replace'

    all_contest_ids_in_data = sorted(all_data_df[contest_id_col].unique())
    
    target_contest_ids_to_calculate = [
        cid for i, cid in enumerate(all_contest_ids_in_data) 
        if cid >= start_processing_from_contest_id and i >= min_hist_contests - 1
    ]
    
    if not target_contest_ids_to_calculate:
        logger.info(f"{step_name}: Nenhum novo concurso para processar após filtros (a partir de {start_processing_from_contest_id}, mínimo histórico {min_hist_contests}).")
        return True
        
    historical_recurrence_data: List[pd.DataFrame] = []
    total_points_to_process = len(target_contest_ids_to_calculate)
    log_interval = max(1, total_points_to_process // 20) if total_points_to_process > 100 else 1
    logger.info(f"{step_name}: Processamento de recorrência (após {min_hist_contests-1} concursos iniciais ou a partir de {start_processing_from_contest_id}) para {total_points_to_process} pontos.")

    processed_points_count = 0
    for i, current_max_contest_id in enumerate(target_contest_ids_to_calculate):
        if (i + 1) % log_interval == 0 or i == 0 or i == total_points_to_process - 1:
            logger.info(f"{step_name}: Progresso - {i+1}/{total_points_to_process} (concurso de corte: {current_max_contest_id})")
        
        # df_upto_contest AINDA é todo o histórico até current_max_contest_id para o cálculo correto de gaps
        df_upto_contest = all_data_df[all_data_df[contest_id_col] <= current_max_contest_id].copy()

        if df_upto_contest.empty or len(df_upto_contest) < min_hist_contests:
            logger.debug(f"Dados insuficientes até concurso {current_max_contest_id} (linhas: {len(df_upto_contest)}).")
            continue

        try:
            draw_matrix = get_draw_matrix(df_upto_contest, config)
            if draw_matrix.empty:
                logger.warning(f"Matriz de sorteios vazia para concurso {current_max_contest_id}.")
                continue # Ou adicione um registro default como em execute_delay

            query_delays = f"SELECT {config.DEZENA_COLUMN_NAME} AS dezena, {config.CURRENT_DELAY_COLUMN_NAME} AS current_delay FROM {delays_table_name} WHERE {contest_id_col} = ?"
            current_delays_df_for_contest = db_manager.execute_query(query_delays, params=(current_max_contest_id,))

            if current_delays_df_for_contest is None or current_delays_df_for_contest.empty:
                logger.debug(f"Atrasos atuais não encontrados para concurso {current_max_contest_id}. Usando default (0).") # MUDADO PARA DEBUG
                current_delays_df_for_contest = pd.DataFrame({
                    config.DEZENA_COLUMN_NAME: config.ALL_NUMBERS, 'current_delay': 0
                })
            
            if config.DEZENA_COLUMN_NAME not in current_delays_df_for_contest.columns or \
               'current_delay' not in current_delays_df_for_contest.columns:
                logger.error(f"Colunas '{config.DEZENA_COLUMN_NAME}' ou 'current_delay' não encontradas nos atrasos do concurso {current_max_contest_id}.")
                continue

            recurrence_stats_df_raw = analyze_recurrence(draw_matrix, current_delays_df_for_contest, config)

            df_to_append = pd.DataFrame() # Inicializa df_to_append
            if recurrence_stats_df_raw is None or recurrence_stats_df_raw.empty:
                logger.warning(f"Análise de recorrência vazia para concurso {current_max_contest_id}.")
                df_to_append = pd.DataFrame({
                    contest_id_col: current_max_contest_id,
                    config.DEZENA_COLUMN_NAME: config.ALL_NUMBERS,
                    recurrence_cdf_col: pd.NA 
                })
            else:
                if config.DEZENA_COLUMN_NAME not in recurrence_stats_df_raw.columns or 'CDF_Atraso_Atual' not in recurrence_stats_df_raw.columns:
                     logger.warning(f"Colunas esperadas não em recurrence_stats_df_raw para {current_max_contest_id}.")
                     df_to_append = pd.DataFrame({
                        contest_id_col: current_max_contest_id,
                        config.DEZENA_COLUMN_NAME: config.ALL_NUMBERS,
                        recurrence_cdf_col: pd.NA
                     })
                else:
                    df_to_append = recurrence_stats_df_raw[[config.DEZENA_COLUMN_NAME, 'CDF_Atraso_Atual']].copy()
                    df_to_append.rename(columns={'CDF_Atraso_Atual': recurrence_cdf_col}, inplace=True)
                    df_to_append[contest_id_col] = current_max_contest_id
            
            df_to_append[config.DEZENA_COLUMN_NAME] = df_to_append[config.DEZENA_COLUMN_NAME].astype(int)
            cols_order = [contest_id_col, config.DEZENA_COLUMN_NAME, recurrence_cdf_col]
            
            for col_name_check in cols_order:
                if col_name_check not in df_to_append.columns:
                    if col_name_check == recurrence_cdf_col: df_to_append[col_name_check] = pd.NA
            
            df_to_append = df_to_append[cols_order]
            df_to_append[recurrence_cdf_col] = pd.to_numeric(df_to_append[recurrence_cdf_col], errors='coerce')

            historical_recurrence_data.append(df_to_append)
            processed_points_count +=1

        except Exception as e_inner:
            logger.error(f"Erro ao processar recorrência para concurso {current_max_contest_id}: {e_inner}", exc_info=True)

    if not historical_recurrence_data:
        logger.info(f"{step_name}: Nenhum novo dado de recorrência histórica foi gerado para o intervalo solicitado.")
        # Não é um erro se não havia nada novo para processar
        # (ex: start_processing_from_contest_id > último concurso nos dados, ou target_contest_ids_to_calculate ficou vazio)
        return True 

    final_historical_df = pd.concat(historical_recurrence_data, ignore_index=True)

    try:
        db_manager.save_dataframe(final_historical_df, table_name_to_save, if_exists=if_exists_mode)
        logger.info(f"Dados de recorrência CDF ({len(final_historical_df)} linhas) salvos em '{table_name_to_save}' (modo: {if_exists_mode}).")
        logger.info(f"==== Etapa: {step_name} CONCLUÍDA ====")
        return True
    except Exception as e:
        logger.error(f"Erro na etapa {step_name} ao salvar dados: {e}", exc_info=True)
        return False

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_repetition_analysis.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_repetition_analysis.py
import logging
import pandas as pd
from typing import Any, Dict # Adicionado
from src.analysis.repetition_analysis import calculate_previous_draw_repetitions
# from src.database_manager import DatabaseManager # Para type hint
# from src.config import Config # Para type hint

logger = logging.getLogger(__name__)

def run_repetition_analysis_step(
    all_data_df: pd.DataFrame, 
    db_manager: Any, # DatabaseManager
    config: Any, # Config
    shared_context: Dict[str, Any], # Adicionado
    **kwargs
) -> bool:
    step_name = "Previous Draw Repetition Analysis"
    logger.info(f"Iniciando etapa: {step_name}.")
    try:
        if all_data_df.empty or len(all_data_df) < 2:
            logger.warning(f"DataFrame de dados (all_data_df) insuficiente para {step_name}. Pulando.")
            return True 
            
        # CORRIGIDO: Passa config para a função de análise
        df_repetitions = calculate_previous_draw_repetitions(all_data_df, config) 
        
        if df_repetitions is not None and not df_repetitions.empty:
            table_name = "analise_repeticao_concurso_anterior"
            db_manager.save_dataframe(df_repetitions, table_name, if_exists='replace')
            logger.info(f"Dados de repetição salvos na tabela '{table_name}' ({len(df_repetitions)} registros).")
        else:
            logger.warning("Nenhum dado de repetição foi gerado ou o DataFrame resultante está vazio.")
        
        logger.info(f"Etapa: {step_name} concluída.")
        return True # Retorna True mesmo se nada for salvo, pois a análise rodou.
    except Exception as e:
        logger.error(f"Erro na etapa {step_name}: {e}", exc_info=True)
        return False

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_seasonality_analysis.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_seasonality_analysis.py
import logging
import pandas as pd
from typing import Any, Dict 

# Importa as funções de análise do módulo de sazonalidade
from src.analysis.seasonality_analysis import (
    analyze_monthly_number_frequency,
    analyze_monthly_draw_properties # Nova importação
)

logger = logging.getLogger(__name__)

def run_seasonality_analysis_step(
    all_data_df: pd.DataFrame,
    db_manager: Any, 
    config: Any,      
    shared_context: Dict[str, Any], 
    **kwargs
) -> bool:
    """
    Executa a etapa de Análise Sazonal, incluindo:
    1. Frequência de Dezenas por Mês.
    2. Sumário de Propriedades Numéricas dos Sorteios por Mês.
    Os argumentos são injetados pelo Orchestrator.
    """
    step_name = "Análise Sazonal" # Nome geral da etapa
    logger.info(f"==== Iniciando Etapa: {step_name} ====")
    
    # Validações básicas (podem ser expandidas conforme necessário)
    if not isinstance(all_data_df, pd.DataFrame) or all_data_df.empty:
        logger.error("DataFrame de sorteios (all_data_df) injetado está inválido ou vazio.")
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False
    if not hasattr(db_manager, 'save_dataframe') or not hasattr(db_manager, 'load_dataframe'):
        logger.error("Objeto db_manager injetado não possui os métodos 'save_dataframe' ou 'load_dataframe'.")
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False
    if not config:
        logger.error("Objeto config não foi injetado corretamente.")
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False

    # --- Sub-etapa 1: Frequência Mensal de Dezenas ---
    try:
        sub_step_name_freq = "Frequência Mensal de Dezenas"
        logger.info(f"--- Iniciando sub-etapa: {sub_step_name_freq} ---")

        if not hasattr(config, 'MONTHLY_NUMBER_FREQUENCY_TABLE_NAME'):
            logger.error("Config 'MONTHLY_NUMBER_FREQUENCY_TABLE_NAME' não encontrada.")
            raise AttributeError("Configuração para tabela de frequência mensal ausente.")

        monthly_freq_table_name = config.MONTHLY_NUMBER_FREQUENCY_TABLE_NAME
        
        monthly_frequency_df = analyze_monthly_number_frequency(all_data_df, config)

        if not isinstance(monthly_frequency_df, pd.DataFrame):
            logger.error("A análise de frequência mensal não retornou um DataFrame.")
            raise TypeError("Resultado inesperado da análise de frequência mensal.")
        
        if monthly_frequency_df.empty and not all_data_df.empty:
            logger.warning(f"DataFrame de frequência mensal vazio. Nada será salvo na tabela '{monthly_freq_table_name}'.")
        elif not monthly_frequency_df.empty:
            db_manager.save_dataframe(monthly_frequency_df, monthly_freq_table_name, if_exists='replace')
            logger.info(f"Frequência mensal das dezenas salva na tabela: {monthly_freq_table_name}")
        
        shared_context['monthly_number_frequency_df'] = monthly_frequency_df
        logger.info(f"Resultado da {sub_step_name_freq} adicionado ao shared_context.")
        logger.info(f"--- Sub-etapa: {sub_step_name_freq} CONCLUÍDA ---")

    except Exception as e_freq:
        logger.error(f"Erro durante a sub-etapa {sub_step_name_freq}: {e_freq}", exc_info=True)
        # Decide se continua para a próxima sub-etapa ou falha a etapa inteira
        # Por ora, vamos logar e tentar continuar. Se for crítico, retorne False.


    # --- Sub-etapa 2: Sumário Mensal de Propriedades Numéricas ---
    try:
        sub_step_name_props = "Sumário Mensal de Propriedades Numéricas"
        logger.info(f"--- Iniciando sub-etapa: {sub_step_name_props} ---")

        if not hasattr(config, 'MONTHLY_DRAW_PROPERTIES_TABLE_NAME'):
            logger.error("Config 'MONTHLY_DRAW_PROPERTIES_TABLE_NAME' não encontrada.")
            raise AttributeError("Configuração para tabela de propriedades mensais ausente.")
        
        # Nome da tabela de onde carregar as propriedades por concurso
        # (Assumindo que esta tabela é gerada por uma etapa anterior como 'number_properties')
        props_per_concurso_table_name = getattr(config, 'PROPRIEDADES_NUMERICAS_TABLE_NAME', 'propriedades_numericas_por_concurso')
        
        if not db_manager.table_exists(props_per_concurso_table_name):
            logger.error(f"Tabela de propriedades por concurso '{props_per_concurso_table_name}' não existe. Etapa 'number_properties' é pré-requisito.")
            raise FileNotFoundError(f"Tabela '{props_per_concurso_table_name}' não encontrada.")

        properties_df = db_manager.load_dataframe(props_per_concurso_table_name)
        if not isinstance(properties_df, pd.DataFrame) or properties_df.empty:
            logger.error(f"DataFrame da tabela '{props_per_concurso_table_name}' vazio ou inválido.")
            raise ValueError(f"Dados de propriedades numéricas por concurso inválidos para {sub_step_name_props}.")

        monthly_props_table_name = config.MONTHLY_DRAW_PROPERTIES_TABLE_NAME
        monthly_draw_properties_df = analyze_monthly_draw_properties(all_data_df, properties_df, config)

        if not isinstance(monthly_draw_properties_df, pd.DataFrame):
            logger.error("A análise de propriedades mensais não retornou um DataFrame.")
            raise TypeError("Resultado inesperado da análise de propriedades mensais.")

        if monthly_draw_properties_df.empty:
            logger.info(f"DataFrame de sumário de propriedades mensais vazio. Nada será salvo na tabela '{monthly_props_table_name}'.")
        else:
            db_manager.save_dataframe(monthly_draw_properties_df, monthly_props_table_name, if_exists='replace')
            logger.info(f"Sumário de propriedades mensais salvo na tabela: {monthly_props_table_name}")
            
        shared_context['monthly_draw_properties_df'] = monthly_draw_properties_df
        logger.info(f"Resultado do {sub_step_name_props} adicionado ao shared_context.")
        logger.info(f"--- Sub-etapa: {sub_step_name_props} CONCLUÍDA ---")

    except Exception as e_props:
        logger.error(f"Erro durante a sub-etapa {sub_step_name_props}: {e_props}", exc_info=True)
        # Se esta sub-etapa falhar, a etapa geral pode ser considerada falha
        logger.info(f"==== Etapa: {step_name} FALHOU (devido a erro na sub-etapa de propriedades) ====")
        return False
            
    logger.info(f"==== Etapa: {step_name} CONCLUÍDA ====")
    return True

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_sequence_analysis.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_sequence_analysis.py
import pandas as pd
from typing import Dict, Any
import logging

# Importações robustas
try:
    from ..analysis.sequence_analysis import analyze_sequences
    from ..config import Config # Usaremos config_obj que é uma instância de Config
    from ..database_manager import DatabaseManager
except ImportError:
    from src.analysis.sequence_analysis import analyze_sequences
    from src.config import Config
    from src.database_manager import DatabaseManager

logger = logging.getLogger(__name__)

def run_sequence_analysis_step(
    all_data_df: pd.DataFrame, 
    db_manager: DatabaseManager, 
    config: Config, # Espera a instância config_obj
    shared_context: Dict[str, Any], # Para consistência e uso futuro se necessário
    **kwargs 
) -> None:
    """
    Executa a análise de sequências numéricas e salva os resultados.
    """
    logger.info("Iniciando etapa do pipeline: Análise de Sequências Numéricas.")

    if all_data_df is None or all_data_df.empty:
        logger.error("DataFrame 'all_data_df' é crucial e não está disponível ou está vazio. Abortando etapa de análise de sequências.")
        return
    
    # A função analyze_sequences já verifica a coluna 'drawn_numbers'
    # e as configurações de SEQUENCE_ANALYSIS_CONFIG dentro do objeto config.
    
    try:
        logger.info("Calculando métricas de sequências numéricas...")
        sequence_metrics_df = analyze_sequences(
            all_data_df.copy(), # Passa uma cópia para a análise, caso ela modifique o df
            config # Passa o objeto config_obj
        )
    except ValueError as ve: 
        logger.error(f"Erro de valor durante a análise de sequências: {ve}", exc_info=False) # Não mostrar traceback completo para ValueError esperado
        return
    except Exception as e:
        logger.error(f"Erro inesperado durante a análise de sequências: {e}", exc_info=True)
        return

    # Nome da tabela obtido do config
    output_table_name = getattr(config, 'SEQUENCE_METRICS_TABLE_NAME', 'sequence_metrics') # Default para 'sequence_metrics'

    if sequence_metrics_df is not None and not sequence_metrics_df.empty:
        try:
            db_manager.save_dataframe(sequence_metrics_df, output_table_name, if_exists="replace")
            logger.info(f"Métricas de sequências numéricas salvas na tabela '{output_table_name}'. {len(sequence_metrics_df)} registros.")
            # Adicionar ao shared_context se outras etapas precisarem
            if shared_context is not None:
                 shared_context[output_table_name + '_df'] = sequence_metrics_df
        except Exception as e:
            logger.error(f"Erro ao salvar métricas de sequências na tabela '{output_table_name}': {e}", exc_info=True)
    else:
        logger.info("Nenhuma métrica de sequência numérica foi gerada ou retornada para salvar (pode ser devido à configuração 'active': False ou nenhum dado).")

    logger.info("Etapa do pipeline: Análise de Sequências Numéricas concluída.")

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_statistical_tests.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_statistical_tests.py
import logging
import pandas as pd
from typing import Any, Dict, List 

# Importa as funções de análise
from src.analysis.statistical_tests_analysis import (
    perform_chi_square_test_number_frequencies,
    perform_normality_test_for_sum_of_numbers,
    perform_poisson_distribution_test # Nova importação
)

logger = logging.getLogger(__name__)

def run_statistical_tests_step(
    all_data_df: pd.DataFrame,
    db_manager: Any, 
    config: Any,      
    shared_context: Dict[str, Any], 
    **kwargs
) -> bool:
    """
    Executa a etapa de Testes Estatísticos, incluindo:
    1. Teste Qui-Quadrado para uniformidade da frequência das dezenas.
    2. Teste de Normalidade para a soma das dezenas sorteadas (Qui-Quadrado e K-S).
    3. Teste de Aderência à Distribuição de Poisson para contagens de eventos configurados.
    Os argumentos são injetados pelo Orchestrator.
    """
    step_name = "Testes Estatísticos"
    logger.info(f"==== Iniciando Etapa: {step_name} ====")
    
    # Validações básicas
    if not hasattr(config, 'STATISTICAL_TESTS_RESULTS_TABLE_NAME') or \
       not hasattr(config, 'NUMBERS_PER_DRAW') or \
       not hasattr(config, 'ALL_NUMBERS') or \
       not hasattr(config, 'SUM_NORMALITY_TEST_BINS') or \
       not hasattr(config, 'POISSON_DISTRIBUTION_TEST_CONFIG'): # Nova verificação
        logger.error("Atributos de configuração necessários para Testes Estatísticos não encontrados.")
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False

    if not isinstance(all_data_df, pd.DataFrame) or all_data_df.empty:
        logger.error("DataFrame de sorteios (all_data_df) injetado está inválido ou vazio.")
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False
    
    if not hasattr(db_manager, 'save_dataframe') or \
       not hasattr(db_manager, 'load_dataframe') or \
       not hasattr(db_manager, 'table_exists'):
        logger.error("Objeto db_manager injetado não possui os métodos necessários.")
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False

    statistical_tests_table_name = config.STATISTICAL_TESTS_RESULTS_TABLE_NAME
    all_test_results: List[Dict[str, Any]] = [] 

    # --- Teste 1: Qui-Quadrado para Frequência das Dezenas ---
    try:
        sub_step_name_freq = "Qui-Quadrado de Uniformidade da Frequência das Dezenas"
        logger.info(f"--- Iniciando sub-etapa: {sub_step_name_freq} ---")
        
        freq_abs_table_name = "frequencia_absoluta" 
        if not db_manager.table_exists(freq_abs_table_name):
            logger.error(f"Tabela '{freq_abs_table_name}' não existe. Etapa 'frequency_analysis' é pré-requisito.")
            raise FileNotFoundError(f"Tabela '{freq_abs_table_name}' não encontrada.")
        
        observed_frequencies_df = db_manager.load_dataframe(freq_abs_table_name)
        if not isinstance(observed_frequencies_df, pd.DataFrame) or observed_frequencies_df.empty:
            logger.error(f"DataFrame de frequência absoluta da tabela '{freq_abs_table_name}' vazio ou inválido.")
            raise ValueError(f"Dados de frequência absoluta inválidos.")
        
        if not all(col in observed_frequencies_df.columns for col in ['Dezena', 'Frequencia Absoluta']):
            logger.error(f"Tabela '{freq_abs_table_name}' não possui colunas 'Dezena' e 'Frequencia Absoluta'.")
            raise ValueError(f"Colunas esperadas ausentes em '{freq_abs_table_name}'.")

        total_draws = len(all_data_df)
        if total_draws == 0:
            logger.warning("Total de sorteios é 0. Pulando teste Qui-Quadrado de frequência.")
        else:
            chi_square_freq_result = perform_chi_square_test_number_frequencies(
                observed_frequencies_df, total_draws, config
            )
            if chi_square_freq_result:
                all_test_results.append(chi_square_freq_result)
                logger.info(f"Resultado do {sub_step_name_freq} obtido.")
            else:
                logger.warning(f"Falha ao obter resultado para {sub_step_name_freq}.")
        logger.info(f"--- Sub-etapa: {sub_step_name_freq} CONCLUÍDA ---")

    except Exception as e_freq_test:
        logger.error(f"Erro durante a sub-etapa {sub_step_name_freq}: {e_freq_test}", exc_info=True)

    # --- Teste 2: Teste de Normalidade para a Soma das Dezenas ---
    # Tabela de propriedades numéricas é carregada uma vez aqui
    props_table_name = getattr(config, 'PROPRIEDADES_NUMERICAS_TABLE_NAME', 'propriedades_numericas_por_concurso')
    properties_df = None
    if db_manager.table_exists(props_table_name):
        properties_df = db_manager.load_dataframe(props_table_name)
    
    if not isinstance(properties_df, pd.DataFrame) or properties_df.empty:
        logger.warning(f"DataFrame da tabela '{props_table_name}' vazio, inválido ou não existe. Pulando testes que dependem dele (Normalidade da Soma, Poisson).")
    else:
        sum_column_name = 'soma_dezenas'
        if sum_column_name not in properties_df.columns:
            logger.warning(f"Coluna '{sum_column_name}' não encontrada na tabela '{props_table_name}'. Pulando teste de normalidade da soma.")
        else:
            sum_of_numbers_series = properties_df[sum_column_name].dropna()
            if sum_of_numbers_series.empty or sum_of_numbers_series.nunique() < 2:
                logger.warning(f"Série de '{sum_column_name}' está vazia ou sem variação. Pulando testes de normalidade da soma.")
            else:
                # Teste com Qui-Quadrado (bins)
                try:
                    logger.info("--- Iniciando sub-etapa: Teste de Normalidade da Soma (Qui-Quadrado Bins) ---")
                    sum_normality_chi2_result = perform_normality_test_for_sum_of_numbers(
                        sum_of_numbers_series, config, method='chi_square_bins'
                    )
                    if sum_normality_chi2_result:
                        all_test_results.append(sum_normality_chi2_result)
                        logger.info("Resultado do Teste de Normalidade da Soma (Qui-Quadrado com Bins) obtido.")
                    else:
                        logger.warning("Falha ao obter resultado para Teste de Normalidade da Soma (Qui-Quadrado com Bins).")
                    logger.info("--- Sub-etapa: Teste de Normalidade da Soma (Qui-Quadrado Bins) CONCLUÍDA ---")
                except Exception as e_sum_norm_chi2:
                    logger.error(f"Erro no Teste de Normalidade da Soma (Qui-Quadrado Bins): {e_sum_norm_chi2}", exc_info=True)

                # Teste com Kolmogorov-Smirnov
                try:
                    logger.info("--- Iniciando sub-etapa: Teste de Normalidade da Soma (Kolmogorov-Smirnov) ---")
                    sum_normality_ks_result = perform_normality_test_for_sum_of_numbers(
                        sum_of_numbers_series, config, method='kolmogorov_smirnov'
                    )
                    if sum_normality_ks_result:
                        all_test_results.append(sum_normality_ks_result)
                        logger.info("Resultado do Teste de Normalidade da Soma (Kolmogorov-Smirnov) obtido.")
                    else:
                        logger.warning("Falha ao obter resultado para Teste de Normalidade da Soma (Kolmogorov-Smirnov).")
                    logger.info("--- Sub-etapa: Teste de Normalidade da Soma (Kolmogorov-Smirnov) CONCLUÍDA ---")
                except Exception as e_sum_norm_ks:
                    logger.error(f"Erro no Teste de Normalidade da Soma (Kolmogorov-Smirnov): {e_sum_norm_ks}", exc_info=True)

        # --- Teste 3: Testes de Aderência à Distribuição de Poisson ---
        poisson_test_config = getattr(config, 'POISSON_DISTRIBUTION_TEST_CONFIG', {})
        if not poisson_test_config:
            logger.info("Nenhuma configuração para testes de Poisson encontrada. Pulando.")
        else:
            for event_key, event_params in poisson_test_config.items():
                try:
                    col_name_for_poisson = event_params.get("column_name")
                    event_desc_for_log = event_params.get("event_description", event_key) # Usa event_key como fallback
                    
                    sub_step_name_poisson = f"Teste de Poisson para '{event_desc_for_log}'"
                    logger.info(f"--- Iniciando sub-etapa: {sub_step_name_poisson} ---")

                    if not col_name_for_poisson:
                        logger.warning(f"Nome da coluna não especificado para o teste de Poisson '{event_key}'. Pulando este teste.")
                        continue
                    
                    if col_name_for_poisson not in properties_df.columns:
                        logger.warning(f"Coluna '{col_name_for_poisson}' para o teste de Poisson '{event_key}' não encontrada na tabela '{props_table_name}'. Pulando.")
                        continue
                    
                    observed_event_counts_series = properties_df[col_name_for_poisson].dropna()

                    if observed_event_counts_series.empty:
                        logger.warning(f"Série de contagens para '{col_name_for_poisson}' (evento '{event_key}') está vazia. Pulando teste de Poisson.")
                        continue
                    
                    # Passa a descrição do evento para a função de análise, se não já em event_params
                    if "event_description" not in event_params:
                         event_params_copy = event_params.copy()
                         event_params_copy["event_description"] = event_desc_for_log # Adiciona descrição para logs dentro da função
                    else:
                         event_params_copy = event_params

                    poisson_result = perform_poisson_distribution_test(
                        observed_event_counts_series, event_params_copy # Passa a cópia com a descrição
                    )
                    if poisson_result:
                        all_test_results.append(poisson_result)
                        logger.info(f"Resultado do {sub_step_name_poisson} obtido.")
                    else:
                        logger.warning(f"Falha ao obter resultado para {sub_step_name_poisson}.")
                    logger.info(f"--- Sub-etapa: {sub_step_name_poisson} CONCLUÍDA ---")
                except Exception as e_poisson_test:
                    logger.error(f"Erro durante a sub-etapa {sub_step_name_poisson}: {e_poisson_test}", exc_info=True)


    # Salvar todos os resultados de testes coletados
    if not all_test_results:
        logger.warning(f"Nenhum resultado de teste estatístico foi gerado para a etapa {step_name}.")
    else:
        results_df_to_save = pd.DataFrame(all_test_results)
        try:
            db_manager.save_dataframe(results_df_to_save, 
                                      statistical_tests_table_name, 
                                      if_exists='append')
            logger.info(f"Resultados dos testes estatísticos ({len(results_df_to_save)} testes) salvos na tabela: {statistical_tests_table_name}")
        except Exception as e_save:
            logger.error(f"Erro ao salvar resultados dos testes estatísticos: {e_save}", exc_info=True)
            logger.info(f"==== Etapa: {step_name} FALHOU (ao salvar resultados) ====")
            return False # Considera falha da etapa se não conseguir salvar
            
    shared_context['statistical_tests_results_list'] = all_test_results
    logger.info(f"Lista de resultados dos Testes Estatísticos adicionada ao shared_context.")
            
    logger.info(f"==== Etapa: {step_name} CONCLUÍDA ====")
    return True

--------------------------------------------------------------------------------
# Arquivo: src/pipeline_steps/execute_temporal_trend_analysis.py
--------------------------------------------------------------------------------
# src/pipeline_steps/execute_temporal_trend_analysis.py
import logging
import pandas as pd
from typing import Any, Dict # Para os type hints dos argumentos

# Importa as funções de análise do módulo de tendências temporais
from src.analysis.temporal_trend_analysis import (
    get_full_draw_matrix, 
    calculate_moving_average_frequency,
    get_historical_delay_matrix,      # Nova importação
    calculate_moving_average_delay    # Nova importação
)

logger = logging.getLogger(__name__)

def run_temporal_trend_analysis_step(
    all_data_df: pd.DataFrame,
    db_manager: Any,  # Espera-se uma instância de DatabaseManager
    config: Any,      # Espera-se uma instância de Config
    shared_context: Dict[str, Any], # Dicionário de contexto compartilhado
    **kwargs
) -> bool:
    """
    Executa a etapa de análise de tendências temporais, incluindo:
    1. Média Móvel de Frequência Geral.
    2. Média Móvel de Atraso (Atual Instantâneo) Geral.
    Os argumentos são injetados pelo Orchestrator.
    """
    step_name = "Análise de Tendências Temporais (Médias Móveis)" # Nome da etapa atualizado
    logger.info(f"==== Iniciando Etapa: {step_name} ====")
    
    # Validações básicas dos argumentos injetados
    if not isinstance(all_data_df, pd.DataFrame) or all_data_df.empty:
        logger.error("DataFrame de sorteios (all_data_df) injetado está inválido ou vazio.")
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False
    
    if not hasattr(db_manager, 'save_dataframe'):
        logger.error("Objeto db_manager injetado não possui o método 'save_dataframe'.")
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False

    if not config:
        logger.error("Objeto config não foi injetado corretamente.")
        logger.info(f"==== Etapa: {step_name} FALHOU ====")
        return False

    # --- Média Móvel de Frequência ---
    try:
        logger.info("--- Iniciando sub-etapa: Média Móvel de Frequência ---")
        if not hasattr(config, 'GERAL_MA_FREQUENCY_WINDOWS') or \
           not hasattr(config, 'GERAL_MA_FREQUENCY_TABLE_NAME'):
            logger.error("Atributos de configuração para M.A. de Frequência não encontrados.")
            raise AttributeError("Configuração para M.A. de Frequência ausente.")

        geral_ma_frequency_table_name = config.GERAL_MA_FREQUENCY_TABLE_NAME
        freq_windows = config.GERAL_MA_FREQUENCY_WINDOWS

        logger.info("Gerando matriz completa de sorteios (ocorrências)...")
        draw_matrix = get_full_draw_matrix(all_data_df, config)

        if draw_matrix.empty:
            if not all_data_df.empty:
                 logger.error("Falha ao gerar a matriz de sorteios (ocorrências) a partir de dados não vazios.")
                 raise ValueError("Matriz de ocorrências não pôde ser gerada.")
            else:
                 logger.info("Dados de entrada vazios, nenhuma M.A. de Frequência para calcular.")
        else:
            logger.info(f"Calculando média móvel de frequência para janelas: {freq_windows}...")
            ma_frequency_df = calculate_moving_average_frequency(draw_matrix, freq_windows, config)

            if not isinstance(ma_frequency_df, pd.DataFrame):
                logger.error("A análise de média móvel de frequência não retornou um DataFrame.")
                raise TypeError("Resultado inesperado da análise de M.A. de Frequência.")
            
            if ma_frequency_df.empty and not draw_matrix.empty:
                logger.warning("M.A. de Frequência resultou em DataFrame vazio, embora a matriz de entrada não estivesse.")
            
            if ma_frequency_df.empty:
                logger.info(f"DataFrame de M.A. de Frequência vazio. Nada salvo em '{geral_ma_frequency_table_name}'.")
            else:
                db_manager.save_dataframe(ma_frequency_df, geral_ma_frequency_table_name, if_exists='replace')
                logger.info(f"M.A. de Frequência salva na tabela: {geral_ma_frequency_table_name}")
            
            shared_context['geral_ma_frequency_df'] = ma_frequency_df
            logger.info("Resultado da M.A. de Frequência adicionado ao shared_context.")
        logger.info("--- Sub-etapa: Média Móvel de Frequência CONCLUÍDA ---")

    except Exception as e_freq:
        logger.error(f"Erro durante a sub-etapa de Média Móvel de Frequência: {e_freq}", exc_info=True)
        # Decide se a falha em uma sub-etapa deve parar toda a etapa temporal_trend_analysis
        # Por ora, vamos continuar para a M.A. de Atraso, mas logamos o erro.
        # Se for crítico, pode-se retornar False aqui.


    # --- Média Móvel de Atraso (Atual Instantâneo) ---
    # A matriz de ocorrências (draw_matrix) já foi gerada acima e pode ser reutilizada.
    try:
        logger.info("--- Iniciando sub-etapa: Média Móvel de Atraso ---")
        if not hasattr(config, 'GERAL_MA_DELAY_WINDOWS') or \
           not hasattr(config, 'GERAL_MA_DELAY_TABLE_NAME'):
            logger.error("Atributos de configuração para M.A. de Atraso não encontrados.")
            raise AttributeError("Configuração para M.A. de Atraso ausente.")

        geral_ma_delay_table_name = config.GERAL_MA_DELAY_TABLE_NAME
        delay_windows = config.GERAL_MA_DELAY_WINDOWS

        if draw_matrix.empty: # Se a draw_matrix não pôde ser gerada antes
            if not all_data_df.empty:
                logger.error("Matriz de ocorrências está vazia, não é possível calcular M.A. de Atraso.")
                raise ValueError("Matriz de ocorrências necessária para M.A. de Atraso está vazia.")
            else:
                logger.info("Dados de entrada vazios, nenhuma M.A. de Atraso para calcular.")
        else:
            logger.info("Gerando matriz de atraso histórico...")
            historical_delay_matrix = get_historical_delay_matrix(draw_matrix, config)

            if historical_delay_matrix.empty:
                if not draw_matrix.empty:
                    logger.error("Falha ao gerar a matriz de atraso histórico a partir de uma matriz de ocorrências não vazia.")
                    raise ValueError("Matriz de atraso histórico não pôde ser gerada.")
                else: # draw_matrix estava vazia, o que é esperado se all_data_df era vazio.
                    logger.info("Matriz de ocorrências vazia, nenhuma M.A. de Atraso para calcular.")
            else:
                logger.info(f"Calculando média móvel de atraso para janelas: {delay_windows}...")
                ma_delay_df = calculate_moving_average_delay(historical_delay_matrix, delay_windows, config)

                if not isinstance(ma_delay_df, pd.DataFrame):
                    logger.error("A análise de média móvel de atraso não retornou um DataFrame.")
                    raise TypeError("Resultado inesperado da análise de M.A. de Atraso.")
                
                if ma_delay_df.empty and not historical_delay_matrix.empty:
                    logger.warning("M.A. de Atraso resultou em DataFrame vazio, embora a matriz de entrada não estivesse.")

                if ma_delay_df.empty:
                    logger.info(f"DataFrame de M.A. de Atraso vazio. Nada salvo em '{geral_ma_delay_table_name}'.")
                else:
                    db_manager.save_dataframe(ma_delay_df, geral_ma_delay_table_name, if_exists='replace')
                    logger.info(f"M.A. de Atraso salva na tabela: {geral_ma_delay_table_name}")
                
                shared_context['geral_ma_delay_df'] = ma_delay_df
                logger.info("Resultado da M.A. de Atraso adicionado ao shared_context.")
        logger.info("--- Sub-etapa: Média Móvel de Atraso CONCLUÍDA ---")

    except Exception as e_delay:
        logger.error(f"Erro durante a sub-etapa de Média Móvel de Atraso: {e_delay}", exc_info=True)
        # Se esta sub-etapa falhar, a etapa geral falha.
        logger.info(f"==== Etapa: {step_name} FALHOU (devido a erro na M.A. de Atraso) ====")
        return False
            
    logger.info(f"==== Etapa: {step_name} CONCLUÍDA ====")
    return True

--------------------------------------------------------------------------------
# Arquivo: src/run_demo.py
--------------------------------------------------------------------------------
# run_demo.py (ou integre em src/main.py)
import pandas as pd
import logging
from typing import Optional, List

# Ajuste os caminhos de importação conforme a estrutura do seu projeto
# Se este script estiver na raiz do projeto e 'src' for um pacote:
from src.database_manager import DatabaseManager
from src.analysis_aggregator import AnalysisAggregator
from src.scorer import ScorerManager
from src.config import config as app_config # Importa o dicionário de configuração

# Configuração básica do logging para vermos os outputs dos nossos componentes
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')
logger = logging.getLogger("DemoRun")

# Configurações do Pandas para melhor visualização de DataFrames no console
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 200)
pd.set_option('display.max_colwidth', 50)


def run_full_demo(latest_concurso_id_param: Optional[int] = None,
                  run_aggregator_test: bool = True,
                  run_scorer_test: bool = True,
                  strategies_to_test: Optional[List[str]] = None): # Lista de nomes de classes de estratégia
    """
    Executa uma demonstração do AnalysisAggregator e do ScorerManager.
    """
    logger.info("Iniciando demonstração do pipeline de agregação e scoring...")

    # 1. Inicializar dependências
    # (Assumindo que config.py define 'db_path' ou detalhes de conexão)
    db_path = app_config.get('database', {}).get('path', 'Data/lotofacil.db') # Exemplo de caminho do config
    
    db_manager = None
    analysis_aggregator = None
    scorer_manager = None

    try:
        db_manager = DatabaseManager(db_path=db_path, config=app_config)
        # db_manager.connect() # Removido se o __enter__ cuidar disso ou se as chamadas conectam sob demanda

        # O BlockAggregator é opcional no AnalysisAggregator, então não precisamos instanciá-lo aqui
        # a menos que queiramos testar especificamente essa integração.
        analysis_aggregator = AnalysisAggregator(db_manager=db_manager, config_dict=app_config)
        
        scorer_manager = ScorerManager(db_manager=db_manager,
                                       analysis_aggregator=analysis_aggregator,
                                       config_dict=app_config)

        # Determinar o latest_concurso_id para o teste
        if latest_concurso_id_param is None:
            # Se o agregador tiver um método para pegar o último ID, use-o.
            # Caso contrário, o método get_historical_metrics_for_dezenas deve tratar None.
            # latest_concurso_id_to_use = analysis_aggregator._get_latest_concurso_id_from_db() # Se quiser pegar explicitamente
            latest_concurso_id_to_use = None # Deixa o agregador resolver se for None
            logger.info("Nenhum 'latest_concurso_id' específico fornecido, o Aggregator usará o mais recente do DB.")
        else:
            latest_concurso_id_to_use = latest_concurso_id_param
            logger.info(f"Usando 'latest_concurso_id' específico: {latest_concurso_id_to_use}")


        # --- Teste do AnalysisAggregator ---
        if run_aggregator_test and analysis_aggregator:
            logger.info("\n--- Testando AnalysisAggregator ---")
            df_aggregated_metrics = analysis_aggregator.get_historical_metrics_for_dezenas(
                latest_concurso_id=latest_concurso_id_to_use
            )
            if not df_aggregated_metrics.empty:
                logger.info(f"DataFrame Agregado retornado (até concurso {latest_concurso_id_to_use if latest_concurso_id_to_use else 'mais recente'}):")
                logger.info(f"Shape: {df_aggregated_metrics.shape}")
                logger.info(f"Colunas: {df_aggregated_metrics.columns.tolist()}")
                logger.info("Primeiras 5 linhas do DataFrame Agregado:\n" + df_aggregated_metrics.head().to_string())
                
                # Verificar se as colunas esperadas pelas estratégias estão presentes
                # Exemplo para SimpleRecencyAndDelayStrategy
                # (O nome da coluna de frequência recente é construído com base no default do aggregator)
                default_window = app_config.get('aggregator_default_recent_window', 10)
                expected_freq_col = f"recent_frequency_window_{default_window}"
                
                missing_cols_for_simple_strategy = []
                if 'current_delay' not in df_aggregated_metrics.columns:
                    missing_cols_for_simple_strategy.append('current_delay')
                if expected_freq_col not in df_aggregated_metrics.columns:
                     missing_cols_for_simple_strategy.append(expected_freq_col)
                
                if missing_cols_for_simple_strategy:
                    logger.warning(f"Colunas FALTANTES para SimpleRecencyAndDelayStrategy no Aggregator: {missing_cols_for_simple_strategy}")
                else:
                    logger.info(f"Colunas OK para SimpleRecencyAndDelayStrategy ('current_delay', '{expected_freq_col}').")

            else:
                logger.warning("AnalysisAggregator retornou um DataFrame vazio.")
        
        # --- Teste do ScorerManager e Estratégias ---
        if run_scorer_test and scorer_manager:
            logger.info("\n--- Testando ScorerManager e Estratégias ---")
            available_strategy_names = scorer_manager.get_available_strategy_names()
            logger.info(f"Estratégias disponíveis descobertas pelo ScorerManager: {available_strategy_names}")

            if not available_strategy_names:
                logger.warning("Nenhuma estratégia disponível para teste.")
                return

            strategies_to_run = strategies_to_test if strategies_to_test else available_strategy_names

            for strategy_name in strategies_to_run:
                if strategy_name not in available_strategy_names:
                    logger.warning(f"Estratégia '{strategy_name}' solicitada para teste não foi descoberta. Pulando.")
                    continue

                logger.info(f"\n--- Executando Estratégia: {strategy_name} ---")
                
                # Parâmetros específicos podem ser definidos por estratégia aqui
                strategy_specific_params = {}
                if strategy_name == "SimpleRecencyAndDelayStrategy":
                    strategy_specific_params = {
                        'target_recent_window_suffix': str(app_config.get('aggregator_default_recent_window', 10)),
                        'delay_weight': 0.5,
                        'frequency_weight': 0.5
                    }
                elif strategy_name == "CombinationAndPropertiesStrategy":
                     strategy_specific_params = {
                         'candidate_pool_size': 25, # Exemplo de override
                         'max_combinations_to_evaluate': 1000 # Reduzir para demo rápida
                     }
                # Adicionar outros 'elif' para parâmetros de outras estratégias se necessário

                # 1. Testar generate_scores
                logger.info(f"Gerando scores para '{strategy_name}' (até concurso {latest_concurso_id_to_use if latest_concurso_id_to_use else 'mais recente'})...")
                scores_df = scorer_manager.generate_scores_for_strategy(
                    strategy_class_name=strategy_name,
                    latest_draw_id=latest_concurso_id_to_use,
                    strategy_specific_params=strategy_specific_params
                )
                if scores_df is not None and not scores_df.empty:
                    logger.info(f"Scores gerados por '{strategy_name}' (primeiras 5 linhas):\n" + scores_df.head().to_string())
                elif scores_df is not None and scores_df.empty:
                     logger.warning(f"'{strategy_name}' gerou um DataFrame de scores vazio (nenhuma dezena qualificada?).")
                else:
                    logger.error(f"Falha ao gerar scores para '{strategy_name}'.")
                    continue # Pula para a próxima estratégia se os scores falharem

                # 2. Testar select_numbers
                logger.info(f"Selecionando números para '{strategy_name}'...")
                selected_numbers = scorer_manager.select_numbers_for_strategy(
                    strategy_class_name=strategy_name,
                    latest_draw_id=latest_concurso_id_to_use,
                    num_to_select=15,
                    strategy_specific_params=strategy_specific_params,
                    # selection_extra_params pode ser usado se o select_numbers da estratégia o suportar
                )
                if selected_numbers:
                    logger.info(f"Dezenas Selecionadas por '{strategy_name}': {selected_numbers}")
                else:
                    logger.warning(f"Não foi possível selecionar dezenas para '{strategy_name}'.")

    except Exception as e:
        logger.error(f"Ocorreu um erro geral na demonstração: {e}", exc_info=True)
    # finally:
        # Removido o db_manager.close() daqui, pois o __exit__ do DatabaseManager
        # (se implementado e usado com 'with') ou o chamador principal cuidaria disso.
        # Se não usar 'with', o fechamento explícito é necessário no final do escopo de uso.
        # logger.info("Demonstração concluída.")


if __name__ == '__main__':
    # Para executar a demonstração:
    # Você pode definir um concurso específico para testar, ou deixar None para usar o mais recente.
    # Ex: test_concurso_id = 3000 # Um concurso que já ocorreu e tem dados de análise
    test_concurso_id = None 

    # Você pode escolher testar apenas algumas estratégias pelo nome da classe:
    # test_only_these_strategies = ["SimpleRecencyAndDelayStrategy", "TrendAndRecurrenceStrategy"]
    test_only_these_strategies = None # None para testar todas as descobertas

    run_full_demo(latest_concurso_id_param=test_concurso_id,
                  strategies_to_test=test_only_these_strategies)

    # Para salvar a saída em um arquivo CSV, você modificaria a impressão dos DataFrames:
    # if df_aggregated_metrics is not None and not df_aggregated_metrics.empty:
    #     df_aggregated_metrics.to_csv("aggregator_output.csv", index=False)
    # if scores_df is not None and not scores_df.empty:
    #     scores_df.to_csv(f"scores_{strategy_name}.csv", index=False)

--------------------------------------------------------------------------------
# Arquivo: src/scorer.py
--------------------------------------------------------------------------------
# src/scorer.py
import importlib
import inspect
import os
import pkgutil # Usado para uma forma mais robusta de descobrir módulos
from typing import List, Optional, Dict, Type, Any
import pandas as pd

# Importações dos nossos componentes centrais
# Ajuste os caminhos de importação conforme a estrutura exata do seu projeto
from .strategies.base_strategy import BaseStrategy
from .database_manager import DatabaseManager
from .analysis_aggregator import AnalysisAggregator
from src.config import config_obj # Importar o config_obj global

# Importar todas as classes de estratégia para que possam ser descobertas.
# Uma alternativa à descoberta dinâmica é registrar explicitamente as estratégias.
# Para descoberta dinâmica, precisamos garantir que os módulos em src/strategies sejam "visíveis".
# Vamos tentar uma descoberta baseada em pkgutil e inspect.

# Path para o diretório de estratégias (relativo a src/)
STRATEGIES_PACKAGE_PATH = os.path.join(os.path.dirname(__file__), 'strategies')
STRATEGIES_PYTHON_MODULE_PATH = 'src.strategies' # Como o Python importa (se src for raiz do PYTHONPATH)


class ScorerManager:
    """
    Gerencia e executa diferentes estratégias de pontuação e seleção de dezenas.
    Descobre automaticamente estratégias que herdam de BaseStrategy.
    """

    def __init__(self,
                 db_manager: DatabaseManager,
                 analysis_aggregator: AnalysisAggregator,
                 config_dict: Optional[Dict[str, Any]] = None): # Recebe o dict de configuração
        """
        Construtor do ScorerManager.

        Args:
            db_manager: Instância do DatabaseManager.
            analysis_aggregator: Instância do AnalysisAggregator.
            config_dict: Dicionário de configuração global do aplicativo.
        """
        self.db_manager = db_manager
        self.analysis_aggregator = analysis_aggregator
        self.config = config_dict if config_dict is not None else app_config
        
        self._strategy_classes: Dict[str, Type[BaseStrategy]] = self._discover_strategies()
        self._strategy_instances: Dict[str, BaseStrategy] = {} # Cache para instâncias com params default

        if not self._strategy_classes:
            print("AVISO (ScorerManager): Nenhuma estratégia foi descoberta. "
                  "Verifique a pasta 'src/strategies' e se as classes herdam de BaseStrategy.")
        else:
            print(f"INFO (ScorerManager): Estratégias descobertas: {list(self._strategy_classes.keys())}")

    def _discover_strategies(self) -> Dict[str, Type[BaseStrategy]]:
        """
        Descobre classes de estratégia no pacote 'src.strategies'.
        Considera apenas classes concretas que herdam de BaseStrategy.
        Usa o nome da classe como chave de registro.
        """
        discovered_strategies: Dict[str, Type[BaseStrategy]] = {}
        
        # Tentar importar o pacote de estratégias
        try:
            strategy_module_root = importlib.import_module(STRATEGIES_PYTHON_MODULE_PATH)
        except ImportError as e:
            print(f"ERRO (ScorerManager): Não foi possível importar o pacote de estratégias '{STRATEGIES_PYTHON_MODULE_PATH}': {e}")
            print("Verifique se 'src' está no PYTHONPATH ou se o caminho está correto.")
            return discovered_strategies

        # Iterar sobre os módulos dentro do pacote de estratégias
        for _, module_name, _ in pkgutil.iter_modules(strategy_module_root.__path__):
            if module_name == 'base_strategy': # Pular o arquivo da classe base
                continue
            try:
                # Montar o nome completo do módulo para importação
                full_module_path = f"{STRATEGIES_PYTHON_MODULE_PATH}.{module_name}"
                module = importlib.import_module(full_module_path)
                
                # Inspecionar membros do módulo importado
                for name, cls in inspect.getmembers(module, inspect.isclass):
                    # Verificar se é uma subclasse de BaseStrategy, não é a própria BaseStrategy,
                    # e não é uma classe abstrata (caso haja outras bases abstratas no futuro)
                    if issubclass(cls, BaseStrategy) and cls is not BaseStrategy and not inspect.isabstract(cls):
                        if name in discovered_strategies:
                            print(f"AVISO (ScorerManager): Nome de classe de estratégia '{name}' duplicado. "
                                  f"Módulo: {full_module_path}. Usando a primeira descoberta.")
                        else:
                            discovered_strategies[name] = cls # Usa o nome da classe como ID
                            # print(f"DEBUG (ScorerManager): Estratégia '{name}' descoberta em {full_module_path}")

            except ImportError as e:
                print(f"ERRO (ScorerManager): Falha ao importar o módulo de estratégia '{module_name}' de '{STRATEGIES_PYTHON_MODULE_PATH}': {e}")
        
        return discovered_strategies

    def get_available_strategy_names(self) -> List[str]:
        """
        Retorna uma lista dos nomes das classes de estratégias descobertas.
        Estes nomes são usados para instanciar e executar estratégias.
        """
        return list(self._strategy_classes.keys())

    def get_strategy_instance(self,
                              strategy_class_name: str,
                              strategy_specific_params: Optional[Dict[str, Any]] = None,
                              use_cache_if_no_specific_params: bool = True) -> Optional[BaseStrategy]:
        """
        Obtém ou cria uma instância de uma estratégia específica.

        Args:
            strategy_class_name: O nome da classe da estratégia (chave do dict _strategy_classes).
            strategy_specific_params: Parâmetros específicos para instanciar/configurar a estratégia.
                                      Se fornecidos, uma nova instância é sempre criada (ou uma existente
                                      reconfigurada, dependendo da implementação da estratégia).
            use_cache_if_no_specific_params: Se True e strategy_specific_params for None,
                                             tenta reutilizar uma instância já criada com parâmetros default.

        Returns:
            Optional[BaseStrategy]: A instância da estratégia, ou None se não encontrada/erro.
        """
        if strategy_class_name not in self._strategy_classes:
            print(f"ERRO (ScorerManager): Estratégia com nome de classe '{strategy_class_name}' não reconhecida.")
            return None

        strategy_cls = self._strategy_classes[strategy_class_name]
        
        # Se parâmetros específicos são fornecidos, não usar cache simples baseado no nome da classe.
        # Ou, a chave do cache precisaria incluir uma representação dos parâmetros.
        # Por simplicidade, se strategy_specific_params for dado, criamos nova instância.
        if strategy_specific_params or not use_cache_if_no_specific_params:
            try:
                # print(f"DEBUG (ScorerManager): Criando nova instância de '{strategy_class_name}' com params: {strategy_specific_params}")
                instance = strategy_cls(
                    db_manager=self.db_manager,
                    config=self.config, # Passa o config global
                    analysis_aggregator=self.analysis_aggregator,
                    **(strategy_specific_params or {}) # Desempacota params específicos da estratégia
                )
                return instance
            except Exception as e:
                print(f"ERRO (ScorerManager): Falha ao instanciar a estratégia '{strategy_class_name}' com params: {e}")
                return None
        
        # Usar cache se não houver parâmetros específicos e cache permitido
        if strategy_class_name not in self._strategy_instances:
            try:
                # print(f"DEBUG (ScorerManager): Criando e cacheando instância default de '{strategy_class_name}'")
                self._strategy_instances[strategy_class_name] = strategy_cls(
                    db_manager=self.db_manager,
                    config=self.config,
                    analysis_aggregator=self.analysis_aggregator
                    # **{} # Sem parâmetros específicos para a instância default cacheada
                )
            except Exception as e:
                print(f"ERRO (ScorerManager): Falha ao instanciar e cachear estratégia default '{strategy_class_name}': {e}")
                return None
        
        return self._strategy_instances.get(strategy_class_name)


    def generate_scores_for_strategy(self,
                                     strategy_class_name: str,
                                     latest_draw_id: Optional[int] = None,
                                     strategy_specific_params: Optional[Dict[str, Any]] = None
                                     ) -> Optional[pd.DataFrame]:
        """
        Gera scores para uma estratégia específica.

        Args:
            strategy_class_name: Nome da classe da estratégia.
            latest_draw_id: ID do último concurso para os cálculos.
            strategy_specific_params: Parâmetros específicos para a estratégia.

        Returns:
            Optional[pd.DataFrame]: DataFrame de scores ou None em caso de erro.
        """
        strategy_instance = self.get_strategy_instance(
            strategy_class_name, 
            strategy_specific_params,
            # Se params são dados, não faz sentido usar cache da instância default.
            # A instância criada será usada para esta chamada.
            use_cache_if_no_specific_params=(strategy_specific_params is None) 
        )
        
        if strategy_instance:
            try:
                # print(f"INFO (ScorerManager): Gerando scores para estratégia '{strategy_instance.get_name()}' (latest_draw_id={latest_draw_id})")
                return strategy_instance.generate_scores(latest_draw_id)
            except Exception as e:
                print(f"ERRO (ScorerManager): Falha ao gerar scores para a estratégia '{strategy_instance.get_name()}': {e}")
                return None
        return None

    def select_numbers_for_strategy(self,
                                    strategy_class_name: str,
                                    latest_draw_id: Optional[int] = None,
                                    num_to_select: int = 15,
                                    strategy_specific_params: Optional[Dict[str, Any]] = None,
                                    selection_extra_params: Optional[Dict[str, Any]] = None
                                    ) -> Optional[List[int]]:
        """
        Seleciona números usando uma estratégia específica.

        Args:
            strategy_class_name: Nome da classe da estratégia.
            latest_draw_id: ID do último concurso para os cálculos.
            num_to_select: Número de dezenas a selecionar.
            strategy_specific_params: Parâmetros para a instanciação da estratégia.
            selection_extra_params: Parâmetros extras para o método `select_numbers` da estratégia.

        Returns:
            Optional[List[int]]: Lista de dezenas selecionadas ou None em caso de erro.
        """
        scores_df = self.generate_scores_for_strategy(
            strategy_class_name, 
            latest_draw_id, 
            strategy_specific_params
        )
        
        if scores_df is not None:
            # Precisamos da instância da estratégia para chamar select_numbers.
            # Reutilizar a lógica de get_strategy_instance.
            strategy_instance = self.get_strategy_instance(
                strategy_class_name, 
                strategy_specific_params,
                use_cache_if_no_specific_params=(strategy_specific_params is None)
            )
            if strategy_instance:
                try:
                    # print(f"INFO (ScorerManager): Selecionando números para estratégia '{strategy_instance.get_name()}'")
                    return strategy_instance.select_numbers(
                        scores_df, 
                        num_to_select, 
                        selection_params=selection_extra_params
                    )
                except Exception as e:
                    print(f"ERRO (ScorerManager): Falha ao selecionar números para a estratégia '{strategy_instance.get_name()}': {e}")
                    return None
        return None

# Exemplo de como o ScorerManager poderia ser usado (ex: no main.py ou runner.py)
# if __name__ == '__main__':
#     # 1. Inicializar dependências (assumindo que elas existem e são configuradas)
#     # Estas são apenas instanciações placeholder para o exemplo
#     db_m = DatabaseManager(db_path='lotofacil.db', config=app_config) # app_config importado de config.py
#     # block_agg = BlockAggregator(db_manager=db_m, config=app_config) # Se usado
#     analysis_agg = AnalysisAggregator(db_manager=db_m, config_dict=app_config) # Passando config_dict

#     # Conectar ao DB se o construtor do DBManager não o fizer
#     # db_m.connect() 

#     # 2. Criar o ScorerManager
#     scorer_mgr = ScorerManager(db_manager=db_m, analysis_aggregator=analysis_agg, config_dict=app_config)

#     # 3. Ver estratégias disponíveis
#     available_strategies = scorer_mgr.get_available_strategy_names()
#     print(f"\nEstratégias Disponíveis: {available_strategies}")

#     if available_strategies:
#         # 4. Escolher uma estratégia para executar (ex: a primeira da lista)
#         # Para estratégias que usam nomes de colunas do aggregator que são construídos dinamicamente
#         # (como SimpleRecencyAndDelayStrategy com `recent_frequency_window_SUFFIX`),
#         # o sufixo (ex: "10") é passado como parte dos strategy_specific_params.
        
#         # Exemplo para SimpleRecencyAndDelayStrategy
#         strategy_name_to_run = 'SimpleRecencyAndDelayStrategy' # Use o nome exato da classe
#         if strategy_name_to_run in available_strategies:
#             print(f"\n--- Executando Estratégia: {strategy_name_to_run} ---")
#             params_for_simple_strategy = {
#                 'target_recent_window_suffix': "10", # Ex: para usar 'recent_frequency_window_10'
#                 'delay_weight': 0.6,
#                 'frequency_weight': 0.4
#             }
#             selected_numbers = scorer_mgr.select_numbers_for_strategy(
#                 strategy_class_name=strategy_name_to_run,
#                 latest_draw_id=None, # Usará o mais recente do DB
#                 num_to_select=15,
#                 strategy_specific_params=params_for_simple_strategy
#             )
#             if selected_numbers:
#                 print(f"Dezenas Selecionadas por {strategy_name_to_run}: {selected_numbers}")
#             else:
#                 print(f"Não foi possível selecionar dezenas para {strategy_name_to_run}.")
        
#         # Exemplo para TrendAndRecurrenceStrategy (sem params específicos extras no __init__ além dos defaults)
#         strategy_name_trend = 'TrendAndRecurrenceStrategy'
#         if strategy_name_trend in available_strategies:
#             print(f"\n--- Executando Estratégia: {strategy_name_trend} ---")
#             selected_numbers_trend = scorer_mgr.select_numbers_for_strategy(
#                 strategy_class_name=strategy_name_trend,
#                 latest_draw_id=None
#             )
#             if selected_numbers_trend:
#                 print(f"Dezenas Selecionadas por {strategy_name_trend}: {selected_numbers_trend}")
#             else:
#                 print(f"Não foi possível selecionar dezenas para {strategy_name_trend}.")

#     # Fechar conexão com DB se o DBManager não usar context manager no uso principal
#     # db_m.close()

--------------------------------------------------------------------------------
# Arquivo: src/strategies/__init__.py
--------------------------------------------------------------------------------
# src/__init__.py
# Este arquivo pode ficar vazio.

--------------------------------------------------------------------------------
# Arquivo: src/strategies/base_strategy.py
--------------------------------------------------------------------------------
# src/strategies/base_strategy.py
import abc
from typing import List, Optional, Dict, Any
import pandas as pd

# Importações de componentes do projeto (ajuste os caminhos se necessário)
# Se os arquivos estiverem em src/
from ..database_manager import DatabaseManager
from ..analysis_aggregator import AnalysisAggregator
from src.config import config_obj # Importar o config_obj global

class BaseStrategy(abc.ABC):
    """
    Classe base abstrata para todas as estratégias de seleção de dezenas.

    Atributos:
        db_manager (DatabaseManager): Instância para interagir com o banco de dados.
        config (Dict[str, Any]): Dicionário de configuração do projeto.
        analysis_aggregator (AnalysisAggregator): Instância para buscar dados agregados das análises.
        strategy_specific_params (Dict[str, Any]): Parâmetros específicos para a estratégia filha.
    """

    def __init__(self,
                 db_manager: DatabaseManager,
                 config: Dict[str, Any], # Config global do app
                 analysis_aggregator: AnalysisAggregator,
                 **strategy_params: Any): # Parâmetros específicos da estratégia
        """
        Construtor para a BaseStrategy.

        Args:
            db_manager: Gerenciador de conexão com o banco de dados.
            config: Configurações globais do projeto (geralmente o dict app_config).
            analysis_aggregator: Agregador de dados de análises.
            **strategy_params: Parâmetros adicionais específicos para a estratégia.
        """
        self.db_manager = db_manager
        self.config = config 
        self.analysis_aggregator = analysis_aggregator
        self.strategy_specific_params = strategy_params
        # _all_dezenas_list pode ser útil para as estratégias filhas
        self._all_dezenas_list = self.config.get('todas_dezenas', list(range(1, 26)))


    @abc.abstractmethod
    def get_name(self) -> str:
        """
        Retorna o nome único e identificável da estratégia.
        Este nome pode incluir valores de parâmetros chave para distinguir
        variações da mesma estratégia base.

        Returns:
            str: O nome da estratégia.
        """
        pass

    @abc.abstractmethod
    def get_description(self) -> str:
        """
        Fornece uma breve descrição da lógica da estratégia.

        Returns:
            str: A descrição da estratégia.
        """
        pass

    @abc.abstractmethod
    def generate_scores(self, latest_draw_id: Optional[int] = None) -> pd.DataFrame:
        """
        Gera pontuações para todas as dezenas com base na lógica da estratégia.

        A pontuação deve refletir o quão "promissora" uma dezena é segundo
        os critérios da estratégia. Opcionalmente, considera dados até
        o 'latest_draw_id'. Se None, usa todos os dados disponíveis (ou o mais recente).

        Args:
            latest_draw_id (Optional[int]): O ID do último concurso a ser considerado
                                             para os cálculos. Se None, o AnalysisAggregator
                                             geralmente usará o concurso mais recente no banco.

        Returns:
            pd.DataFrame: DataFrame com as colunas ['dezena', 'score', 'ranking_strategy'],
                          onde 'dezena' é o número (1-25), 'score' é a pontuação
                          atribuída pela estratégia (maior é melhor), e 'ranking_strategy'
                          é a classificação da dezena dentro desta estratégia.
                          O DataFrame deve estar ordenado por 'score' descendente.
        """
        pass

    def select_numbers(self,
                       scores_df: pd.DataFrame,
                       num_to_select: int = 15, # Padrão Lotofácil
                       selection_params: Optional[Dict[str, Any]] = None) -> List[int]:
        """
        Seleciona um conjunto de dezenas com base nos scores gerados.

        A implementação padrão seleciona as 'num_to_select' dezenas com os maiores scores.
        Estratégias filhas podem sobrescrever este método para implementar lógicas
        de seleção mais complexas (ex: garantir diversidade, aplicar filtros
        baseados em 'selection_params').

        Args:
            scores_df (pd.DataFrame): DataFrame retornado por `generate_scores`.
                                      Deve conter colunas 'dezena' e 'score'.
            num_to_select (int): Número de dezenas a serem selecionadas.
            selection_params (Optional[Dict[str, Any]]): Parâmetros adicionais para
                                                        guiar o processo de seleção.

        Returns:
            List[int]: Uma lista de 'num_to_select' dezenas selecionadas, ordenadas.
                       Retorna lista vazia se scores_df for None ou não tiver dezenas suficientes.
        """
        if scores_df is None or scores_df.empty:
            print(f"AVISO (BaseStrategy:{self.get_name()}): DataFrame de scores vazio ou None. Retornando lista vazia.")
            return []
        
        if not all(col in scores_df.columns for col in ['dezena', 'score']):
            raise ValueError("scores_df deve ser um DataFrame com colunas 'dezena' e 'score'.")
        
        # Garante que está ordenado pelo score para pegar as N melhores
        # e depois ordena as dezenas selecionadas para consistência.
        sorted_scores_df = scores_df.sort_values(by='score', ascending=False)
        
        selected_dezenas = sorted_scores_df['dezena'].head(num_to_select).tolist()
        
        # Garante que o número correto de dezenas seja retornado, mesmo que scores_df tenha menos linhas.
        # Se tiver menos, retorna o que tem.
        if len(selected_dezenas) < num_to_select:
            print(f"AVISO (BaseStrategy:{self.get_name()}): Menos de {num_to_select} dezenas disponíveis com scores. "
                  f"Retornando {len(selected_dezenas)} dezenas.")
            
        return sorted(selected_dezenas) # Retorna as dezenas selecionadas ordenadas numericamente

--------------------------------------------------------------------------------
# Arquivo: src/strategies/combination_properties_strategy.py
--------------------------------------------------------------------------------
# src/strategies/combination_properties_strategy.py
from typing import List, Optional, Dict, Any, Tuple
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from itertools import combinations
import math # Para math.comb

# Supondo que BaseStrategy e os componentes do Aggregator/DBManager são importáveis
# Ajuste os caminhos de importação conforme a estrutura do seu projeto.
from .base_strategy import BaseStrategy
from ..database_manager import DatabaseManager
from ..analysis_aggregator import AnalysisAggregator
# from ..config import config as app_config

class CombinationAndPropertiesStrategy(BaseStrategy):
    """
    Estratégia que pontua dezenas com base na sua participação em itemsets fortes
    e, em seguida, seleciona o conjunto final de 15 dezenas que melhor
    se adequa a propriedades globais de jogo desejadas.
    Utiliza o AnalysisAggregator para buscar dados de itemsets.
    """

    def __init__(self,
                 db_manager: DatabaseManager,
                 config: Dict[str, Any], # Config global do app
                 analysis_aggregator: AnalysisAggregator,
                 # Parâmetros específicos da estratégia:
                 itemset_k_values: Optional[List[int]] = None, # Default [2,3] se None, ou o que o Aggregator suportar
                 itemset_min_support: Optional[float] = None,
                 itemset_min_lift: Optional[float] = None,
                 itemset_metric_to_use_as_score: str = 'itemset_score', # Coluna do Aggregator para score do itemset
                 # Parâmetros para Fase 2: Propriedades Globais (permanecem na estratégia)
                 target_sum_range: Tuple[int, int] = (180, 220),
                 target_even_count_range: Tuple[int, int] = (6, 9), # Implica 15 - count para ímpares
                 # Adicionar outros targets de propriedades (primos, linhas, colunas) conforme necessário
                 # via strategy_params e processados no __init__ ou _score_combination_properties
                 candidate_pool_size: int = 30, # Top N dezenas da fase de scoring para gerar combinações
                 max_combinations_to_evaluate: int = 50000 # Limite para avaliação de combinações
                 ):
        # Passando todos os params para BaseStrategy para que fiquem em self.strategy_specific_params
        super().__init__(db_manager, config, analysis_aggregator,
                         itemset_k_values=itemset_k_values if itemset_k_values is not None else [2,3],
                         itemset_min_support=itemset_min_support,
                         itemset_min_lift=itemset_min_lift,
                         itemset_metric_to_use_as_score=itemset_metric_to_use_as_score,
                         target_sum_range=target_sum_range,
                         target_even_count_range=target_even_count_range,
                         candidate_pool_size=candidate_pool_size,
                         max_combinations_to_evaluate=max_combinations_to_evaluate)

        # Atribuindo os parâmetros à instância para fácil acesso
        self.itemset_k_values = self.strategy_specific_params.get('itemset_k_values')
        self.itemset_min_support = self.strategy_specific_params.get('itemset_min_support')
        self.itemset_min_lift = self.strategy_specific_params.get('itemset_min_lift')
        self.itemset_metric_to_use_as_score = self.strategy_specific_params.get('itemset_metric_to_use_as_score')
        
        self.target_sum_range = self.strategy_specific_params.get('target_sum_range')
        self.target_even_count_range = self.strategy_specific_params.get('target_even_count_range')
        self.candidate_pool_size = self.strategy_specific_params.get('candidate_pool_size')
        self.max_combinations_to_evaluate = self.strategy_specific_params.get('max_combinations_to_evaluate')
        
        # Cache para dados de itemsets, se necessário, embora cada chamada possa ser única
        # self._itemset_data_cache: Dict[str, pd.DataFrame] = {}

    def get_name(self) -> str:
        k_values_str = str(self.itemset_k_values).replace(" ","")
        return (f"CombinationAndPropertiesStrategy(k={k_values_str}, "
                f"sup>={self.itemset_min_support}, lift>={self.itemset_min_lift}, "
                f"pool={self.candidate_pool_size})")

    def get_description(self) -> str:
        return ("Pontua dezenas por participação em itemsets fortes (via AnalysisAggregator) e seleciona "
                "o jogo final baseado em propriedades globais (soma, pares/ímpares, etc.) definidas na estratégia.")

    def _get_strong_itemsets_data(self, latest_draw_id: Optional[int] = None) -> pd.DataFrame:
        """
        Busca dados de itemsets (pares, trios, etc.) que atendem a critérios de "força",
        utilizando o método get_itemset_analysis_data do AnalysisAggregator.
        """
        # print(f"INFO (Strategy:{self.get_name()}): Buscando dados de itemsets fortes via Aggregator (até concurso {latest_draw_id})...")
        
        df_itemsets = self.analysis_aggregator.get_itemset_analysis_data(
            latest_concurso_id=latest_draw_id,
            k_values=self.itemset_k_values,
            min_support=self.itemset_min_support,
            min_lift=self.itemset_min_lift
        )

        if df_itemsets is None or df_itemsets.empty:
            print(f"AVISO ({self.get_name()}): Nenhum itemset forte encontrado pelos critérios definidos via Aggregator.")
            return pd.DataFrame(columns=['itemset', 'metric_value', 'k'])

        # A coluna 'itemset_metric_to_use_as_score' (ex: 'itemset_score', 'support', 'lift')
        # deve ser usada como a métrica de força do itemset.
        if self.itemset_metric_to_use_as_score not in df_itemsets.columns:
            print(f"ERRO ({self.get_name()}): Coluna '{self.itemset_metric_to_use_as_score}' "
                  "não encontrada nos dados de itemset do Aggregator. Verifique o parâmetro "
                  "'itemset_metric_to_use_as_score' da estratégia e a saída do Aggregator.")
            # Adiciona uma coluna de score neutro para evitar falhas, mas idealmente isso não deveria acontecer.
            df_itemsets['metric_value'] = 0.0
        else:
            df_itemsets = df_itemsets.rename(columns={self.itemset_metric_to_use_as_score: 'metric_value'})
        
        # Garantir que a coluna 'itemset' contenha tuplas (o aggregator deve cuidar disso)
        # Mas uma verificação/conversão defensiva pode ser útil.
        if 'itemset' in df_itemsets.columns:
            df_itemsets['itemset'] = df_itemsets['itemset'].apply(
                lambda x: tuple(sorted(x)) if isinstance(x, (list, pd.Series, set)) and not isinstance(x, tuple) else x
            )
        else:
             print(f"ERRO ({self.get_name()}): Coluna 'itemset' não encontrada nos dados do Aggregator.")
             return pd.DataFrame(columns=['itemset', 'metric_value', 'k'])


        # Garantir que a coluna 'k' (tamanho do itemset) exista, se não vier do aggregator, inferir.
        if 'k' not in df_itemsets.columns and 'itemset' in df_itemsets.columns:
            df_itemsets['k'] = df_itemsets['itemset'].apply(len)
        elif 'k' not in df_itemsets.columns:
             print(f"ERRO ({self.get_name()}): Coluna 'k' não encontrada e não pode ser inferida.")
             return pd.DataFrame(columns=['itemset', 'metric_value', 'k'])

        return df_itemsets[['itemset', 'metric_value', 'k']]


    def generate_scores(self, latest_draw_id: Optional[int] = None) -> pd.DataFrame:
        """
        Fase 1: Pontua dezenas individualmente com base na sua participação em itemsets fortes.
        """
        df_strong_itemsets = self._get_strong_itemsets_data(latest_draw_id)
        
        # _all_dezenas_list é herdado da BaseStrategy
        base_dezenas_df = pd.DataFrame({'dezena': self._all_dezenas_list})

        if df_strong_itemsets.empty:
            df_scores = base_dezenas_df.copy()
            df_scores['raw_itemset_score'] = 0.0
        else:
            dezena_scores_accumulation: Dict[int, float] = {d: 0.0 for d in self._all_dezenas_list}
            for _, row in df_strong_itemsets.iterrows():
                itemset = row.get('itemset') # Usar .get() para segurança
                metric = row.get('metric_value', 0.0) # Default para 0 se a coluna não existir por algum motivo
                
                if not isinstance(itemset, tuple):
                    # print(f"AVISO ({self.get_name()}): Itemset não é uma tupla: {itemset} (tipo: {type(itemset)}). Pulando.")
                    continue
                for dezena in itemset:
                    if dezena in dezena_scores_accumulation: # Segurança adicional
                        dezena_scores_accumulation[dezena] += metric
                    # else:
                        # print(f"AVISO ({self.get_name()}): Dezena {dezena} do itemset {itemset} não está na lista de dezenas válidas.")


            df_scores = pd.DataFrame(list(dezena_scores_accumulation.items()), columns=['dezena', 'raw_itemset_score'])

        # Garantir que todas as dezenas estejam presentes, mesmo que com score 0
        df_scores = pd.merge(base_dezenas_df, df_scores, on='dezena', how='left').fillna({'raw_itemset_score': 0.0})

        # Normalizar o 'raw_itemset_score'
        scaler = MinMaxScaler()
        if 'raw_itemset_score' in df_scores.columns and df_scores['raw_itemset_score'].nunique() > 1:
            df_scores['score'] = scaler.fit_transform(df_scores[['raw_itemset_score']])
        elif 'raw_itemset_score' in df_scores.columns: # Todos os scores são iguais
            df_scores['score'] = 0.5 # Valor neutro
        else: # Coluna não existe (improvável devido ao fillna, mas defensivo)
            df_scores['score'] = 0.0
        
        df_final_scores = df_scores.sort_values(by='score', ascending=False).reset_index(drop=True)
        df_final_scores['ranking_strategy'] = df_final_scores.index + 1
        
        return df_final_scores[['dezena', 'score', 'ranking_strategy']]

    def _score_combination_properties(self, combination: List[int]) -> float:
        """
        Calcula um score para uma combinação de 15 dezenas com base
        no seu alinhamento com as propriedades de jogo desejadas.
        (Lógica interna da estratégia, não depende diretamente do Aggregator para ESTA avaliação).
        """
        if len(combination) != 15: return 0.0

        score = 0.0
        num_properties_checked = 0

        # 1. Soma das dezenas
        soma = sum(combination)
        num_properties_checked += 1
        if self.target_sum_range[0] <= soma <= self.target_sum_range[1]:
            score += 1.0
        
        # 2. Contagem de Pares
        even_count = sum(1 for d in combination if d % 2 == 0)
        num_properties_checked += 1
        if self.target_even_count_range[0] <= even_count <= self.target_even_count_range[1]:
            score += 1.0
        
        # Adicionar aqui a verificação de outras propriedades desejadas (primos, etc.)
        # Exemplo: Contagem de Primos (lista de primos até 25: 2,3,5,7,11,13,17,19,23)
        # primos_lotofacil = {2, 3, 5, 7, 11, 13, 17, 19, 23}
        # target_prime_count_range = self.strategy_specific_params.get('target_prime_count_range', (3, 6)) # Exemplo
        # if target_prime_count_range:
        #     prime_count = sum(1 for d in combination if d in primos_lotofacil)
        #     num_properties_checked += 1
        #     if target_prime_count_range[0] <= prime_count <= target_prime_count_range[1]:
        #         score += 1.0

        return score / num_properties_checked if num_properties_checked > 0 else 0.0

    def select_numbers(self,
                       scores_df: pd.DataFrame, 
                       num_to_select: int = 15,
                       selection_params: Optional[Dict[str, Any]] = None) -> List[int]:
        """
        Fase 2: Seleciona o conjunto final de N dezenas (idealmente 15 para esta estratégia).
        Gera combinações a partir de um pool de candidatas e escolhe a melhor
        com base nas propriedades globais do jogo.
        """
        if num_to_select != 15 and self.target_sum_range: # Se avalia propriedades, idealmente são 15
            print(f"AVISO ({self.get_name()}): Esta estratégia é otimizada para selecionar 15 dezenas "
                  "devido à avaliação de propriedades. Selecionando {num_to_select} dezenas.")
            # Se num_to_select != 15, a avaliação de propriedades pode não fazer sentido ou precisar de adaptação.
            # Por simplicidade, se não for 15, pode recorrer à seleção padrão.
            # return super().select_numbers(scores_df, num_to_select, selection_params)


        candidate_dezenas = scores_df.head(self.candidate_pool_size)['dezena'].tolist()

        if len(candidate_dezenas) < num_to_select:
            print(f"AVISO ({self.get_name()}): Pool de candidatos ({len(candidate_dezenas)}) "
                  f"menor que o número a selecionar ({num_to_select}). Retornando top N scores individuais.")
            return sorted(scores_df.head(num_to_select)['dezena'].tolist())

        best_combination: Optional[List[int]] = None
        max_property_score = -1.0 # Inicializa com -1 para garantir que qualquer score positivo seja melhor

        # Calcula o número de combinações possíveis para decidir se a avaliação completa é viável
        num_possible_combos = 0
        if len(candidate_dezenas) >= num_to_select:
            try:
                num_possible_combos = math.comb(len(candidate_dezenas), num_to_select)
            except AttributeError: # math.comb é Python 3.8+
                from scipy.special import comb # Fallback para scipy
                num_possible_combos = comb(len(candidate_dezenas), num_to_select, exact=True)

        
        print(f"INFO ({self.get_name()}): Pool de {len(candidate_dezenas)} dezenas. Gerando combinações de {num_to_select}. "
              f"Combinações possíveis: {num_possible_combos}. Limite de avaliação: {self.max_combinations_to_evaluate}")
        
        # Se o número de combinações for gerenciável, gera todas.
        # Caso contrário, precisaria de amostragem ou heurística.
        iterator_combinations = combinations(candidate_dezenas, num_to_select)
        
        # Se for exceder o limite, e quisermos uma amostra aleatória em vez de apenas as primeiras N:
        # import random
        # if num_possible_combos > self.max_combinations_to_evaluate:
        #     all_combinations = list(iterator_combinations) # Materializa todas, pode ser problemático
        #     iterator_combinations = random.sample(all_combinations, self.max_combinations_to_evaluate)
        #     print(f"INFO ({self.get_name()}): Amostrando {self.max_combinations_to_evaluate} combinações aleatoriamente.")

        count = 0
        for combo_tuple in iterator_combinations:
            if count >= self.max_combinations_to_evaluate and num_possible_combos > self.max_combinations_to_evaluate :
                print(f"AVISO ({self.get_name()}): Limite de {self.max_combinations_to_evaluate} combinações atingido para avaliação.")
                break
            
            combo_list = list(combo_tuple)
            property_score = self._score_combination_properties(combo_list)

            if property_score > max_property_score:
                max_property_score = property_score
                best_combination = combo_list
            count += 1
            if count % 10000 == 0 and count > 0: # Log de progresso para muitas combinações
                print(f"INFO ({self.get_name()}): Avaliadas {count} combinações...")


        if best_combination:
            print(f"INFO ({self.get_name()}): Melhor combinação encontrada com score de propriedade: {max_property_score:.4f}")
            return sorted(best_combination)
        else:
            # Se nenhuma combinação foi avaliada ou nenhuma atingiu um score > -1 (o que é improvável se houver combinações)
            print(f"AVISO ({self.get_name()}): Nenhuma combinação adequada encontrada (ou limite de avaliação/pool baixo). "
                  "Retornando top N dezenas com base no score individual de itemsets.")
            return sorted(scores_df.head(num_to_select)['dezena'].tolist())

--------------------------------------------------------------------------------
# Arquivo: src/strategies/cycle_focus_strategy.py
--------------------------------------------------------------------------------
# src/strategies/cycle_focus_strategy.py
from typing import List, Optional, Dict, Any
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Supondo que BaseStrategy e os componentes do Aggregator/DBManager são importáveis
# Ajuste os caminhos de importação conforme a estrutura do seu projeto.
from .base_strategy import BaseStrategy
from ..database_manager import DatabaseManager
from ..analysis_aggregator import AnalysisAggregator
# from ..config import config as app_config

class CycleFocusStrategy(BaseStrategy):
    """
    Estratégia que pontua dezenas com base no seu status e comportamento
    dentro dos ciclos da Lotofácil, utilizando o AnalysisAggregator.
    Prioriza dezenas para fechar ciclos, aquelas com atraso relevante no contexto
    do ciclo e com comportamento histórico favorável em fechamentos.
    """

    def __init__(self,
                 db_manager: DatabaseManager,
                 config: Dict[str, Any], # Config global do app
                 analysis_aggregator: AnalysisAggregator,
                 # Parâmetros específicos da estratégia:
                 missing_in_cycle_weight: float = 0.5, # Aumentei o peso default
                 sub_cycle_delay_weight: float = 0.3,
                 closing_behavior_weight: float = 0.2  # Ajustei para somar 1 com os outros defaults
                 ):
        super().__init__(db_manager, config, analysis_aggregator,
                         missing_in_cycle_weight=missing_in_cycle_weight,
                         sub_cycle_delay_weight=sub_cycle_delay_weight,
                         closing_behavior_weight=closing_behavior_weight)
        
        self.missing_in_cycle_weight = self.strategy_specific_params.get('missing_in_cycle_weight')
        self.sub_cycle_delay_weight = self.strategy_specific_params.get('sub_cycle_delay_weight')
        self.closing_behavior_weight = self.strategy_specific_params.get('closing_behavior_weight')
        
        # Cache para dados do agregador
        self._data_cache: Dict[str, pd.DataFrame] = {}
        
        # Validação dos pesos
        total_weight = self.missing_in_cycle_weight + self.sub_cycle_delay_weight + self.closing_behavior_weight
        if not (total_weight > 0): # Pelo menos um peso deve ser efetivo
            print(f"AVISO ({self.get_name()}): A soma dos pesos é {total_weight}. "
                  "Scores podem não ser significativos se todos os pesos forem zero.")
        # Se você quiser que a soma seja exatamente 1, adicione uma normalização ou validação aqui.

    def get_name(self) -> str:
        return (f"CycleFocusStrategy(missing_w={self.missing_in_cycle_weight:.2f}, "
                f"sub_delay_w={self.sub_cycle_delay_weight:.2f}, closing_w={self.closing_behavior_weight:.2f})")

    def get_description(self) -> str:
        return ("Pontua dezenas com base em: importância para fechar ciclos (coluna 'is_missing_in_current_cycle'), "
                "atraso atual ('current_delay' usado como proxy para atraso sub-ciclo), e "
                "comportamento histórico em fechamentos de ciclo (coluna 'cycle_closing_propensity_score'). "
                "Todos os dados são via AnalysisAggregator.")

    def _fetch_and_cache_aggregated_data(self, latest_draw_id: Optional[int] = None) -> pd.DataFrame:
        """
        Busca dados do AnalysisAggregator e os armazena em cache na instância da estratégia.
        """
        cache_key = str(latest_draw_id) if latest_draw_id is not None else "latest_overall"
        
        if cache_key not in self._data_cache:
            # print(f"INFO (Strategy:{self.get_name()}): Cache miss. Buscando dados agregados para concurso/ponto {cache_key}")
            self._data_cache[cache_key] = self.analysis_aggregator.get_historical_metrics_for_dezenas(
                latest_concurso_id=latest_draw_id
            )
        # else:
            # print(f"INFO (Strategy:{self.get_name()}): Cache hit para concurso/ponto {cache_key}")
        return self._data_cache[cache_key].copy()

    def _get_missing_dezenas_scores_df(self, latest_draw_id: Optional[int] = None) -> pd.DataFrame:
        """
        Extrai o status de 'is_missing_in_current_cycle' do DataFrame do Aggregator.
        Retorna DataFrame com ['dezena', 'missing_score'].
        """
        df_aggregated_metrics = self._fetch_and_cache_aggregated_data(latest_draw_id)
        
        # Nome da coluna esperado do Aggregator (pode vir do config para flexibilidade)
        target_col_missing = self.config.get('aggregator_col_is_missing_in_cycle', 'is_missing_in_current_cycle')

        if df_aggregated_metrics.empty or target_col_missing not in df_aggregated_metrics.columns:
            print(f"AVISO ({self.get_name()}): Coluna '{target_col_missing}' não encontrada nos dados agregados. "
                  "Assumindo score 0 para 'missing_score' para todas as dezenas.")
            return pd.DataFrame({'dezena': self._all_dezenas_list, 'missing_score': 0.0})
            
        df_missing = df_aggregated_metrics[['dezena', target_col_missing]].copy()
        # A coluna 'is_missing_in_current_cycle' deve ser 0 ou 1. Convertendo para float para consistência.
        df_missing['missing_score'] = df_missing[target_col_missing].astype(float)
        return df_missing[['dezena', 'missing_score']]

    def _get_sub_cycle_delay_scores_df(self, latest_draw_id: Optional[int] = None) -> pd.DataFrame:
        """
        Usa 'current_delay' do Aggregator como base para o score de atraso sub-ciclo.
        Normaliza o 'current_delay'.
        Retorna DataFrame com ['dezena', 'sub_cycle_delay_score'].
        """
        df_aggregated_metrics = self._fetch_and_cache_aggregated_data(latest_draw_id)
        
        target_col_delay = self.config.get('aggregator_col_current_delay', 'current_delay')

        if df_aggregated_metrics.empty or target_col_delay not in df_aggregated_metrics.columns:
            print(f"AVISO ({self.get_name()}): Coluna '{target_col_delay}' não encontrada. "
                  "Usando 0 para 'sub_cycle_delay_score' para todas as dezenas.")
            return pd.DataFrame({'dezena': self._all_dezenas_list, 'sub_cycle_delay_score': 0.0})

        df_sub_delay = df_aggregated_metrics[['dezena', target_col_delay]].copy()
        df_sub_delay = df_sub_delay.rename(columns={target_col_delay: 'raw_sub_cycle_delay'})

        scaler = MinMaxScaler()
        if 'raw_sub_cycle_delay' in df_sub_delay.columns and df_sub_delay['raw_sub_cycle_delay'].nunique() > 1:
            df_sub_delay['sub_cycle_delay_score'] = scaler.fit_transform(df_sub_delay[['raw_sub_cycle_delay']])
        elif 'raw_sub_cycle_delay' in df_sub_delay.columns: # Coluna existe, mas todos os valores são iguais
            df_sub_delay['sub_cycle_delay_score'] = 0.5 # Valor neutro
        else: # Coluna não existe (improvável, mas defensivo)
             df_sub_delay['sub_cycle_delay_score'] = 0.0
        
        return df_sub_delay[['dezena', 'sub_cycle_delay_score']]

    def _get_cycle_closing_behavior_scores_df(self, latest_draw_id: Optional[int] = None) -> pd.DataFrame:
        """
        Extrai 'cycle_closing_propensity_score' (ou similar) do Aggregator.
        Este score reflete a tendência histórica da dezena aparecer em fechamentos de ciclo.
        Retorna DataFrame com ['dezena', 'closing_behavior_score'].
        """
        df_aggregated_metrics = self._fetch_and_cache_aggregated_data(latest_draw_id)
        
        target_col_closing = self.config.get('aggregator_col_cycle_closing_propensity', 'cycle_closing_propensity_score')

        if df_aggregated_metrics.empty or target_col_closing not in df_aggregated_metrics.columns:
            print(f"AVISO ({self.get_name()}): Coluna '{target_col_closing}' não encontrada. "
                  "Usando 0 para 'closing_behavior_score' para todas as dezenas.")
            return pd.DataFrame({'dezena': self._all_dezenas_list, 'closing_behavior_score': 0.0})

        df_closing = df_aggregated_metrics[['dezena', target_col_closing]].copy()
        
        # Assumindo que o score do aggregator já está numa escala razoável ou é normalizado aqui.
        # Se o score do agregador for, por exemplo, uma contagem, a normalização é crucial.
        scaler = MinMaxScaler()
        if target_col_closing in df_closing.columns and df_closing[target_col_closing].nunique() > 1:
            df_closing['closing_behavior_score'] = scaler.fit_transform(df_closing[[target_col_closing]])
        elif target_col_closing in df_closing.columns: # Coluna existe, mas valores iguais
            # Se for um score já normalizado (ex: todos 0.5), usa esse valor.
            # Se for uma contagem (ex: todos 0), o scaler pode não funcionar bem, então 0.5 é um default.
            # O ideal é que o agregador forneça scores já significativos ou normalizáveis.
            df_closing['closing_behavior_score'] = 0.5 if df_closing[target_col_closing].iloc[0] != 0 else 0.0
        else: # Coluna não existe
             df_closing['closing_behavior_score'] = 0.0
            
        return df_closing[['dezena', 'closing_behavior_score']]

    def generate_scores(self, latest_draw_id: Optional[int] = None) -> pd.DataFrame:
        df_missing_scores = self._get_missing_dezenas_scores_df(latest_draw_id)
        df_sub_delay_scores = self._get_sub_cycle_delay_scores_df(latest_draw_id)
        df_closing_behavior_scores = self._get_cycle_closing_behavior_scores_df(latest_draw_id)

        # Merge dos dataframes de scores parciais
        # _all_dezenas_list é herdado da BaseStrategy
        base_dezenas_df = pd.DataFrame({'dezena': self._all_dezenas_list})
        df_merged = pd.merge(base_dezenas_df, df_missing_scores, on='dezena', how='left')
        df_merged = pd.merge(df_merged, df_sub_delay_scores, on='dezena', how='left')
        df_merged = pd.merge(df_merged, df_closing_behavior_scores, on='dezena', how='left')
        
        # Preencher NaNs que podem surgir dos merges (embora os métodos _get_ devam retornar todas as dezenas)
        df_merged = df_merged.fillna({
            'missing_score': 0.0, 
            'sub_cycle_delay_score': 0.0, 
            'closing_behavior_score': 0.0
        })

        if df_merged.empty: # Improvável se base_dezenas_df for usado
            return pd.DataFrame(columns=['dezena', 'score', 'ranking_strategy'])

        # Score final combinado pelos pesos
        df_merged['score'] = (
            self.missing_in_cycle_weight * df_merged['missing_score'] +
            self.sub_cycle_delay_weight * df_merged['sub_cycle_delay_score'] +
            self.closing_behavior_weight * df_merged['closing_behavior_score']
        )

        # Ordenar e adicionar ranking
        df_final_scores = df_merged.sort_values(by='score', ascending=False).reset_index(drop=True)
        df_final_scores['ranking_strategy'] = df_final_scores.index + 1

        return df_final_scores[['dezena', 'score', 'ranking_strategy']]

    # O método select_numbers usará a implementação padrão da BaseStrategy (top N scores),
    # a menos que uma lógica de seleção mais específica para ciclos seja desejada.

--------------------------------------------------------------------------------
# Arquivo: src/strategies/delay_strategies.py
--------------------------------------------------------------------------------
# src/strategies/delay_strategies.py

import pandas as pd
from typing import Optional, Set, Dict, Any

from src.config import logger, ALL_NUMBERS
# Não importa mais funções de analysis

NUM_DEZENAS_LOTOFACIL = 15
if 'ALL_NUMBERS' not in globals(): ALL_NUMBERS = list(range(1, 26))

# --- Assinatura da função alterada ---
def select_most_delayed(current_analysis: Dict[str, Any],
                        num_to_select: int = NUM_DEZENAS_LOTOFACIL
                        ) -> Optional[Set[int]]:
    """ Seleciona N dezenas MAIS ATRASADAS (usa dados do dict). """
    logger.debug(f"Estratégia 'Mais Atrasadas': Usando dados pré-calculados...")
    delay_series = current_analysis.get('current_delay')

    if delay_series is None or not isinstance(delay_series, pd.Series):
        logger.error("Série 'current_delay' não encontrada ou inválida.")
        return None

    # Trata NAs (que podem vir como pd.NA ou float('nan'))
    # Atribuímos -1 para NAs para que fiquem no fim de nlargest
    delay_series_filled = delay_series.fillna(-1)

    most_delayed = delay_series_filled.nlargest(num_to_select)
    selected_numbers = set(most_delayed.index)

    # Opcional: Verificar se selecionou algum com -1?
    if -1 in most_delayed.values:
         logger.warning("Estratégia 'Mais Atrasadas' considerou dezenas com atraso NA.")

    logger.debug(f"Estratégia 'Mais Atrasadas': Selecionadas {len(selected_numbers)}")
    return selected_numbers

--------------------------------------------------------------------------------
# Arquivo: src/strategies/frequency_strategies.py
--------------------------------------------------------------------------------
# src/strategies/frequency_strategies.py

import pandas as pd
from typing import Optional, Set, Dict, Any

from src.config import logger, ALL_NUMBERS # Usa ALL_NUMBERS
# Não importa mais funções de analysis daqui, recebe via dicionário

NUM_DEZENAS_LOTOFACIL = 15
if 'ALL_NUMBERS' not in globals(): ALL_NUMBERS = list(range(1, 26))

# --- Assinaturas das funções alteradas ---

def select_most_frequent_overall(current_analysis: Dict[str, Any],
                                 num_to_select: int = NUM_DEZENAS_LOTOFACIL
                                 ) -> Optional[Set[int]]:
    """ Seleciona N dezenas MAIS frequentes GERAL (usa dados do dict). """
    logger.debug(f"Estratégia 'Mais Frequentes': Usando dados pré-calculados...")
    freq_series = current_analysis.get('overall_freq')

    if freq_series is None or not isinstance(freq_series, pd.Series):
        logger.error("Série 'overall_freq' não encontrada ou inválida no dicionário de análise.")
        return None

    most_frequent = freq_series.nlargest(num_to_select)
    selected_numbers = set(most_frequent.index)
    logger.debug(f"Estratégia 'Mais Frequentes': Selecionadas {len(selected_numbers)}")
    return selected_numbers


def select_least_frequent_overall(current_analysis: Dict[str, Any],
                                  num_to_select: int = NUM_DEZENAS_LOTOFACIL
                                  ) -> Optional[Set[int]]:
    """ Seleciona N dezenas MENOS frequentes GERAL (usa dados do dict). """
    logger.debug(f"Estratégia 'Menos Frequentes': Usando dados pré-calculados...")
    freq_series = current_analysis.get('overall_freq')

    if freq_series is None or not isinstance(freq_series, pd.Series):
        logger.error("Série 'overall_freq' não encontrada ou inválida.")
        return None

    least_frequent = freq_series.nsmallest(num_to_select)
    selected_numbers = set(least_frequent.index)
    logger.debug(f"Estratégia 'Menos Frequentes': Selecionadas {len(selected_numbers)}")
    return selected_numbers


def select_most_frequent_recent(current_analysis: Dict[str, Any],
                                window: int = 25, # Janela precisa ser passada ou configurada
                                num_to_select: int = NUM_DEZENAS_LOTOFACIL
                                ) -> Optional[Set[int]]:
    """ Seleciona N dezenas MAIS frequentes RECENTES (usa dados do dict). """
    logger.debug(f"Estratégia 'Mais Frequentes Recentes ({window})': Usando dados pré-calculados...")
    freq_key = f'recent_freq_{window}'
    freq_series = current_analysis.get(freq_key)

    if freq_series is None or not isinstance(freq_series, pd.Series):
        logger.error(f"Série '{freq_key}' não encontrada ou inválida.")
        # Poderia tentar calcular na hora como fallback? Não, o objetivo é usar o estado.
        return None

    most_frequent = freq_series.nlargest(num_to_select)
    selected_numbers = set(most_frequent.index)
    logger.debug(f"Estratégia 'Mais Frequentes Recentes ({window})': Selecionadas {len(selected_numbers)}")
    return selected_numbers

--------------------------------------------------------------------------------
# Arquivo: src/strategies/scoring_strategies.py
--------------------------------------------------------------------------------
# src/strategies/scoring_strategies.py

from typing import Optional, Set, Dict, Any
import pandas as pd

from src.config import logger, ALL_NUMBERS
# Importa APENAS o scorer para calcular o score a partir dos dados recebidos
from src.scorer import calculate_scores
# NÃO importa mais o agregador

NUM_DEZENAS_LOTOFACIL = 15
if 'ALL_NUMBERS' not in globals(): ALL_NUMBERS = list(range(1, 26))

# --- Assinatura da função alterada ---
def select_top_scored(current_analysis: Dict[str, Any],
                      num_to_select: int = NUM_DEZENAS_LOTOFACIL,
                      scoring_config: Optional[Dict[str, Dict]] = None
                      ) -> Optional[Set[int]]:
    """
    Seleciona N dezenas com maior pontuação, usando dados JÁ CALCULADOS.
    """
    logger.debug(f"Estratégia 'Top Score': Usando dados pré-calculados...")

    # *** NÃO CHAMA MAIS o agregador get_consolidated_analysis ***
    # Os dados necessários já devem estar em current_analysis

    # 1. Calcula os scores a partir dos dados recebidos
    scores = calculate_scores(current_analysis, config=scoring_config) # Passa o dict recebido
    if scores is None:
        logger.error("Falha ao calcular scores para estratégia 'Top Score'.")
        return None
    if scores.empty:
         logger.warning("Scores calculados resultaram em Series vazia.")
         return None

    # 2. Seleciona os top N scores
    top_scored = scores.nlargest(num_to_select)
    selected_numbers = set(top_scored.index)

    logger.debug(f"Estratégia 'Top Score': Selecionadas {len(selected_numbers)}")
    return selected_numbers

--------------------------------------------------------------------------------
# Arquivo: src/strategies/simple_recency_delay_strategy.py
--------------------------------------------------------------------------------
# src/strategies/simple_recency_delay_strategy.py
from typing import List, Optional, Dict, Any
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Supondo que BaseStrategy e os componentes do Aggregator/DBManager são importáveis
# Ajuste os caminhos de importação conforme a estrutura do seu projeto.
# Se os arquivos estiverem em src/
from .base_strategy import BaseStrategy
from ..database_manager import DatabaseManager
from ..analysis_aggregator import AnalysisAggregator
# from ..config import config as app_config # Se você importar o config global

class SimpleRecencyAndDelayStrategy(BaseStrategy):
    """
    Estratégia que prioriza dezenas com base na frequência recente (recência)
    e no atraso atual, utilizando o AnalysisAggregator para obter os dados.
    """

    def __init__(self,
                 db_manager: DatabaseManager,
                 config: Dict[str, Any], # Config global do app
                 analysis_aggregator: AnalysisAggregator,
                 # Parâmetros específicos da estratégia:
                 target_recent_window_suffix: str = "10", # Ex: "10" para usar 'recent_frequency_window_10' do Aggregator
                 delay_weight: float = 0.5,
                 frequency_weight: float = 0.5
                 ):
        super().__init__(db_manager, config, analysis_aggregator,
                         # Passando os parâmetros específicos para BaseStrategy se ela os armazenar
                         target_recent_window_suffix=target_recent_window_suffix,
                         delay_weight=delay_weight,
                         frequency_weight=frequency_weight)
        
        # O nome da coluna de frequência recente esperada do Aggregator
        self.target_recent_window_col = f"recent_frequency_window_{target_recent_window_suffix}"
        
        self.delay_weight = delay_weight
        self.frequency_weight = frequency_weight
        
        # Cache para dados do agregador, para evitar múltiplas chamadas com o mesmo latest_draw_id
        self._data_cache: Dict[str, pd.DataFrame] = {}

        if not (0 <= self.delay_weight <= 1 and 0 <= self.frequency_weight <= 1 and (self.delay_weight + self.frequency_weight > 0)):
            # Permitir soma > 1 se os pesos forem relativos, mas não negativos e pelo menos um > 0.
            # Se a soma dos pesos positivos deve ser 1, uma validação adicional seria necessária.
            raise ValueError("Os pesos de delay e frequência devem estar entre 0 e 1, e pelo menos um deve ser positivo.")

    def get_name(self) -> str:
        # O nome da estratégia pode incluir seus parâmetros principais para fácil identificação
        return (f"SimpleRecencyAndDelayStrategy(target_window_col={self.target_recent_window_col}, "
                f"delay_w={self.delay_weight}, freq_w={self.frequency_weight})")

    def get_description(self) -> str:
        return (f"Combina scores normalizados de frequência recente (usando a coluna {self.target_recent_window_col} "
                "do AnalysisAggregator) e atraso atual das dezenas.")

    def _fetch_and_cache_aggregated_data(self, latest_draw_id: Optional[int] = None) -> pd.DataFrame:
        """
        Busca dados do AnalysisAggregator e os armazena em cache na instância da estratégia
        para o latest_draw_id fornecido.
        """
        cache_key = str(latest_draw_id) if latest_draw_id is not None else "latest_overall"
        
        if cache_key not in self._data_cache:
            # print(f"INFO (Strategy:{self.get_name()}): Cache miss. Buscando dados agregados para concurso/ponto {cache_key}")
            self._data_cache[cache_key] = self.analysis_aggregator.get_historical_metrics_for_dezenas(
                latest_concurso_id=latest_draw_id
            )
        # else:
            # print(f"INFO (Strategy:{self.get_name()}): Cache hit para concurso/ponto {cache_key}")
        return self._data_cache[cache_key].copy() # Retorna cópia para evitar modificação acidental do cache

    def _get_recent_frequency_df(self, latest_draw_id: Optional[int] = None) -> pd.DataFrame:
        """
        Extrai a frequência recente do DataFrame consolidado fornecido pelo AnalysisAggregator.
        Retorna um DataFrame com colunas ['dezena', 'recent_frequency'].
        """
        df_aggregated_metrics = self._fetch_and_cache_aggregated_data(latest_draw_id)

        if df_aggregated_metrics.empty or self.target_recent_window_col not in df_aggregated_metrics.columns:
            print(f"AVISO ({self.get_name()}): Coluna de frequência recente '{self.target_recent_window_col}' "
                  "não encontrada nos dados agregados. Verifique se o AnalysisAggregator a fornece ou ajuste "
                  f"o parâmetro 'target_recent_window_suffix'. Usando frequência 0 para todas as dezenas.")
            return pd.DataFrame({'dezena': self._all_dezenas_list, 'recent_frequency': 0.0})
        
        return df_aggregated_metrics[['dezena', self.target_recent_window_col]].rename(
            columns={self.target_recent_window_col: 'recent_frequency'}
        )

    def _get_current_delays_df(self, latest_draw_id: Optional[int] = None) -> pd.DataFrame:
        """
        Extrai os atrasos atuais do DataFrame consolidado fornecido pelo AnalysisAggregator.
        Retorna um DataFrame com colunas ['dezena', 'current_delay'].
        """
        df_aggregated_metrics = self._fetch_and_cache_aggregated_data(latest_draw_id)

        if df_aggregated_metrics.empty or 'current_delay' not in df_aggregated_metrics.columns:
            print(f"AVISO ({self.get_name()}): Coluna 'current_delay' não encontrada nos dados agregados. "
                  "Usando atraso 0 para todas as dezenas.")
            return pd.DataFrame({'dezena': self._all_dezenas_list, 'current_delay': 0.0})
            
        return df_aggregated_metrics[['dezena', 'current_delay']]

    def generate_scores(self, latest_draw_id: Optional[int] = None) -> pd.DataFrame:
        # Os métodos _get_ já utilizam o _fetch_and_cache_aggregated_data internamente
        df_recent_freq = self._get_recent_frequency_df(latest_draw_id)
        df_current_delays = self._get_current_delays_df(latest_draw_id)

        # Merge dos dataframes de métricas (embora venham da mesma fonte agregada,
        # esta etapa garante o alinhamento e a presença de todas as dezenas).
        # O _all_dezenas_list é herdado de BaseStrategy ou pode ser pego do aggregator.
        base_dezenas_df = pd.DataFrame({'dezena': self._all_dezenas_list})
        
        df_merged = pd.merge(base_dezenas_df, df_recent_freq, on='dezena', how='left')
        df_merged = pd.merge(df_merged, df_current_delays, on='dezena', how='left')
        
        # Preencher NaNs que podem surgir se alguma dezena não estiver no resultado do aggregator
        # (embora o aggregator deva idealmente retornar todas as dezenas com fillna apropriado).
        # Para 'recent_frequency' e 'current_delay', 0 é um default razoável se a informação estiver ausente.
        df_merged['recent_frequency'] = df_merged['recent_frequency'].fillna(0)
        df_merged['current_delay'] = df_merged['current_delay'].fillna(0)
        
        if df_merged.empty: # Pouco provável se base_dezenas_df for usado
            return pd.DataFrame(columns=['dezena', 'score', 'ranking_strategy'])

        # Normalização dos scores (MinMax para colocar entre 0 e 1)
        scaler = MinMaxScaler()
        
        # Normalizar frequência recente
        if 'recent_frequency' in df_merged.columns and df_merged['recent_frequency'].nunique() > 1:
            df_merged['freq_score_norm'] = scaler.fit_transform(df_merged[['recent_frequency']])
        elif 'recent_frequency' in df_merged.columns: # Existe, mas todos os valores são iguais
            df_merged['freq_score_norm'] = 0.5 # Atribui um valor neutro
        else: # Coluna não existe
            df_merged['freq_score_norm'] = 0.0

        # Normalizar atraso atual
        if 'current_delay' in df_merged.columns and df_merged['current_delay'].nunique() > 1:
            df_merged['delay_score_norm'] = scaler.fit_transform(df_merged[['current_delay']])
        elif 'current_delay' in df_merged.columns: # Existe, mas todos os valores são iguais
            df_merged['delay_score_norm'] = 0.5 # Atribui um valor neutro
        else: # Coluna não existe
            df_merged['delay_score_norm'] = 0.0
            
        # Score final combinado pelos pesos
        df_merged['score'] = (self.frequency_weight * df_merged['freq_score_norm'] +
                              self.delay_weight * df_merged['delay_score_norm'])

        # Ordenar e adicionar ranking
        df_final_scores = df_merged.sort_values(by='score', ascending=False).reset_index(drop=True)
        df_final_scores['ranking_strategy'] = df_final_scores.index + 1

        return df_final_scores[['dezena', 'score', 'ranking_strategy']]

--------------------------------------------------------------------------------
# Arquivo: src/strategies/trend_recurrence_strategy.py
--------------------------------------------------------------------------------
# src/strategies/trend_recurrence_strategy.py
from typing import List, Optional, Dict, Any
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Supondo que BaseStrategy e os componentes do Aggregator/DBManager são importáveis
# Ajuste os caminhos de importação conforme a estrutura do seu projeto.
from .base_strategy import BaseStrategy
from ..database_manager import DatabaseManager
from ..analysis_aggregator import AnalysisAggregator
# from ..config import config as app_config

class TrendAndRecurrenceStrategy(BaseStrategy):
    """
    Estratégia que foca em dezenas com tendência de ranking ascendente
    e alta probabilidade de recorrência baseada no atraso atual,
    utilizando o AnalysisAggregator para obter os dados.
    """

    def __init__(self,
                 db_manager: DatabaseManager,
                 config: Dict[str, Any], # Config global do app
                 analysis_aggregator: AnalysisAggregator,
                 # Parâmetros específicos da estratégia:
                 trend_weight: float = 0.6,
                 recurrence_weight: float = 0.4,
                 min_recurrence_cdf_filter: float = 0.0 # Filtro opcional para CDF mínimo antes da pontuação
                 ):
        super().__init__(db_manager, config, analysis_aggregator,
                         trend_weight=trend_weight,
                         recurrence_weight=recurrence_weight,
                         min_recurrence_cdf_filter=min_recurrence_cdf_filter)
        
        self.trend_weight = trend_weight
        self.recurrence_weight = recurrence_weight
        self.min_recurrence_cdf_filter = min_recurrence_cdf_filter
        
        # Cache para dados do agregador
        self._data_cache: Dict[str, pd.DataFrame] = {}

        if not (0 <= self.trend_weight <= 1 and 0 <= self.recurrence_weight <= 1):
            raise ValueError("Os pesos de tendência e recorrência devem estar entre 0 e 1.")
        if not (self.trend_weight + self.recurrence_weight > 0): # Pelo menos um peso deve ser positivo
            # Considerar se a soma dos pesos deve ser 1.0, ou se são apenas relativos.
            # Se a soma precisar ser 1, adicione validação ou normalize os pesos.
            print(f"AVISO ({self.get_name()}): A soma dos pesos é 0. Scores podem não ser significativos.")


    def get_name(self) -> str:
        return (f"TrendAndRecurrenceStrategy(trend_w={self.trend_weight}, "
                f"recur_w={self.recurrence_weight}, min_cdf_filter={self.min_recurrence_cdf_filter})")

    def get_description(self) -> str:
        return ("Combina scores normalizados de tendência de ranking (coluna 'rank_slope' do Aggregator) e "
                "probabilidade de recorrência (coluna 'recurrence_cdf' do Aggregator). "
                f"Aplica um filtro opcional de CDF de recorrência mínimo de {self.min_recurrence_cdf_filter} antes de pontuar.")

    def _fetch_and_cache_aggregated_data(self, latest_draw_id: Optional[int] = None) -> pd.DataFrame:
        """
        Busca dados do AnalysisAggregator e os armazena em cache na instância da estratégia.
        """
        cache_key = str(latest_draw_id) if latest_draw_id is not None else "latest_overall"
        
        if cache_key not in self._data_cache:
            # print(f"INFO (Strategy:{self.get_name()}): Cache miss. Buscando dados agregados para concurso/ponto {cache_key}")
            self._data_cache[cache_key] = self.analysis_aggregator.get_historical_metrics_for_dezenas(
                latest_concurso_id=latest_draw_id
            )
        # else:
            # print(f"INFO (Strategy:{self.get_name()}): Cache hit para concurso/ponto {cache_key}")
        return self._data_cache[cache_key].copy()

    def _get_rank_trend_metrics_df(self, latest_draw_id: Optional[int] = None) -> pd.DataFrame:
        """
        Extrai métricas de tendência de rank do DataFrame consolidado do AnalysisAggregator.
        Espera uma coluna como 'rank_slope'.
        Retorna DataFrame com colunas ['dezena', 'trend_metric'].
        """
        df_aggregated_metrics = self._fetch_and_cache_aggregated_data(latest_draw_id)
        
        target_col_trend = self.config.get('aggregator_col_rank_trend_slope', 'rank_slope') # Nome da coluna do config ou default

        if df_aggregated_metrics.empty or target_col_trend not in df_aggregated_metrics.columns:
            print(f"AVISO ({self.get_name()}): Coluna de tendência '{target_col_trend}' não encontrada nos dados agregados. "
                  "Verifique se o AnalysisAggregator a fornece. Usando métrica de tendência 0 para todas as dezenas.")
            return pd.DataFrame({'dezena': self._all_dezenas_list, 'trend_metric': 0.0})
            
        return df_aggregated_metrics[['dezena', target_col_trend]].rename(
            columns={target_col_trend: 'trend_metric'}
        )

    def _get_recurrence_cdf_df(self, latest_draw_id: Optional[int] = None) -> pd.DataFrame:
        """
        Extrai a probabilidade de recorrência (CDF do atraso atual) do DataFrame do AnalysisAggregator.
        Espera uma coluna como 'recurrence_cdf'.
        Retorna DataFrame com colunas ['dezena', 'recurrence_cdf'].
        """
        df_aggregated_metrics = self._fetch_and_cache_aggregated_data(latest_draw_id)
        
        target_col_cdf = self.config.get('aggregator_col_recurrence_cdf', 'recurrence_cdf')

        if df_aggregated_metrics.empty or target_col_cdf not in df_aggregated_metrics.columns:
            print(f"AVISO ({self.get_name()}): Coluna de CDF de recorrência '{target_col_cdf}' não encontrada. "
                  "Usando CDF 0 para todas as dezenas.")
            return pd.DataFrame({'dezena': self._all_dezenas_list, 'recurrence_cdf': 0.0})
            
        return df_aggregated_metrics[['dezena', target_col_cdf]].rename(
            columns={target_col_cdf: 'recurrence_cdf'}
        )

    def generate_scores(self, latest_draw_id: Optional[int] = None) -> pd.DataFrame:
        df_trend_metrics = self._get_rank_trend_metrics_df(latest_draw_id)
        df_recurrence_cdf = self._get_recurrence_cdf_df(latest_draw_id)

        base_dezenas_df = pd.DataFrame({'dezena': self._all_dezenas_list})
        df_merged = pd.merge(base_dezenas_df, df_trend_metrics, on='dezena', how='left')
        df_merged = pd.merge(df_merged, df_recurrence_cdf, on='dezena', how='left')
        
        # Preenchimento de NaNs
        df_merged['trend_metric'] = df_merged['trend_metric'].fillna(0.0)
        df_merged['recurrence_cdf'] = df_merged['recurrence_cdf'].fillna(0.0)

        if df_merged.empty: # Pouco provável se base_dezenas_df for usado
            return pd.DataFrame(columns=['dezena', 'score', 'ranking_strategy'])
            
        # Aplicar filtro opcional de CDF mínimo ANTES da pontuação e normalização
        if self.min_recurrence_cdf_filter > 0:
            df_merged = df_merged[df_merged['recurrence_cdf'] >= self.min_recurrence_cdf_filter].copy()
            if df_merged.empty:
                 print(f"AVISO ({self.get_name()}): Nenhuma dezena passou pelo filtro min_recurrence_cdf_filter={self.min_recurrence_cdf_filter}.")
                 return pd.DataFrame({'dezena': [], 'score': [], 'ranking_strategy': []}) # Retorna DF vazio com colunas


        # Se após o filtro o df_merged ficar vazio, não há o que pontuar.
        if df_merged.empty:
            return pd.DataFrame({'dezena': [], 'score': [], 'ranking_strategy': []})

        scaler = MinMaxScaler()

        # Normalizar métrica de tendência
        if 'trend_metric' in df_merged.columns and df_merged['trend_metric'].nunique() > 1:
            df_merged['trend_score_norm'] = scaler.fit_transform(df_merged[['trend_metric']])
        elif 'trend_metric' in df_merged.columns:
            df_merged['trend_score_norm'] = 0.5 
        else:
            df_merged['trend_score_norm'] = 0.0
        
        # Normalizar CDF de recorrência (embora já seja 0-1, escalar pode ajudar se a distribuição for ruim)
        if 'recurrence_cdf' in df_merged.columns and df_merged['recurrence_cdf'].nunique() > 1:
            df_merged['recurrence_score_norm'] = scaler.fit_transform(df_merged[['recurrence_cdf']])
        elif 'recurrence_cdf' in df_merged.columns:
            df_merged['recurrence_score_norm'] = df_merged['recurrence_cdf'].iloc[0] if not df_merged.empty else 0.5 # Usa o valor único ou 0.5
        else:
            df_merged['recurrence_score_norm'] = 0.0

        # Score final combinado
        df_merged['score'] = (self.trend_weight * df_merged['trend_score_norm'] +
                              self.recurrence_weight * df_merged['recurrence_score_norm'])

        df_final_scores = df_merged.sort_values(by='score', ascending=False).reset_index(drop=True)
        df_final_scores['ranking_strategy'] = df_final_scores.index + 1
        
        return df_final_scores[['dezena', 'score', 'ranking_strategy']]

--------------------------------------------------------------------------------
# Arquivo: src/table_updater.py
--------------------------------------------------------------------------------
# src/table_updater.py

import pandas as pd
import sqlite3
import numpy as np
from typing import List, Optional, Dict, Set

# Importa constantes e logger
from src.config import (
    logger, ALL_NUMBERS, DEFAULT_SNAPSHOT_INTERVALS, DATABASE_PATH,
    NEW_BALL_COLUMNS, TABLE_NAME
)

# Fallbacks
if 'ALL_NUMBERS' not in globals(): ALL_NUMBERS = list(range(1, 26))
if 'DEFAULT_SNAPSHOT_INTERVALS' not in globals(): DEFAULT_SNAPSHOT_INTERVALS = [10, 25, 50, 100, 200, 300, 400, 500]
if 'NEW_BALL_COLUMNS' not in globals(): NEW_BALL_COLUMNS = [f'b{i}' for i in range(1,16)]
if 'TABLE_NAME' not in globals(): TABLE_NAME = 'sorteios'

# Importa funções do DB Manager
from src.database_manager import (
    read_data_from_db, get_last_freq_snapshot_contest, save_freq_snapshot,
    get_closest_freq_snapshot, create_freq_snap_table, FREQ_SNAP_TABLE_NAME,
    create_chunk_stats_final_table, save_chunk_final_stats_row, # <<< Novas
    get_last_contest_in_chunk_stats_final, get_chunk_final_stats_table_name # <<< Novas
)

BASE_COLS: List[str] = ['concurso'] + NEW_BALL_COLUMNS


# --- update_freq_geral_snap_table ---
# (Código idêntico ao da última versão correta)
def update_freq_geral_snap_table(intervals: List[int] = DEFAULT_SNAPSHOT_INTERVALS, force_rebuild: bool = False):
    logger.info(f"Iniciando atualização snapshots de frequência geral...")
    create_freq_snap_table(); last_snapshot_contest = 0; current_freq_counts = pd.Series(0, index=ALL_NUMBERS)
    if force_rebuild:
        logger.warning(f"REBUILD: Apagando snapshots de '{FREQ_SNAP_TABLE_NAME}'.");
        try:
            with sqlite3.connect(DATABASE_PATH) as conn: conn.execute(f"DELETE FROM {FREQ_SNAP_TABLE_NAME};")
        except sqlite3.Error as e: logger.error(f"Erro limpar '{FREQ_SNAP_TABLE_NAME}': {e}"); return
    else:
        last_snapshot_contest_val = get_last_freq_snapshot_contest()
        if last_snapshot_contest_val is not None:
            last_snapshot_contest = last_snapshot_contest_val; snap_info = get_closest_freq_snapshot(last_snapshot_contest)
            if snap_info: _, current_freq_counts = snap_info; logger.info(f"Continuando do snapshot {last_snapshot_contest}.")
            else: logger.warning(f"Snapshot {last_snapshot_contest} não encontrado? Recalculando."); last_snapshot_contest = 0
        else: logger.info(f"Nenhum snapshot. Calculando do início."); last_snapshot_contest = 0
    start_processing_from = last_snapshot_contest + 1
    df_new_draws = read_data_from_db(table_name=TABLE_NAME, columns=BASE_COLS, concurso_minimo=start_processing_from)
    if df_new_draws is None or df_new_draws.empty: logger.info(f"Nenhum sorteio novo após {last_snapshot_contest}."); return
    max_contest_in_data_val = df_new_draws['concurso'].max();
    if pd.isna(max_contest_in_data_val): logger.error("max_contest inválido."); return
    max_contest_in_data = int(max_contest_in_data_val)
    logger.info(f"Processando {len(df_new_draws)} sorteios ({start_processing_from} a {max_contest_in_data})...")
    snapshot_points_to_save = set();
    for interval in intervals:
        if interval <= 0: continue
        first_multiple = ((start_processing_from + interval - 1) // interval) * interval
        snapshot_points_to_save.update(range(first_multiple, max_contest_in_data + 1, interval))
    sorted_snapshot_points = sorted(list(snapshot_points_to_save)); snapshot_idx = 0; processed_count = 0; snapshots_saved_count = 0
    current_freq_counts = current_freq_counts.astype(int)
    for index, row in df_new_draws.iterrows():
        current_concurso_val = row['concurso'];
        if pd.isna(current_concurso_val): continue
        current_concurso = int(current_concurso_val)
        drawn_numbers = {int(num) for num in row[NEW_BALL_COLUMNS].dropna().values}
        for num in drawn_numbers:
            if num in current_freq_counts.index: current_freq_counts[num] += 1
        processed_count += 1
        if snapshot_idx < len(sorted_snapshot_points) and current_concurso == sorted_snapshot_points[snapshot_idx]:
            save_freq_snapshot(current_concurso, current_freq_counts.copy())
            snapshots_saved_count += 1; snapshot_idx += 1
            if snapshots_saved_count % 50 == 0: logger.info(f"{snapshots_saved_count}/{len(sorted_snapshot_points)} snapshots salvos...")
        if processed_count % 500 == 0: logger.info(f"Processados {processed_count}/{len(df_new_draws)} sorteios...")
    logger.info(f"Atualização/Reconstrução de '{FREQ_SNAP_TABLE_NAME}' concluída. {snapshots_saved_count} snapshots.")


# --- NOVA FUNÇÃO PARA STATS FINAIS DE CHUNK ---
def update_chunk_final_stats_table(interval_size: int, force_rebuild: bool = False):
    """
    Calcula e salva as estatísticas FINAIS (freq, rank) de cada bloco completo.
    Args:
        interval_size (int): Tamanho do bloco.
        force_rebuild (bool): Se True, reconstrói do zero.
    """
    table_name = get_chunk_final_stats_table_name(interval_size)
    logger.info(f"Iniciando atualização/rebuild da tabela '{table_name}'...")

    create_chunk_stats_final_table(interval_size) # Garante que a tabela exista

    last_processed_chunk_end = 0 # Começa do zero se for rebuild ou tabela vazia
    if force_rebuild:
        logger.warning(f"REBUILD: Apagando dados de '{table_name}'.")
        try:
            with sqlite3.connect(DATABASE_PATH) as conn: conn.execute(f"DELETE FROM {table_name};")
        except sqlite3.Error as e: logger.error(f"Erro ao limpar '{table_name}': {e}"); return
    else:
        # Descobre o último concurso final de chunk já processado e salvo
        last_processed_val = get_last_contest_in_chunk_stats_final(interval_size)
        if last_processed_val is not None:
            last_processed_chunk_end = last_processed_val
            logger.info(f"Último chunk final processado para intervalo {interval_size}: {last_processed_chunk_end}")
        else:
             logger.info(f"Nenhum chunk final encontrado para intervalo {interval_size}. Calculando do início.")

    # Determina a partir de qual concurso ler os sorteios
    # Precisamos ler desde o início do PRIMEIRO chunk incompleto
    start_processing_from = last_processed_chunk_end + 1

    df_new_draws = read_data_from_db(table_name=TABLE_NAME, columns=BASE_COLS, concurso_minimo=start_processing_from)

    if df_new_draws is None or df_new_draws.empty:
        logger.info(f"Nenhum sorteio novo encontrado após {last_processed_chunk_end}. Tabela '{table_name}' atualizada.")
        return

    max_contest_in_data = int(df_new_draws['concurso'].max())
    logger.info(f"Processando {len(df_new_draws)} sorteios ({start_processing_from} a {max_contest_in_data}) para chunks de {interval_size}...")

    current_chunk_counts = pd.Series(0, index=ALL_NUMBERS).astype(int)
    current_chunk_start_contest = ((start_processing_from - 1) // interval_size) * interval_size + 1
    chunks_saved_count = 0

    # Itera sobre os novos sorteios
    for index, row in df_new_draws.iterrows():
        current_concurso_val = row['concurso']
        if pd.isna(current_concurso_val): continue
        current_concurso = int(current_concurso_val)
        drawn_numbers = {int(num) for num in row[NEW_BALL_COLUMNS].dropna().values}

        # Verifica se este concurso inicia um novo chunk (exceto o primeiríssimo)
        is_start_of_new_chunk = (current_concurso - 1) % interval_size == 0
        if is_start_of_new_chunk and current_concurso != start_processing_from:
            # Este é o início de um NOVO chunk, o anterior acabou no concurso `current_concurso - 1`.
            # Mas só salvamos quando o chunk REALMENTE termina (múltiplo do intervalo)
            # A lógica de salvar está no if abaixo. Aqui só precisamos resetar.
            logger.debug(f"Resetando contagem para novo chunk {interval_size} no concurso {current_concurso}")
            current_chunk_counts = pd.Series(0, index=ALL_NUMBERS).astype(int)
            current_chunk_start_contest = current_concurso

        # Incrementa contagem CUMULATIVA dentro do chunk atual
        for num in drawn_numbers:
            if num in current_chunk_counts.index:
                current_chunk_counts[num] += 1

        # Verifica se este concurso é o FIM de um chunk
        if current_concurso % interval_size == 0:
            logger.debug(f"Fim do chunk {interval_size} detectado no concurso {current_concurso}. Calculando e salvando stats...")
            # Calcula o rank baseado na frequência acumulada deste chunk
            ranks = current_chunk_counts.rank(method='min', ascending=False, pct=False).astype(int)
            # Salva a frequência final E o rank final deste chunk
            save_chunk_final_stats_row(interval_size, current_concurso, current_chunk_counts.copy(), ranks.copy())
            chunks_saved_count += 1
            if chunks_saved_count % 50 == 0: logger.info(f"{chunks_saved_count} chunks finais de {interval_size} salvos...")

    logger.info(f"Atualização/Reconstrução da tabela '{table_name}' concluída. {chunks_saved_count} chunks finais salvos/atualizados.")

--------------------------------------------------------------------------------
# Arquivo: src/visualization/__init__.py
--------------------------------------------------------------------------------


--------------------------------------------------------------------------------
# Arquivo: src/visualization/plotter.py
--------------------------------------------------------------------------------
# src/visualization/plotter.py
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import seaborn as sns
import os
import logging
from typing import List, Optional, Dict, Any 
from pathlib import Path # <<< --- IMPORTAÇÃO DE PATH ADICIONADA/CONFIRMADA

from src.database_manager import DatabaseManager
from src.config import PLOT_DIR_CONFIG, ALL_NUMBERS

logger = logging.getLogger(__name__)

def ensure_output_dir(output_dir: str):
    """Garante que o diretório de saída exista."""
    Path(output_dir).mkdir(parents=True, exist_ok=True) # Agora 'Path' está definido


def plot_frequency(
    df_frequency: pd.DataFrame,
    metric_type: str = 'Absoluta',
    output_dir: str = str(PLOT_DIR_CONFIG)
):
    """
    Plota a frequência (absoluta ou relativa) das dezenas.
    """
    if df_frequency is None or df_frequency.empty:
        logger.warning(f"DataFrame de frequência {metric_type.lower()} está vazio. Não é possível gerar o gráfico.")
        return

    if metric_type == 'Absoluta':
        freq_col_to_plot = next((col for col in ['Frequencia Absoluta', 'frequencia_absoluta'] if col in df_frequency.columns), None)
    elif metric_type == 'Relativa':
        freq_col_to_plot = next((col for col in ['Frequencia Relativa', 'frequencia_relativa'] if col in df_frequency.columns), None)
    else:
        logger.error(f"Tipo de métrica de frequência desconhecido: {metric_type}")
        return
        
    if not freq_col_to_plot:
        logger.error(f"Coluna de frequência para '{metric_type}' não encontrada. Colunas: {df_frequency.columns.tolist()}")
        return
        
    dezena_col = next((col for col in ['Dezena', 'dezena'] if col in df_frequency.columns), None)
    if not dezena_col:
        logger.error(f"Coluna 'Dezena' ou 'dezena' não encontrada. Colunas: {df_frequency.columns.tolist()}")
        return

    ensure_output_dir(output_dir)
    
    plt.figure(figsize=(14, 7)) # Aumentado um pouco para melhor visualização dos ticks
    plot_data = df_frequency.sort_values(by=freq_col_to_plot, ascending=False)
    
    # Garante que a coluna de dezenas seja tratada como categórica para ordenação correta no barplot se for numérica
    plot_data[dezena_col] = plot_data[dezena_col].astype(str) 
    
    sns.barplot(x=dezena_col, y=freq_col_to_plot, data=plot_data, palette="viridis", order=plot_data[dezena_col])
    plt.title(f'Frequência {metric_type} das Dezenas da Lotofácil')
    plt.xlabel('Dezena')
    plt.ylabel(f'Frequência {metric_type}')
    # Ajusta os ticks do eixo X para mostrar todos os números se ALL_NUMBERS estiver disponível e for o caso
    # Isso é mais útil se as dezenas forem de 1 a 25 e quisermos garantir que todas apareçam.
    # No entanto, a ordenação por frequência é mais comum.
    # Se as dezenas forem numéricas e quisermos ordenação numérica no eixo:
    # df_frequency[dezena_col] = pd.to_numeric(df_frequency[dezena_col])
    # sns.barplot(x=dezena_col, y=freq_col_to_plot, data=df_frequency.sort_values(by=dezena_col), ...)
    
    plt.xticks(rotation=70, ha="right") # Aumentada a rotação para melhor visualização
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    
    plot_filename = f"frequencia_{metric_type.lower().replace(' ', '_')}_dezenas.png"
    full_plot_path = os.path.join(output_dir, plot_filename)
    
    try:
        plt.savefig(full_plot_path)
        logger.info(f"Gráfico de frequência {metric_type.lower()} salvo em: {full_plot_path}")
    except Exception as e:
        logger.error(f"Erro ao salvar o gráfico de frequência {metric_type.lower()} em '{full_plot_path}': {e}", exc_info=True)
    finally:
        plt.close()


def plot_delay(
    df_delay: pd.DataFrame,
    delay_type: str = 'Atual', 
    output_dir: str = str(PLOT_DIR_CONFIG)
):
    """
    Plota o atraso (atual, máximo ou médio) das dezenas.
    """
    if df_delay is None or df_delay.empty:
        logger.warning(f"DataFrame de atraso '{delay_type}' está vazio. Não é possível gerar o gráfico.")
        return

    if delay_type == 'Atual':
        delay_col_to_plot = next((col for col in ['Atraso Atual', 'atraso_atual', 'Atraso'] if col in df_delay.columns), None)
    elif delay_type == 'Maximo':
        delay_col_to_plot = next((col for col in ['Atraso Maximo', 'atraso_maximo'] if col in df_delay.columns), None)
    elif delay_type == 'Medio':
        delay_col_to_plot = next((col for col in ['Atraso Medio', 'atraso_medio'] if col in df_delay.columns), None)
    else:
        logger.error(f"Tipo de atraso desconhecido: {delay_type}")
        return

    if not delay_col_to_plot:
        logger.error(f"Coluna de atraso para '{delay_type}' não encontrada. Colunas: {df_delay.columns.tolist()}")
        return

    dezena_col = next((col for col in ['Dezena', 'dezena'] if col in df_delay.columns), None)
    if not dezena_col:
        logger.error(f"Coluna 'Dezena' ou 'dezena' não encontrada. Colunas: {df_delay.columns.tolist()}")
        return

    ensure_output_dir(output_dir)

    plt.figure(figsize=(14, 7)) # Aumentado
    plot_data = df_delay.sort_values(by=delay_col_to_plot, ascending=False)
    plot_data[dezena_col] = plot_data[dezena_col].astype(str) # Para ordenação correta no barplot

    sns.barplot(x=dezena_col, y=delay_col_to_plot, data=plot_data, palette="coolwarm", order=plot_data[dezena_col])
    plt.title(f'Atraso {delay_type} das Dezenas da Lotofácil')
    plt.xlabel('Dezena')
    plt.ylabel(f'Atraso {delay_type} (em concursos)')
    plt.xticks(rotation=70, ha="right") # Aumentada a rotação
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()

    plot_filename = f"atraso_{delay_type.lower()}_dezenas.png"
    full_plot_path = os.path.join(output_dir, plot_filename)
    
    try:
        plt.savefig(full_plot_path)
        logger.info(f"Gráfico de atraso {delay_type.lower()} salvo em: {full_plot_path}")
    except Exception as e:
        logger.error(f"Erro ao salvar o gráfico de atraso {delay_type.lower()} em '{full_plot_path}': {e}", exc_info=True)
    finally:
        plt.close()


def plot_chunk_metric_evolution(
    db_manager: DatabaseManager,
    chunk_type: str,
    chunk_size: int,
    metric_to_plot: str, 
    dezenas_to_plot: List[int],
    output_dir: str = str(PLOT_DIR_CONFIG)
):
    """
    Plota a evolução de uma métrica específica para um conjunto de dezenas
    ao longo de diferentes chunks sequenciais.
    """
    logger.info(f"Gerando gráfico de evolução da métrica '{metric_to_plot}' para dezenas {dezenas_to_plot} em chunks {chunk_type}_{chunk_size}.")

    metric_map: Dict[str, Dict[str, str]] = { # Type hint para metric_map
        "Frequencia Absoluta": {"table_suffix": "frequency", "column_name": "frequencia_absoluta"},
    }

    if metric_to_plot not in metric_map:
        logger.error(f"Métrica '{metric_to_plot}' não mapeada. Métricas: {list(metric_map.keys())}")
        return

    metric_info = metric_map[metric_to_plot]
    table_name_core = metric_info["table_suffix"] 
    metric_column = metric_info["column_name"]    

    table_name = f"evol_metric_{table_name_core}_{chunk_type}_{chunk_size}"

    df_evolution: Optional[pd.DataFrame] = None
    try:
        if not db_manager.table_exists(table_name):
            logger.error(f"Tabela '{table_name}' não encontrada.")
            return
        df_evolution = db_manager.load_dataframe_from_db(table_name)
    except Exception as e:
        logger.error(f"Erro ao carregar dados da tabela '{table_name}': {e}", exc_info=True)
        return

    if df_evolution is None or df_evolution.empty:
        logger.warning(f"DataFrame da tabela '{table_name}' vazio ou não carregado.")
        return

    df_plot = df_evolution[df_evolution['dezena'].isin(dezenas_to_plot)]

    if df_plot.empty:
        logger.warning(f"Nenhum dado para as dezenas {dezenas_to_plot} na tabela '{table_name}'.")
        return

    ensure_output_dir(output_dir) # Chamada de ensure_output_dir
    plt.figure(figsize=(15, 8))
    
    # Definir um conjunto de cores para as dezenas para melhor distinção se houver muitas
    # default_colors = plt.cm.get_cmap('tab10', len(dezenas_to_plot)) # 'tab10' tem 10 cores distintas
    # Ou usar seaborn's default palette que é geralmente boa
    
    for i, dezena_val in enumerate(sorted(list(set(dezenas_to_plot)))): # sorted para ordem consistente na legenda
        df_dezena = df_plot[df_plot['dezena'] == dezena_val].sort_values(by='chunk_seq_id')
        if not df_dezena.empty:
            # color = default_colors(i % default_colors.N) # Ciclar cores se mais de 10 dezenas
            plt.plot(
                df_dezena['chunk_seq_id'], 
                df_dezena[metric_column], 
                marker='o', 
                linestyle='-', 
                label=f'Dezena {dezena_val}'
                # color=color # Opcional: para controle manual de cor
            )
        else:
            logger.debug(f"Sem dados para a dezena {dezena_val} para plotagem.")

    plt.title(f"Evolução: {metric_to_plot}\nChunks: {chunk_type.capitalize()} (Tamanho {chunk_size})", fontsize=16)
    plt.xlabel(f"ID Sequencial do Bloco ({chunk_type.capitalize()} de {chunk_size} concursos)", fontsize=12)
    plt.ylabel(metric_to_plot, fontsize=12)
    
    ax = plt.gca()
    ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True, nbins='auto')) # nbins='auto' para melhor espaçamento
    ax.tick_params(axis='both', which='major', labelsize=10) # Tamanho dos ticks

    plt.legend(title="Dezenas", loc="best", fontsize=10)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.tight_layout()
    
    dezenas_str = "_".join(map(str, sorted(list(set(dezenas_to_plot)))))
    plot_filename = f"evol_{table_name_core}_{chunk_type}_{chunk_size}_dezenas_{dezenas_str}.png"
    full_plot_path = os.path.join(output_dir, plot_filename)

    try:
        plt.savefig(full_plot_path, dpi=120) # Aumentar DPI para melhor qualidade
        logger.info(f"Gráfico salvo em: {full_plot_path}")
    except Exception as e:
        logger.error(f"Erro ao salvar o gráfico em '{full_plot_path}': {e}", exc_info=True)
    finally:
        plt.close()

--------------------------------------------------------------------------------
# Arquivo: test_chunk.py
--------------------------------------------------------------------------------
# Exemplo de script de teste (ex: test_chunk.py na raiz)
import logging
from pathlib import Path
import sys
import pandas as pd
from src.analysis.chunk_analysis import get_chunk_final_stats
from src.config import logger, DATABASE_PATH

def validate_database():
    """Valida se o banco de dados existe antes de executar os testes."""
    if not Path(DATABASE_PATH).exists():
        logger.error(f"Banco de dados não encontrado em: {DATABASE_PATH}")
        return False
    return True

def run_chunk_test():
    """Executa o teste de análise de chunks com validações."""
    logger.setLevel(logging.DEBUG)
    
    if not validate_database():
        return False
    
    try:
        print("\nTestando get_chunk_final_stats para intervalo 10...")
        chunk_stats_10 = get_chunk_final_stats(interval_size=10)
        
        if chunk_stats_10 is None:
            logger.error("get_chunk_final_stats retornou None")
            return False
            
        if chunk_stats_10.empty:
            logger.error("Nenhum dado de chunk encontrado")
            return False
            
        print(f"\nEncontrados {len(chunk_stats_10)} chunks completos de 10.")
        print("\nÚltimos 5 chunks:")
        cols_to_show = ['d1_freq', 'd1_rank', 'd10_freq', 'd10_rank', 'd25_freq', 'd25_rank']
        print(chunk_stats_10[cols_to_show].tail())
        
        return True
        
    except Exception as e:
        logger.error(f"Erro durante execução do teste: {str(e)}")
        return False

if __name__ == "__main__":
    success = run_chunk_test()
    sys.exit(0 if success else 1)

--------------------------------------------------------------------------------
# Arquivo: tests/__init__.py
--------------------------------------------------------------------------------


--------------------------------------------------------------------------------
# Arquivo: tests/conftest.py
--------------------------------------------------------------------------------
# tests/conftest.py

import pytest
import sqlite3
import pandas as pd
from pathlib import Path
from typing import List, Optional, Set

# Importa constantes do config (com try/except para garantir fallback)
try:
    from src.config import (
        ALL_NUMBERS, NEW_BALL_COLUMNS, TABLE_NAME as SORTEIOS_TABLE_NAME,
        CYCLES_TABLE_NAME, # <<< Importa nome da tabela ciclos
        CHUNK_STATS_FINAL_PREFIX
    )
except ImportError:
    print("WARN: conftest.py usando Fallbacks para constantes do config.")
    ALL_NUMBERS = list(range(1, 26))
    NEW_BALL_COLUMNS = [f'b{i}' for i in range(1, 16)]
    SORTEIOS_TABLE_NAME = 'sorteios'
    CYCLES_TABLE_NAME = 'ciclos' # <<< Fallback no nível do módulo
    CHUNK_STATS_FINAL_PREFIX = 'chunk_stats_'

# Importa função de nome de tabela
from src.database_manager import get_chunk_final_stats_table_name

# --- Dados de Teste (20 concursos) ---
TEST_DATA_SORTEIOS = [ # Omitido por brevidade - MANTENHA OS 20 SORTEIOS AQUI
    {'concurso': 1,  'data_sorteio': '2023-01-01', 'b1': 1,  'b2': 2,  'b3': 3,  'b4': 4,  'b5': 5,  'b6': 6,  'b7': 7,  'b8': 8,  'b9': 9,  'b10': 10, 'b11': 11, 'b12': 12, 'b13': 13, 'b14': 14, 'b15': 15},
    {'concurso': 2,  'data_sorteio': '2023-01-02', 'b1': 11, 'b2': 12, 'b3': 13, 'b4': 14, 'b5': 15, 'b6': 16, 'b7': 17, 'b8': 18, 'b9': 19, 'b10': 20, 'b11': 21, 'b12': 22, 'b13': 23, 'b14': 24, 'b15': 25},
    {'concurso': 3,  'data_sorteio': '2023-01-03', 'b1': 1,  'b2': 3,  'b3': 5,  'b4': 7,  'b5': 9,  'b6': 11, 'b7': 13, 'b8': 15, 'b9': 17, 'b10': 19, 'b11': 21, 'b12': 23, 'b13': 25, 'b14': 2,  'b15': 4},
    {'concurso': 4,  'data_sorteio': '2023-01-04', 'b1': 2,  'b2': 4,  'b3': 6,  'b4': 8,  'b5': 10, 'b6': 12, 'b7': 14, 'b8': 16, 'b9': 18, 'b10': 20, 'b11': 22, 'b12': 24, 'b13': 1,  'b14': 3,  'b15': 5},
    {'concurso': 5,  'data_sorteio': '2023-01-05', 'b1': 21, 'b2': 22, 'b3': 23, 'b4': 24, 'b5': 25, 'b6': 1,  'b7': 2,  'b8': 3,  'b9': 4,  'b10': 5,  'b11': 6,  'b12': 7,  'b13': 8,  'b14': 9,  'b15': 10},
    {'concurso': 6,  'data_sorteio': '2023-01-06', 'b1': 1,  'b2': 2,  'b3': 3,  'b4': 4,  'b5': 5,  'b6': 16, 'b7': 17, 'b8': 18, 'b9': 19, 'b10': 20, 'b11': 11, 'b12': 12, 'b13': 13, 'b14': 14, 'b15': 15},
    {'concurso': 7,  'data_sorteio': '2023-01-07', 'b1': 1,  'b2': 2,  'b3': 3,  'b4': 4,  'b5': 5,  'b6': 16, 'b7': 17, 'b8': 18, 'b9': 19, 'b10': 20, 'b11': 21, 'b12': 22, 'b13': 23, 'b14': 24, 'b15': 25},
    {'concurso': 8,  'data_sorteio': '2023-01-08', 'b1': 6,  'b2': 7,  'b3': 8,  'b4': 9,  'b5': 10, 'b6': 11, 'b7': 12, 'b8': 13, 'b9': 14, 'b10': 15, 'b11': 1,  'b12': 3,  'b13': 5,  'b14': 7,  'b15': 9},
    {'concurso': 9,  'data_sorteio': '2023-01-09', 'b1': 2,  'b2': 4,  'b3': 6,  'b4': 8,  'b5': 10, 'b6': 12, 'b7': 14, 'b8': 16, 'b9': 18, 'b10': 20, 'b11': 21, 'b12': 23, 'b13': 25, 'b14': 1,  'b15': 5},
    {'concurso': 10, 'data_sorteio': '2023-01-10', 'b1': 1,  'b2': 2,  'b3': 3,  'b4': 4,  'b5': 5,  'b6': 6,  'b7': 7,  'b8': 8,  'b9': 9,  'b10': 10, 'b11': 11, 'b12': 12, 'b13': 13, 'b14': 14, 'b15': 15},
    {'concurso': 11, 'data_sorteio': '2023-01-11', 'b1': 1,  'b2': 2,  'b3': 3,  'b4': 4,  'b5': 5,  'b6': 6,  'b7': 7,  'b8': 8,  'b9': 9,  'b10': 10, 'b11': 11, 'b12': 12, 'b13': 13, 'b14': 14, 'b15': 15},
    {'concurso': 12, 'data_sorteio': '2023-01-12', 'b1': 11, 'b2': 12, 'b3': 13, 'b4': 14, 'b5': 15, 'b6': 16, 'b7': 17, 'b8': 18, 'b9': 19, 'b10': 20, 'b11': 21, 'b12': 22, 'b13': 23, 'b14': 24, 'b15': 25},
    {'concurso': 13, 'data_sorteio': '2023-01-13', 'b1': 1,  'b2': 3,  'b3': 5,  'b4': 7,  'b5': 9,  'b6': 11, 'b7': 13, 'b8': 15, 'b9': 17, 'b10': 19, 'b11': 21, 'b12': 23, 'b13': 25, 'b14': 2,  'b15': 4},
    {'concurso': 14, 'data_sorteio': '2023-01-14', 'b1': 2,  'b2': 4,  'b3': 6,  'b4': 8,  'b5': 10, 'b6': 12, 'b7': 14, 'b8': 16, 'b9': 18, 'b10': 20, 'b11': 22, 'b12': 24, 'b13': 1,  'b14': 3,  'b15': 5},
    {'concurso': 15, 'data_sorteio': '2023-01-15', 'b1': 1,  'b2': 2,  'b3': 3,  'b4': 4,  'b5': 5,  'b6': 6,  'b7': 7,  'b8': 8,  'b9': 9,  'b10': 10, 'b11': 11, 'b12': 12, 'b13': 13, 'b14': 14, 'b15': 15},
    {'concurso': 16, 'data_sorteio': '2023-01-16', 'b1': 11, 'b2': 12, 'b3': 13, 'b4': 14, 'b5': 15, 'b6': 16, 'b7': 17, 'b8': 18, 'b9': 19, 'b10': 20, 'b11': 21, 'b12': 22, 'b13': 23, 'b14': 24, 'b15': 25},
    {'concurso': 17, 'data_sorteio': '2023-01-17', 'b1': 1,  'b2': 3,  'b3': 5,  'b4': 7,  'b5': 9,  'b6': 11, 'b7': 13, 'b8': 15, 'b9': 17, 'b10': 19, 'b11': 21, 'b12': 23, 'b13': 25, 'b14': 2,  'b15': 4},
    {'concurso': 18, 'data_sorteio': '2023-01-18', 'b1': 2,  'b2': 4,  'b3': 6,  'b4': 8,  'b5': 10, 'b6': 12, 'b7': 14, 'b8': 16, 'b9': 18, 'b10': 20, 'b11': 22, 'b12': 24, 'b13': 1,  'b14': 3,  'b15': 5},
    {'concurso': 19, 'data_sorteio': '2023-01-19', 'b1': 1,  'b2': 2,  'b3': 3,  'b4': 4,  'b5': 5,  'b6': 11, 'b7': 12, 'b8': 13, 'b9': 14, 'b10': 15, 'b11': 21, 'b12': 22, 'b13': 23, 'b14': 24, 'b15': 25},
    {'concurso': 20, 'data_sorteio': '2023-01-20', 'b1': 6,  'b2': 7,  'b3': 8,  'b4': 9,  'b5': 10, 'b6': 16, 'b7': 17, 'b8': 18, 'b9': 19, 'b10': 20, 'b11': 1,  'b12': 2,  'b13': 3,  'b14': 4,  'b15': 5},
]
NUM_TEST_RECORDS = len(TEST_DATA_SORTEIOS)

# Dados PRÉ-CALCULADOS para chunk_stats_10_final
EXPECTED_CHUNK_10_STATS_CONC_10 = { # (Valores corretos)
    'concurso_fim': 10, 'd1_freq': 9, 'd2_freq': 8, 'd3_freq': 8, 'd4_freq': 8, 'd5_freq': 9, 'd6_freq': 8, 'd7_freq': 7, 'd8_freq': 8, 'd9_freq': 7, 'd10_freq': 8, 'd11_freq': 6, 'd12_freq': 7, 'd13_freq': 6, 'd14_freq': 7, 'd15_freq': 6, 'd16_freq': 5, 'd17_freq': 5, 'd18_freq': 5, 'd19_freq': 5, 'd20_freq': 5, 'd21_freq': 5, 'd22_freq': 4, 'd23_freq': 5, 'd24_freq': 4, 'd25_freq': 5,
    'd1_rank': 1, 'd2_rank': 3, 'd3_rank': 3, 'd4_rank': 3, 'd5_rank': 1, 'd6_rank': 3, 'd7_rank': 9, 'd8_rank': 3, 'd9_rank': 9, 'd10_rank': 3, 'd11_rank': 13, 'd12_rank': 9, 'd13_rank': 13, 'd14_rank': 9, 'd15_rank': 13, 'd16_rank': 16, 'd17_rank': 16, 'd18_rank': 16, 'd19_rank': 16, 'd20_rank': 16, 'd21_rank': 16, 'd22_rank': 24, 'd23_rank': 16, 'd24_rank': 24, 'd25_rank': 16,
}
EXPECTED_CHUNK_10_STATS_CONC_20 = { # (Valores corretos)
    'concurso_fim': 20, 'd1_freq': 6, 'd2_freq': 7, 'd3_freq': 7, 'd4_freq': 7, 'd5_freq': 7, 'd6_freq': 5, 'd7_freq': 5, 'd8_freq': 5, 'd9_freq': 5, 'd10_freq': 5, 'd11_freq': 6, 'd12_freq': 7, 'd13_freq': 7, 'd14_freq': 7, 'd15_freq': 7, 'd16_freq': 5, 'd17_freq': 5, 'd18_freq': 5, 'd19_freq': 5, 'd20_freq': 5, 'd21_freq': 5, 'd22_freq': 5, 'd23_freq': 5, 'd24_freq': 5, 'd25_freq': 6,
    'd1_rank': 9, 'd2_rank': 1, 'd3_rank': 1, 'd4_rank': 1, 'd5_rank': 1, 'd6_rank': 12, 'd7_rank': 12, 'd8_rank': 12, 'd9_rank': 12, 'd10_rank': 12, 'd11_rank': 9, 'd12_rank': 1, 'd13_rank': 1, 'd14_rank': 1, 'd15_rank': 1, 'd16_rank': 12, 'd17_rank': 12, 'd18_rank': 12, 'd19_rank': 12, 'd20_rank': 12, 'd21_rank': 12, 'd22_rank': 12, 'd23_rank': 12, 'd24_rank': 12, 'd25_rank': 9,
}

# --- Fixtures ---
@pytest.fixture(scope="function")
def test_db_conn() -> sqlite3.Connection:
    conn = sqlite3.connect(":memory:"); yield conn; conn.close()

@pytest.fixture(scope="function")
def populated_db_conn(test_db_conn: sqlite3.Connection) -> sqlite3.Connection:
    """ Popula BD em memória com sorteios, chunk_stats_10 e ciclos. """
    conn = test_db_conn; cursor = conn.cursor()
    # Cria e popula 'sorteios'
    ball_cols_schema = ', '.join([f'"{col}" INTEGER' for col in NEW_BALL_COLUMNS]); create_sorteios_sql = f"CREATE TABLE IF NOT EXISTS {SORTEIOS_TABLE_NAME} (concurso INTEGER PRIMARY KEY, data_sorteio TEXT, {ball_cols_schema});"; cursor.execute(create_sorteios_sql)
    cols_ordered_sorteios = ['concurso', 'data_sorteio'] + NEW_BALL_COLUMNS; placeholders_sorteios = ', '.join(['?'] * len(cols_ordered_sorteios)); sql_insert_sorteios = f"INSERT INTO {SORTEIOS_TABLE_NAME} ({', '.join(cols_ordered_sorteios)}) VALUES ({placeholders_sorteios})"
    for record in TEST_DATA_SORTEIOS: cursor.execute(sql_insert_sorteios, [record.get(col) for col in cols_ordered_sorteios])
    conn.commit()

    # Cria e popula 'chunk_stats_10_final'
    interval_10 = 10; table_name_chunk_10 = get_chunk_final_stats_table_name(interval_10)
    freq_col_defs = ', '.join([f'd{i}_freq INTEGER NOT NULL' for i in ALL_NUMBERS]); rank_col_defs = ', '.join([f'd{i}_rank INTEGER NOT NULL' for i in ALL_NUMBERS])
    create_chunk_sql = f"CREATE TABLE IF NOT EXISTS {table_name_chunk_10} (concurso_fim INTEGER PRIMARY KEY, {freq_col_defs}, {rank_col_defs});"; cursor.execute(create_chunk_sql); conn.commit()
    chunk_stat_cols = ['concurso_fim'] + [f'd{i}_freq' for i in ALL_NUMBERS] + [f'd{i}_rank' for i in ALL_NUMBERS]; chunk_placeholders = ', '.join(['?'] * len(chunk_stat_cols)); sql_insert_chunk = f"INSERT OR REPLACE INTO {table_name_chunk_10} ({', '.join(chunk_stat_cols)}) VALUES ({chunk_placeholders})"
    values_conc_10 = [EXPECTED_CHUNK_10_STATS_CONC_10.get(col, 0) for col in chunk_stat_cols]; cursor.execute(sql_insert_chunk, values_conc_10)
    values_conc_20 = [EXPECTED_CHUNK_10_STATS_CONC_20.get(col, 0) for col in chunk_stat_cols]; cursor.execute(sql_insert_chunk, values_conc_20)
    conn.commit()

    # *** CORREÇÃO APLICADA: Cria a tabela 'ciclos' usando SQL direto ***
    create_cycles_sql = f"""CREATE TABLE IF NOT EXISTS {CYCLES_TABLE_NAME} (numero_ciclo INTEGER PRIMARY KEY, concurso_inicio INTEGER NOT NULL, concurso_fim INTEGER NOT NULL UNIQUE, duracao INTEGER NOT NULL);"""
    cursor.execute(create_cycles_sql)
    test_cycles = [
        {'numero_ciclo': 1, 'concurso_inicio': 1, 'concurso_fim': 5, 'duracao': 5},
        {'numero_ciclo': 2, 'concurso_inicio': 6, 'concurso_fim': 10, 'duracao': 5},
        {'numero_ciclo': 3, 'concurso_inicio': 11, 'concurso_fim': 16, 'duracao': 6},
    ]
    cols_cycles = ['numero_ciclo', 'concurso_inicio', 'concurso_fim', 'duracao']; placeholders_cycles = ', '.join(['?'] * len(cols_cycles)); sql_insert_cycles = f"INSERT INTO {CYCLES_TABLE_NAME} ({', '.join(cols_cycles)}) VALUES ({placeholders_cycles})"
    for record in test_cycles: cursor.execute(sql_insert_cycles, [record.get(col) for col in cols_cycles])
    conn.commit()
    # *** FIM DA CORREÇÃO ***

    return conn

# Fixture separada (mantida)
@pytest.fixture(scope="function")
def populated_db_sem_ciclos(test_db_conn: sqlite3.Connection) -> sqlite3.Connection:
     conn = test_db_conn; cursor = conn.cursor()
     ball_cols_schema = ', '.join([f'"{col}" INTEGER' for col in NEW_BALL_COLUMNS]); create_sorteios_sql = f"CREATE TABLE IF NOT EXISTS {SORTEIOS_TABLE_NAME} (concurso INTEGER PRIMARY KEY, data_sorteio TEXT, {ball_cols_schema});"; cursor.execute(create_sorteios_sql)
     cols_ordered_sorteios = ['concurso', 'data_sorteio'] + NEW_BALL_COLUMNS; placeholders_sorteios = ', '.join(['?'] * len(cols_ordered_sorteios)); sql_insert_sorteios = f"INSERT INTO {SORTEIOS_TABLE_NAME} ({', '.join(cols_ordered_sorteios)}) VALUES ({placeholders_sorteios})"
     for record in TEST_DATA_SORTEIOS: cursor.execute(sql_insert_sorteios, [record.get(col) for col in cols_ordered_sorteios])
     conn.commit()
     return conn

--------------------------------------------------------------------------------
# Arquivo: tests/test_analysis_frequency.py
--------------------------------------------------------------------------------
# tests/test_analysis_frequency.py

import pytest
import pandas as pd
from pathlib import Path
import sqlite3

# Importa função a testar
from src.analysis.frequency_analysis import calculate_frequency, get_cumulative_frequency
# Importa DB manager para poder *chamar* a função passando a conexão mockada
# Ou mockamos read_data_from_db para retornar um DF fixo? Mais fácil.
from unittest.mock import patch

# Usa a fixture 'populated_db_conn' definida em conftest.py
# Mas vamos mockar a leitura do BD para isolar o teste da lógica de frequência

@pytest.fixture
def sample_dataframe():
    """ Retorna um DataFrame simples para testes de frequência. """
    # Dados similares aos da fixture de BD, mas direto aqui
    test_data = [
        {'concurso': 1, 'b1': 1, 'b2': 2, 'b3': 3, 'b4': 4, 'b5': 5, 'b6': 6, 'b7': 7, 'b8': 8, 'b9': 9, 'b10': 10, 'b11': 11, 'b12': 12, 'b13': 13, 'b14': 14, 'b15': 15},
        {'concurso': 2, 'b1': 11,'b2': 12,'b3': 13,'b4': 14,'b5': 15,'b6': 16,'b7': 17,'b8': 18,'b9': 19,'b10': 20,'b11': 21,'b12': 22,'b13': 23,'b14': 24,'b15': 25},
        {'concurso': 3, 'b1': 1, 'b2': 3, 'b3': 5, 'b4': 7, 'b5': 9, 'b6': 11,'b7': 13,'b8': 15,'b9': 17,'b10': 19,'b11': 21,'b12': 23,'b13': 25,'b14': 2, 'b15': 4},
    ]
    df = pd.DataFrame(test_data)
    # Garante tipos corretos como no data_loader
    for col in df.columns:
        if col != 'data_sorteio': # Ignora data se houver
             df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')
    return df

# Mocka a função que lê do banco de dados para estes testes
@patch('src.analysis.frequency_analysis.read_data_from_db')
def test_calculate_frequency_all(mock_read_db, sample_dataframe):
    """ Testa o cálculo de frequência para todos os dados mockados. """
    mock_read_db.return_value = sample_dataframe # Configura o mock

    freq = calculate_frequency(concurso_minimo=1, concurso_maximo=3)

    mock_read_db.assert_called_once() # Verifica se read_data_from_db foi chamado
    assert freq is not None
    assert freq[1] == 2 # Número 1 aparece nos concursos 1 e 3
    assert freq[2] == 2 # Número 2 aparece nos concursos 1 e 3 (bola 14)
    assert freq[16] == 1 # Número 16 aparece só no concurso 2
    assert freq[6] == 1  # Número 6 aparece só no concurso 1
    assert freq.sum() == 15 * 3 # Total de bolas sorteadas = 15 * num_concursos

@patch('src.analysis.frequency_analysis.read_data_from_db')
def test_calculate_frequency_subset(mock_read_db, sample_dataframe):
    """ Testa o cálculo de frequência para um subconjunto de concursos. """
    # Filtra o DF para simular a leitura apenas do concurso 2
    mock_read_db.return_value = sample_dataframe[sample_dataframe['concurso'] == 2]

    freq = calculate_frequency(concurso_minimo=2, concurso_maximo=2)

    mock_read_db.assert_called_once()
    assert freq is not None
    assert freq[1] == 0 # Não aparece no concurso 2
    assert freq[11] == 1
    assert freq[25] == 1
    assert freq.sum() == 15 # Apenas 15 bolas no concurso 2

@patch('src.analysis.frequency_analysis.read_data_from_db')
@patch('src.analysis.frequency_analysis.get_closest_freq_snapshot')
def test_get_cumulative_frequency_no_snapshot(mock_get_snapshot, mock_read_db, sample_dataframe):
    """ Testa get_cumulative_frequency quando não há snapshot. """
    mock_get_snapshot.return_value = None # Simula nenhum snapshot encontrado
    mock_read_db.return_value = sample_dataframe # Simula leitura de 1 a 3

    freq = get_cumulative_frequency(concurso_maximo=3)

    mock_get_snapshot.assert_called_once_with(3)
    # Espera chamar calculate_frequency (via read_data_from_db) para 1-3
    mock_read_db.assert_called_once()
    assert freq is not None
    assert freq[1] == 2 # Igual ao teste geral
    assert freq[16] == 1

@patch('src.analysis.frequency_analysis.read_data_from_db')
@patch('src.analysis.frequency_analysis.get_closest_freq_snapshot')
def test_get_cumulative_frequency_with_snapshot(mock_get_snapshot, mock_read_db, sample_dataframe):
    """ Testa get_cumulative_frequency usando um snapshot mockado. """
    # Snapshot no concurso 1
    snap_concurso = 1
    snap_data = {num: 1 for num in range(1, 16)} # Frequência 1 para 1-15
    snap_series = pd.Series(snap_data).reindex(range(1, 26), fill_value=0)
    mock_get_snapshot.return_value = (snap_concurso, snap_series)

    # Mocka a leitura do delta (concursos 2 e 3)
    delta_df = sample_dataframe[sample_dataframe['concurso'] > snap_concurso]
    mock_read_db.return_value = delta_df

    freq = get_cumulative_frequency(concurso_maximo=3)

    mock_get_snapshot.assert_called_once_with(3)
    # Verifica se chamou calculate_frequency (via read_db) para o delta correto (2 a 3)
    # (A forma como mockamos read_db aqui não permite verificar os args min/max facilmente)
    # Mas podemos verificar se foi chamado.
    mock_read_db.assert_called_once()
    assert freq is not None
    # Frequência final deve ser a soma do snapshot + delta
    # No snapshot: freq[1]=1, freq[11]=1, freq[16]=0
    # No delta (conc 2 e 3): freq[1]=1, freq[11]=1+1=2, freq[16]=1
    # Total esperado: freq[1]=1+1=2, freq[11]=1+2=3, freq[16]=0+1=1
    assert freq[1] == 2
    assert freq[11] == 3 # 1 do snap + 1 do conc 2 + 1 do conc 3
    assert freq[16] == 1 # 0 do snap + 1 do conc 2 + 0 do conc 3

--------------------------------------------------------------------------------
# Arquivo: tests/test_chunk_analysis.py
--------------------------------------------------------------------------------
# tests/test_chunk_analysis.py

import pytest
import pandas as pd
import sqlite3
from typing import Optional
import numpy as np

# Importa as funções a serem testadas
from src.analysis.chunk_analysis import get_chunk_final_stats, calculate_historical_rank_stats
# Importa DB manager para mock
from src.database_manager import DATABASE_PATH
from unittest.mock import patch

# Importa dados esperados do conftest
from .conftest import (
    EXPECTED_CHUNK_10_STATS_CONC_10, EXPECTED_CHUNK_10_STATS_CONC_20,
    ALL_NUMBERS, NUM_TEST_RECORDS # Usa NUM_TEST_RECORDS
)

# Garante fallback
if 'ALL_NUMBERS' not in globals(): ALL_NUMBERS_TEST = list(range(1, 26))
else: ALL_NUMBERS_TEST = ALL_NUMBERS


# --- Testes para get_chunk_final_stats ---
@patch('src.analysis.chunk_analysis.DATABASE_PATH', ':memory:')
@patch('src.analysis.chunk_analysis.sqlite3.connect')
def test_get_chunk_final_stats_with_data(mock_connect, populated_db_conn: sqlite3.Connection):
    """ Testa se busca e retorna corretamente os dados dos chunks 10 e 20. """
    mock_connect.return_value = populated_db_conn
    interval = 10
    result_df = get_chunk_final_stats(interval_size=interval, concurso_maximo=None) # Pede todos

    assert result_df is not None; assert isinstance(result_df, pd.DataFrame); assert not result_df.empty
    assert result_df.index.tolist() == [10, 20]; assert len(result_df) == 2

    # Verifica concurso 10 e 20 (como antes)
    row_10 = result_df.loc[10]; row_20 = result_df.loc[20]
    assert row_10['d1_rank'] == EXPECTED_CHUNK_10_STATS_CONC_10['d1_rank']
    assert row_20['d1_rank'] == EXPECTED_CHUNK_10_STATS_CONC_20['d1_rank']
    assert row_10['d25_rank'] == EXPECTED_CHUNK_10_STATS_CONC_10['d25_rank']
    assert row_20['d25_rank'] == EXPECTED_CHUNK_10_STATS_CONC_20['d25_rank']


@patch('src.analysis.chunk_analysis.DATABASE_PATH', ':memory:')
@patch('src.analysis.chunk_analysis.sqlite3.connect')
def test_get_chunk_final_stats_limit(mock_connect, populated_db_conn: sqlite3.Connection):
    """ Testa se o filtro concurso_maximo funciona. """
    mock_connect.return_value = populated_db_conn
    # Pede stats até o concurso 15 (só deve retornar o chunk 10)
    result_df = get_chunk_final_stats(interval_size=10, concurso_maximo=15)
    assert result_df is not None; assert isinstance(result_df, pd.DataFrame)
    assert not result_df.empty; assert result_df.index.tolist() == [10]


@patch('src.analysis.chunk_analysis.DATABASE_PATH', ':memory:')
@patch('src.analysis.chunk_analysis.sqlite3.connect')
def test_get_chunk_final_stats_missing_table(mock_connect, test_db_conn: sqlite3.Connection):
    """ Testa o comportamento se a tabela chunk não existir (deve retornar None). """
    mock_connect.return_value = test_db_conn
    result_df = get_chunk_final_stats(interval_size=999, concurso_maximo=10)
    assert result_df is None


# --- TESTE IMPLEMENTADO para calculate_historical_rank_stats ---
@patch('src.analysis.chunk_analysis.DATABASE_PATH', ':memory:')
@patch('src.analysis.chunk_analysis.sqlite3.connect')
def test_calculate_historical_rank_stats(mock_connect, populated_db_conn: sqlite3.Connection):
    """ Testa cálculo de média e std dev dos ranks históricos dos chunks 10 e 20. """
    mock_connect.return_value = populated_db_conn
    interval = 10

    # Calcula os stats históricos usando os dados da fixture (chunks 10 e 20)
    hist_stats_df = calculate_historical_rank_stats(interval_size=interval, concurso_maximo=None) # Pega tudo da fixture

    assert hist_stats_df is not None; assert isinstance(hist_stats_df, pd.DataFrame)
    assert not hist_stats_df.empty; assert len(hist_stats_df) == 25
    assert hist_stats_df.index.tolist() == ALL_NUMBERS_TEST
    col_avg = f'avg_rank_chunk{interval}'; col_std = f'std_rank_chunk{interval}'
    assert col_avg in hist_stats_df.columns; assert col_std in hist_stats_df.columns

    # Verifica valores calculados manualmente (baseados nos ranks dos chunks 10 e 20)
    # Dezena 1: Ranks [1, 9]. Avg=5.0. Std=sqrt(32)=5.657
    assert hist_stats_df.loc[1, col_avg] == pytest.approx(5.0)
    assert hist_stats_df.loc[1, col_std] == pytest.approx(np.sqrt(32))

    # Dezena 2: Ranks [3, 1]. Avg=2.0. Std=sqrt(2)=1.414
    assert hist_stats_df.loc[2, col_avg] == pytest.approx(2.0)
    assert hist_stats_df.loc[2, col_std] == pytest.approx(np.sqrt(2))

    # Dezena 25: Ranks [16, 9]. Avg=12.5. Std=sqrt(24.5)=4.950
    assert hist_stats_df.loc[25, col_avg] == pytest.approx(12.5)
    assert hist_stats_df.loc[25, col_std] == pytest.approx(np.sqrt(24.5))

    # Verifica se não há NaNs (pois temos 2 pontos para todos)
    assert not hist_stats_df.isnull().any().any()

@patch('src.analysis.chunk_analysis.get_chunk_final_stats') # Mocka a função que lê dados
def test_calculate_historical_rank_stats_insufficient_data(mock_get_stats):
    """ Testa o caso com menos de 2 chunks (std dev deve ser NaN). """
    # Mock get_chunk_final_stats para retornar apenas 1 linha
    rank_cols = [f'd{i}_rank' for i in ALL_NUMBERS_TEST]
    mock_data = {col: [EXPECTED_CHUNK_10_STATS_CONC_10[col]] for col in rank_cols} # Pega ranks do C10
    mock_df = pd.DataFrame(mock_data, index=[10])
    mock_df.index.name = 'concurso_fim'
    mock_get_stats.return_value = mock_df

    hist_stats_df = calculate_historical_rank_stats(interval_size=10)

    assert hist_stats_df is not None; assert len(hist_stats_df) == 25
    col_avg = 'avg_rank_chunk10'; col_std = 'std_rank_chunk10'
    # Média deve ser igual ao rank do único chunk
    assert hist_stats_df.loc[1, col_avg] == EXPECTED_CHUNK_10_STATS_CONC_10['d1_rank']
    # Std dev deve ser NaN
    assert pd.isna(hist_stats_df.loc[1, col_std])
    assert hist_stats_df[col_std].isnull().all() # Todos os std dev devem ser NaN

--------------------------------------------------------------------------------
# Arquivo: tests/test_cycle_analysis.py
--------------------------------------------------------------------------------
# src/analysis/cycle_analysis.py

import sqlite3
import pandas as pd
import numpy as np
from typing import Optional, List
import logging # Certifique-se que logging está importado

# Importa constantes e logger
try:
    from src.config import DB_PATH, DRAW_TABLE_NAME, CYCLES_TABLE_NAME, ALL_NUMBERS, LOGGER_NAME
    from src.database_manager import read_data_from_db # Importa read_data_from_db
except ImportError:
    # Fallbacks para ambiente de teste ou execução isolada
    DB_PATH = 'lotofacil.db'
    DRAW_TABLE_NAME = 'sorteios'
    CYCLES_TABLE_NAME = 'ciclos'
    ALL_NUMBERS = list(range(1, 26))
    LOGGER_NAME = __name__ # Ou um nome padrão
    def read_data_from_db(*args, **kwargs): return None # Mock simples se não encontrado

logger = logging.getLogger(LOGGER_NAME)


def get_cycles_df(concurso_maximo: Optional[int] = None) -> Optional[pd.DataFrame]:
    """ Busca dados dos ciclos do banco de dados. """
    try:
        conn = sqlite3.connect(DB_PATH)
        query = f"SELECT * FROM {CYCLES_TABLE_NAME}"
        if concurso_maximo is not None:
            query += f" WHERE concurso_fim <= {concurso_maximo}"
        query += " ORDER BY numero_ciclo"
        df = pd.read_sql_query(query, conn)
        conn.close()
        logger.info(f"{len(df)} registros de ciclos lidos" + (f" até concurso {concurso_maximo}." if concurso_maximo else "."))
        return df
    except Exception as e:
        logger.error(f"Erro ao ler tabela de ciclos: {e}")
        return None


def _calculate_intra_cycle_delays(cycle_draws_df: pd.DataFrame) -> pd.DataFrame:
    """
    Calculates the intra-cycle delay for each number within a single cycle.
    Delay = contests since last seen *within this cycle*. 0 if present.
    Increments from 1 at the start if not present in the first draw.
    """
    if cycle_draws_df is None or cycle_draws_df.empty:
        return pd.DataFrame()

    all_numbers = set(range(1, 26))
    # Ensure ball columns exist
    ball_cols = [col for col in cycle_draws_df.columns if col.startswith('bola')]
    if not ball_cols:
         logger.warning("Nenhuma coluna 'bolaX' encontrada nos dados do ciclo.")
         return pd.DataFrame()

    melted_draws = cycle_draws_df.melt(
        id_vars=['concurso', 'data_sorteio'],
        value_vars=ball_cols, # Use dynamic ball columns
        value_name='numero'
    ).dropna()
    # Convert 'numero' to int after melt, handling potential errors
    melted_draws['numero'] = pd.to_numeric(melted_draws['numero'], errors='coerce').astype('Int64') # Use nullable Int64
    melted_draws.dropna(subset=['numero'], inplace=True)


    # Pivot to get presence matrix (concurso x numero)
    presence_matrix = pd.pivot_table(
        melted_draws, index='concurso', columns='numero', aggfunc='size', fill_value=0
    ).applymap(lambda x: 1 if x > 0 else 0)

    # Reindex to include all contests and numbers, fill missing numbers with 0 (not present)
    all_contests_in_df = cycle_draws_df['concurso'].unique()
    all_contests_in_df.sort() # Ensure contests are sorted
    presence_matrix = presence_matrix.reindex(index=all_contests_in_df, columns=list(all_numbers), fill_value=0)
    # presence_matrix = presence_matrix.sort_index() # Already sorted above


    delay_matrix = pd.DataFrame(0, index=presence_matrix.index, columns=presence_matrix.columns)
    last_seen_in_cycle = {} # Store contest number where number was last seen

    for contest in presence_matrix.index:
        for number in presence_matrix.columns:
            if presence_matrix.loc[contest, number] == 1:
                delay_matrix.loc[contest, number] = 0
                last_seen_in_cycle[number] = contest
            else:
                if number in last_seen_in_cycle:
                    # Calculate delay since last seen IN THIS CYCLE
                    delay_matrix.loc[contest, number] = contest - last_seen_in_cycle[number]
                else:
                    # Number not seen yet in this cycle. Delay is contests since cycle start.
                    cycle_start_contest = presence_matrix.index.min()
                    delay_matrix.loc[contest, number] = contest - cycle_start_contest + 1

    return delay_matrix


def calculate_historical_intra_cycle_delay_stats(
    cycles_df: pd.DataFrame, history_limit: int = 10
) -> Optional[pd.DataFrame]:
    """
    Calculates historical (avg, max, std) intra-cycle delay stats for each number
    based on the last 'history_limit' completed cycles.
    """
    logger.info(f"Calculando stats históricos de atraso intra-ciclo (Últimos {history_limit} ciclos)...")
    if cycles_df is None or cycles_df.empty:
        logger.warning("DataFrame de ciclos vazio fornecido.")
        return None
    if not all(col in cycles_df.columns for col in ['numero_ciclo', 'concurso_inicio', 'concurso_fim']):
        logger.error("DataFrame de ciclos não contém as colunas necessárias ('numero_ciclo', 'concurso_inicio', 'concurso_fim').")
        return None

    # Get the N most recent cycles
    recent_cycles = cycles_df.nlargest(history_limit, 'numero_ciclo')
    if recent_cycles.empty:
        logger.warning("Nenhum ciclo encontrado para calcular stats históricos de atraso intra-ciclo.")
        return None

    logger.info(f"Analisando {len(recent_cycles)} ciclos recentes para atraso intra-ciclo...")
    all_delays_by_number = {num: [] for num in range(1, 26)}

    # Iterate through the selected cycles
    for _, cycle_info in recent_cycles.iterrows():
        cycle_num = cycle_info['numero_ciclo']
        min_c = int(cycle_info['concurso_inicio'])
        max_c = int(cycle_info['concurso_fim'])

        # Read draw data for this specific cycle
        # Make sure read_data_from_db is correctly imported or available in scope
        cycle_draws_df = read_data_from_db(
            table_name=DRAW_TABLE_NAME,
            concurso_minimo=min_c,
            concurso_maximo=max_c
        )

        if cycle_draws_df is None or cycle_draws_df.empty:
            logger.warning(f"Nenhum dado de sorteio encontrado para o ciclo {cycle_num} ({min_c}-{max_c}). Pulando.")
            continue

        # Calculate delays for this single cycle
        intra_cycle_delays = _calculate_intra_cycle_delays(cycle_draws_df) # Calls the helper

        # Append delays for each number to the global list
        if not intra_cycle_delays.empty:
            for number in all_delays_by_number.keys():
                if number in intra_cycle_delays.columns:
                     # Collect ALL delay values for this number in this cycle
                    all_delays_by_number[number].extend(intra_cycle_delays[number].tolist())

    # Now calculate stats over the collected delays for all relevant cycles
    results = []
    for number, delays in all_delays_by_number.items():
        if not delays: # No data collected for this number
            results.append({
                'numero': number,
                'avg_hist_intra_delay': np.nan,
                'max_hist_intra_delay': np.nan,
                'std_hist_intra_delay': np.nan
            })
            continue

        # ======================================================
        # ADICIONE O LOG AQUI DENTRO DO LOOP, ANTES DOS CÁLCULOS:
        if number == 1:
            logger.info(f"DEBUG: Lista de atrasos calculada para Dezena 1: {delays}")
        # ======================================================

        # Calculate statistics
        avg_delay = np.mean(delays)
        max_delay = np.max(delays)
        # Check for sufficient data points for standard deviation
        std_delay = np.std(delays, ddof=1) if len(delays) > 1 else 0.0 # Use 0.0 for std dev if only 1 element

        results.append({
            'numero': number,
            'avg_hist_intra_delay': avg_delay,
            'max_hist_intra_delay': max_delay,
            'std_hist_intra_delay': std_delay
        })

    if not results:
        logger.warning("Nenhuma estatística de atraso intra-ciclo foi calculada.")
        return None

    hist_stats_df = pd.DataFrame(results)
    # Convert max delay to integer if possible, handle NaN
    hist_stats_df['max_hist_intra_delay'] = pd.to_numeric(hist_stats_df['max_hist_intra_delay'], errors='coerce').astype('Int64')
    hist_stats_df.set_index('numero', inplace=True)
    logger.info("Stats históricos de atraso intra-ciclo (limitado) concluídos.")
    return hist_stats_df

# Você pode adicionar outras funções de análise de ciclo aqui, se necessário.

--------------------------------------------------------------------------------
# Arquivo: tests/test_database_manager.py
--------------------------------------------------------------------------------
# tests/test_database_manager.py

import pytest
import pandas as pd
from pathlib import Path
import sqlite3
from typing import Optional, Set

# Importa funções a testar
from src.database_manager import read_data_from_db, get_draw_numbers, save_to_db
# Importa constantes
try:
    from src.config import NEW_BALL_COLUMNS, TABLE_NAME as SORTEIOS_TABLE_NAME
    from tests.conftest import NUM_TEST_RECORDS # <<< Importa do conftest
except ImportError:
    NEW_BALL_COLUMNS = [f'b{i}' for i in range(1, 16)]
    SORTEIOS_TABLE_NAME = 'sorteios'
    NUM_TEST_RECORDS = 10 # Fallback se import falhar

# Usa a fixture 'populated_db_conn' definida em conftest.py
def test_get_draw_numbers_found(populated_db_conn: sqlite3.Connection):
    """ Testa buscar dezenas de um concurso existente. """
    # Função auxiliar (mantida como antes)
    def _get_draw_numbers_test(concurso: int, conn: sqlite3.Connection) -> Optional[Set[int]]:
         cursor = conn.cursor(); ball_cols_str = ', '.join(f'"{col}"' for col in NEW_BALL_COLUMNS); sql = f'SELECT {ball_cols_str} FROM {SORTEIOS_TABLE_NAME} WHERE concurso = ?'
         try: cursor.execute(sql, (concurso,)); result = cursor.fetchone(); return {int(n) for n in result if n is not None and pd.notna(n)} if result else None
         except Exception as e: print(f"Erro: {e}"); return None

    numbers_conc_2 = _get_draw_numbers_test(2, populated_db_conn)
    expected_conc_2 = {11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25}
    assert numbers_conc_2 == expected_conc_2

def test_get_draw_numbers_not_found(populated_db_conn: sqlite3.Connection):
    """ Testa buscar dezenas de um concurso inexistente. """
    def _get_draw_numbers_test(concurso: int, conn: sqlite3.Connection) -> Optional[Set[int]]:
         cursor = conn.cursor(); ball_cols_str = ', '.join(f'"{col}"' for col in NEW_BALL_COLUMNS); sql = f'SELECT {ball_cols_str} FROM {SORTEIOS_TABLE_NAME} WHERE concurso = ?'
         try: cursor.execute(sql, (concurso,)); result = cursor.fetchone(); return {int(n) for n in result if n is not None and pd.notna(n)} if result else None
         except Exception as e: print(f"Erro: {e}"); return None

    numbers_conc_99 = _get_draw_numbers_test(99, populated_db_conn)
    assert numbers_conc_99 is None

def test_read_data_from_db_basic(populated_db_conn: sqlite3.Connection):
    """ Testa leitura básica de dados. """
    df = pd.read_sql(f"SELECT * FROM {SORTEIOS_TABLE_NAME} ORDER BY concurso", populated_db_conn)
    assert df is not None
    # *** CORREÇÃO AQUI: Usa a constante importada ***
    assert len(df) == NUM_TEST_RECORDS, f"Esperado {NUM_TEST_RECORDS} linhas, obteve {len(df)}"
    assert df['concurso'].iloc[0] == 1
    assert df['concurso'].iloc[-1] == NUM_TEST_RECORDS # Último concurso é o número de registros

def test_read_data_from_db_filtered(populated_db_conn: sqlite3.Connection):
    """ Testa leitura com filtros min/max. """
    df = pd.read_sql(f"SELECT * FROM {SORTEIOS_TABLE_NAME} WHERE concurso >= ? AND concurso <= ? ORDER BY concurso",
                     populated_db_conn, params=(2, 4))
    assert df is not None
    assert len(df) == 3
    assert df['concurso'].tolist() == [2, 3, 4]

--------------------------------------------------------------------------------
# Arquivo: tests/test_evaluator.py
--------------------------------------------------------------------------------
# tests/test_evaluator.py

import pytest
# Importa a função a ser testada
from src.backtester.evaluator import evaluate_hits

def test_evaluate_hits_basic():
    """ Testa a contagem básica de acertos. """
    escolhidos = {1, 2, 3, 4, 5, 11, 12, 13, 14, 15, 21, 22, 23, 24, 25}
    sorteados = {1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 2, 4} # 11 acertos
    assert evaluate_hits(escolhidos, sorteados) == 11

def test_evaluate_hits_all():
    """ Testa 15 acertos. """
    escolhidos = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}
    sorteados = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}
    assert evaluate_hits(escolhidos, sorteados) == 15

def test_evaluate_hits_none():
    """ Testa 0 acertos. """
    escolhidos = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}
    sorteados = {16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30} # Apenas exemplo
    assert evaluate_hits(escolhidos, sorteados) == 0

def test_evaluate_hits_empty():
    """ Testa com conjuntos vazios. """
    assert evaluate_hits(set(), {1, 2, 3}) == 0
    assert evaluate_hits({1, 2, 3}, set()) == 0
    assert evaluate_hits(set(), set()) == 0

def test_evaluate_hits_invalid_input():
     """ Testa com inputs None (deve retornar -1 ou levantar erro). """
     # Assumindo que retorna -1 para erro, como no código atual
     assert evaluate_hits(None, {1,2}) == -1
     assert evaluate_hits({1,2}, None) == -1
     assert evaluate_hits(None, None) == -1
     # Teste para tipo incorreto
     assert evaluate_hits([1,2], {1,2}) == -1 # Passando lista em vez de set

--------------------------------------------------------------------------------
# Arquivo: tests/test_repetition_analysis.py
--------------------------------------------------------------------------------
# tests/test_repetition_analysis.py

import pytest
import pandas as pd
import sqlite3
from typing import Optional, Set

# Importa a função a ser testada
from src.analysis.repetition_analysis import calculate_historical_repetition_rate
# Importa constantes e fixture do conftest
from .conftest import populated_db_conn, TEST_DATA_SORTEIOS, ALL_NUMBERS
# Mock para simular read_data_from_db
from unittest.mock import patch
import numpy as np

# Garante fallback
if 'ALL_NUMBERS' not in globals(): ALL_NUMBERS = list(range(1, 26))

@patch('src.analysis.repetition_analysis.read_data_from_db')
def test_calculate_historical_repetition_rate(mock_read_db):
    """ Testa o cálculo da taxa de repetição com dados mockados (10 sorteios, C8 inválido). """
    test_df = pd.DataFrame(TEST_DATA_SORTEIOS[:10])
    from src.config import NEW_BALL_COLUMNS
    if 'NEW_BALL_COLUMNS' not in globals(): NEW_BALL_COLUMNS = [f'b{i}' for i in range(1,16)]
    for col in test_df.columns:
        if col != 'data_sorteio': test_df[col] = pd.to_numeric(test_df[col], errors='coerce').astype('Int64')

    mock_read_db.return_value = test_df
    rates = calculate_historical_repetition_rate(concurso_maximo=10)

    assert rates is not None; assert isinstance(rates, pd.Series); assert len(rates) == 25
    assert rates.index.tolist() == ALL_NUMBERS; assert not rates.isnull().any()

    # --- Asserts Corrigidos com base na LÓGICA FINAL e TRACE CORRETO FINAL ---
    # Apps(N-1) VÁLIDOS / Reps(N | N-1 VÁLIDO) -> Taxa
    # 1: Apps=6, Reps=5 -> 5/6
    # 5: Apps=6, Reps=5 -> 5/6
    # 11: Apps=4, Reps=2 -> 0.5
    # 16: Apps=4, Reps=1 -> 0.25
    # 22: Apps=3, Reps=1 -> 1/3
    # 24: Apps=3, Reps=1 -> 1/3
    # 25: Apps=5, Reps=1 -> 0.2

    assert rates[1] == pytest.approx(5/6)
    assert rates[5] == pytest.approx(5/6)
    assert rates[11] == pytest.approx(0.5)
    assert rates[16] == pytest.approx(0.25)
    assert rates[22] == pytest.approx(1/3) # Correto
    # *** ASSERT CORRIGIDO PARA 24 ***
    assert rates[24] == pytest.approx(1/3) # Esperado ~0.333
    assert rates[25] == pytest.approx(0.25)

def test_calculate_historical_repetition_rate_no_data():
    """ Testa com DataFrame vazio ou None ou < 2 linhas. """
    with patch('src.analysis.repetition_analysis.read_data_from_db', return_value=None):
        rates_none = calculate_historical_repetition_rate(concurso_maximo=10)
        assert rates_none.eq(0.0).all()
    with patch('src.analysis.repetition_analysis.read_data_from_db', return_value=pd.DataFrame()):
        rates_empty = calculate_historical_repetition_rate(concurso_maximo=10)
        assert rates_empty.eq(0.0).all()
    with patch('src.analysis.repetition_analysis.read_data_from_db') as mock_read:
        mock_read.return_value = pd.DataFrame(TEST_DATA_SORTEIOS[:1])
        rates_one = calculate_historical_repetition_rate(concurso_maximo=1)
        assert rates_one.eq(0.0).all()

--------------------------------------------------------------------------------
# Arquivo: tests/test_scorer.py
--------------------------------------------------------------------------------
# tests/test_scorer.py

import pytest
import pandas as pd
import numpy as np
from typing import List, Optional, Set

# <<< Importa a função e a config V8 >>>
from src.scorer import calculate_scores, DEFAULT_SCORING_CONFIG_V8, MISSING_CYCLE_BONUS, REPEAT_PENALTY
# Importa ALL_NUMBERS e janelas de grupo do config
try: from src.config import ALL_NUMBERS, DEFAULT_GROUP_WINDOWS, AGGREGATOR_WINDOWS
except ImportError: ALL_NUMBERS = list(range(1, 26)); DEFAULT_GROUP_WINDOWS=[25,100]; AGGREGATOR_WINDOWS=[10,25,50,100,200,300,400,500]

if 'ALL_NUMBERS' not in globals() or not ALL_NUMBERS: ALL_NUMBERS_TEST: List[int] = list(range(1, 26))
else: ALL_NUMBERS_TEST: List[int] = ALL_NUMBERS


# --- Fixture com Dados Mockados (para V8) ---
@pytest.fixture
def mock_analysis_results():
    """ Cria um dicionário de resultados de análise mockado para V8. """
    results = {}
    idx = ALL_NUMBERS_TEST
    # Frequências
    results['overall_freq'] = pd.Series(range(25, 0, -1), index=idx)
    for w in AGGREGATOR_WINDOWS: results[f'recent_freq_{w}'] = pd.Series(range(25, 0, -1), index=idx) * (1 + w/1000.0)
    # Atrasos
    results['current_delay'] = pd.Series(range(25, 0, -1), index=idx)
    results['delay_std_dev'] = pd.Series(range(1, 26), index=idx)
    results['avg_hist_intra_delay'] = pd.Series(range(1, 26), index=idx)
    results['max_hist_intra_delay'] = pd.Series(range(1, 26), index=idx)
    # Ciclos
    results['last_cycle_freq'] = pd.Series(5, index=idx)
    results['current_cycle_freq'] = pd.Series(range(25, 0, -1), index=idx)
    results['missing_current_cycle'] = set()
    results['current_intra_cycle_delay'] = pd.Series(range(25, 0, -1), index=idx)
    results['closing_freq'] = pd.Series(range(1, 26), index=idx)
    results['sole_closing_freq'] = pd.Series(range(1, 26), index=idx)
    # Tendências
    trend_values = np.arange(5.0, 5.0 - (25 * 0.2), -0.2)[:25]; results['freq_trend'] = pd.Series(trend_values, index=idx)
    results['rank_trend'] = pd.Series(range(10, -15, -1), index=idx)
    # Grupo Stats
    for w in DEFAULT_GROUP_WINDOWS: results[f'group_W{w}_avg_freq'] = pd.Series(5.0, index=idx)
    # Repetição
    results['numbers_in_last_draw'] = {1, 3, 5} # Penaliza 1, 3, 5
    results['repetition_rate'] = pd.Series(np.linspace(0.1, 0.9, 25), index=idx)

    # Adiciona NaNs
    results['last_cycle_freq'].loc[10] = pd.NA
    results['delay_std_dev'].loc[15] = pd.NA
    #results['avg_hist_intra_delay'].loc[20] = pd.NA # Não usado na V8 atual
    results['closing_freq'].loc[2] = pd.NA
    results['rank_trend'].loc[4] = pd.NA
    results['group_W25_avg_freq'].loc[6] = pd.NA # Adiciona NaN em stat de grupo

    return results

# --- Testes (Usam a config V8 agora) ---

def test_calculate_scores_returns_valid_series(mock_analysis_results):
    """ Testa se retorna Series válida com config V8. """
    # <<< USA V8 >>>
    scores = calculate_scores(mock_analysis_results, config=DEFAULT_SCORING_CONFIG_V8)
    assert scores is not None; assert isinstance(scores, pd.Series); assert len(scores) == 25
    assert set(scores.index) == set(ALL_NUMBERS_TEST); assert not scores.isnull().any()

def test_calculate_scores_ranking_extremos(mock_analysis_results):
    """ Testa se os extremos do ranking V8 estão corretos para mock data. """
    # <<< USA V8 >>>
    scores = calculate_scores(mock_analysis_results, config=DEFAULT_SCORING_CONFIG_V8)
    assert scores is not None
    # Verifica apenas se calcula e se os extremos não são NaN
    assert pd.notna(scores.idxmax()) and scores.idxmax() in ALL_NUMBERS_TEST
    assert pd.notna(scores.idxmin()) and scores.idxmin() in ALL_NUMBERS_TEST
    print("\nScores (Teste Extremos V8):"); print(scores.head(3)); print(scores.tail(3))

def test_calculate_scores_cycle_bonus(mock_analysis_results):
    """ Testa a aplicação correta do bônus de ciclo com config V8. """
    missing_set: Set[int] = {23, 24, 25}
    mock_copy_sem_bonus = mock_analysis_results.copy(); mock_copy_sem_bonus['missing_current_cycle'] = set()
    mock_copy_com_bonus = mock_analysis_results.copy(); mock_copy_com_bonus['missing_current_cycle'] = missing_set
    # <<< USA V8 >>>
    scores_sem_bonus = calculate_scores(mock_copy_sem_bonus, config=DEFAULT_SCORING_CONFIG_V8)
    scores_com_bonus = calculate_scores(mock_copy_com_bonus, config=DEFAULT_SCORING_CONFIG_V8)
    assert scores_com_bonus is not None; assert scores_sem_bonus is not None
    for dezena in missing_set: assert scores_com_bonus[dezena] == pytest.approx(scores_sem_bonus[dezena] + MISSING_CYCLE_BONUS)
    for dezena in set(ALL_NUMBERS_TEST) - missing_set: assert scores_com_bonus[dezena] == pytest.approx(scores_sem_bonus[dezena])

def test_calculate_scores_repeat_penalty(mock_analysis_results):
     """ Testa a aplicação da penalidade de repetição com V8. """
     repeated_set: Set[int] = {1, 3, 5}
     mock_copy_sem_penalty = mock_analysis_results.copy(); mock_copy_sem_penalty['numbers_in_last_draw'] = set()
     mock_copy_com_penalty = mock_analysis_results.copy();
     # <<< USA V8 >>>
     scores_sem_penalty = calculate_scores(mock_copy_sem_penalty, config=DEFAULT_SCORING_CONFIG_V8)
     scores_com_penalty = calculate_scores(mock_copy_com_penalty, config=DEFAULT_SCORING_CONFIG_V8)
     assert scores_com_penalty is not None; assert scores_sem_penalty is not None
     for dezena in repeated_set: assert scores_com_penalty[dezena] == pytest.approx(scores_sem_penalty[dezena] + REPEAT_PENALTY)
     for dezena in set(ALL_NUMBERS_TEST) - repeated_set: assert scores_com_penalty[dezena] == pytest.approx(scores_sem_penalty[dezena])

def test_calculate_scores_missing_metric(mock_analysis_results):
     """ Testa se lida bem com métrica faltando com V8. """
     import copy
     results_copy = copy.deepcopy(mock_analysis_results)
     metric_to_remove = 'rank_trend' # Remove uma das novas métricas
     # <<< USA V8 >>>
     assert metric_to_remove in DEFAULT_SCORING_CONFIG_V8
     del results_copy[metric_to_remove]
     scores = calculate_scores(results_copy, config=DEFAULT_SCORING_CONFIG_V8)
     assert scores is not None; assert len(scores) == 25; assert not scores.isnull().any()
     print("\nScores (Métrica Faltante V8): OK")

